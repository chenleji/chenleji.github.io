{"meta":{"title":"ljchen's Notes","subtitle":"Quick Notes","description":"不切实际的理想主义者...","author":"ljchen","url":"http://ljchen.net"},"pages":[{"title":"Categories","date":"2018-05-21T06:49:55.000Z","updated":"2020-04-06T09:24:01.506Z","comments":true,"path":"categories/index.html","permalink":"http://ljchen.net/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-05-21T06:49:27.000Z","updated":"2020-04-06T09:24:01.506Z","comments":true,"path":"tags/index.html","permalink":"http://ljchen.net/tags/index.html","excerpt":"","text":""},{"title":"","date":"2018-06-23T07:39:52.000Z","updated":"2020-04-06T09:24:01.506Z","comments":true,"path":"about/index.html","permalink":"http://ljchen.net/about/index.html","excerpt":"","text":"I’m ljchen, working on container cloud platforms. Has many years of practical experience in kubernetes, cloud computing IaaS, SDN and other fields. Contact me if you wanna work together or just chat."}],"posts":[{"title":"使用Eureka的微服务如何上Istio","slug":"使用Eureka的微服务如何上Istio","date":"2020-04-06T03:44:06.000Z","updated":"2020-04-06T09:24:01.486Z","comments":true,"path":"2020/04/06/使用Eureka的微服务如何上Istio/","link":"","permalink":"http://ljchen.net/2020/04/06/使用Eureka的微服务如何上Istio/","excerpt":"由于当前公司的绝大多数应用都还是使用eureka来做服务发现，算法团队又提出一些灰度的需求。从不重复造轮子以及技术趋势的角度，自然想到是否能够将Istio整合到平台产品中。但是，据我所知，eureka2.0已经被放弃了，Istio也在早期版本中，直接废弃了对Eureka做服务发现的支持。因此，这可能需要对现有的脚手架做一定改造，这个后面具体再论。另一封面，听说istio在架构上发生了很大的变化，于是下载了1.5版本来亲自体验了一下。","text":"由于当前公司的绝大多数应用都还是使用eureka来做服务发现，算法团队又提出一些灰度的需求。从不重复造轮子以及技术趋势的角度，自然想到是否能够将Istio整合到平台产品中。但是，据我所知，eureka2.0已经被放弃了，Istio也在早期版本中，直接废弃了对Eureka做服务发现的支持。因此，这可能需要对现有的脚手架做一定改造，这个后面具体再论。另一封面，听说istio在架构上发生了很大的变化，于是下载了1.5版本来亲自体验了一下。 v1.5架构变化其实也不能够算是软件架构的大变化，更准确的说法应该是部署架构的变化。就是将之前控制面上多个微服务都变为模块的形势，统一放到istiod里面去运行。这样一个比较大的好处是，部署起来就方便多了。当然，提到部署的便捷性，istioctl这个工具还是相当不错的。基于istioctl manifest apply 可以快速按照用户指定的profile拉起来一个istio环境。但是，除了istio自带的模块被打包到了istiod里面之外，如果你还启用了调用链、监控、遥测之类的功能的话，在istio-system中，依然还有一大堆pod。如果想要彻底变革，这些是不是也可以考虑整合一把，哪怕是放到一个pod里面去，比如说kiali其实是高度依赖于prometheus的数据的。 使用Istio的顾虑早年openstack上neutron还不够成熟的时候，经常出现VM网络不通，然后又各种折腾排查，动不动就需要在某个接口上tcpdump。说实在的，这样的日子如履薄冰，天天神经脆弱。我所理解的Istio中traffic的部分，本质是在容器层面的SDN；比起VM网络，envoy上大量的XDS规则，一旦有问题，可能比VM网络更难排查。所以，长久以来，对这货都不敢轻易引入（当年v1.0似乎也是在赶鸭子上架，很快速的又推出了新版本）。 好在如今的版本中，通过istioctl pc等工具，极大的降低了登录到各个数据面节点上排查路由规则的复杂度，同时，istioctl ps也提供了控制面板到数据面配置一致性的debug手法。所以，现在采用istio，在功能层面，我是没啥顾虑的；那么，唯一可能比较担心的还是在于性能层面。 但是，我们完全可以将该技术先只使用到开发测试环境，等待其数据面代理性能得到一个整体提升后，再最终用到生产中。 最适用的功能在Istio提供的所有功能中，其实也没必要全都采纳，比如调用链APM这块，我就觉得比较二，对代码的入侵性太强，我更愿意在平台类产品中整合pinpoint或者skywalking之类专业的调用链。 另外，数据安全加密这块，对于绝大多数的应用场景，其实都是非必要的，关键它还对转发性能造成了较大影响。就像做管理决策，当我们排优先级的时候，哪些事情是能够为企业带来更大价值产出的，那么我认为这些事理应提高其优先级。 如前面所述，先在开发测试环境中，使用Istio的traffic控制相关的功能，同时，能够使用遥测数据来生成项目微服务全局的流量状况视图，应该算是比较稳妥的。 与eureka/consul兼容我这里提到consul，其实并非是想将consul与Istio的pilot-discovery对接，因为之前在研究这块的时候，感觉不过是一个炫头，大家一般都不这么用。 这里有一个插曲，大概是2018年的时候，有一次我在团队里面提出过：“到底istio的数据面是如何转发流量的？” 这个问题是源于istio默认结合k8s的服务发现来工作，我们都知道，K8S的服务发现，是通过watch service上的cluster IP变化来刷新dns的（这个设计很巧妙，为啥不是直接刷新pod IP，而要使用cluster IP呢？因为dns client一般都有缓存，而cluster IP到pod ip的路由是由K8S实时控制的，因此…）。 我们回来讲Istio，因为Istio是基于K8S来做服务发现的，如果服务A要访问B，不管A和B的sidecar会对流量做怎么的操作。毕竟A的程序看不到sidecar，它要转流量到B之前，总是会查询B的地址，但是在DNS查询返回的IP列表里面，目的IP是cluster IP。这时，报文被封装，并基于网络协议栈发送出去，再被A的sidecar拦截。此时A的sidecar会基于路由规则来修改A的真实转发地址。那么，在这个过程中，A发送出去报文的目的IP地址，到底有啥“卵用”？ 从本质上来说，这货确实没啥用，因为都被替换了。但是，也不是绝对的，假设A、B之间发送的报文是基于HTTP协议。我们知道HTTP协议有一个header，header里面有一个叫做“host”的一级公民。其实envoy就是依据这个host来首先过滤流量的。比如说，A要发送请求个B服务，如果它使用的是service-b这个域名的话，虽然协议栈会解析出service-b的IP来做底层IP报文的destination IP。但是顶层的HTTP协议封装的时候，依然会使用“host:service-b”来封装header。另一方面，所有TCP报文送出pod的时候，都会被其sidecar拦截并基于转发逻辑转发报文；所以，都还没有等IP报文中的destination IP发挥作用，就被sidecar替换了。因此，这个只要我们保障HTTP的“host”字段匹配了envoy中的规则，目的IP不重要。 比如，A要发送报文给B，当我们配置好了Istio规则之后，A在封装报文的时候，只需要确保HTTP的header中包含“host:B”，而目的IP可以随便填写。 所以，结论就是：A怎么知道B的目的IP这个事情，真的不重要！A是通过consul，eureka还是基于K8S dns查询到B的地址的，都一样。唯一重要的是是否满足envoy的转发规则。 但是，HTTP的很多SDK库都会忽略掉我们在上层直接设置的host值，而采用所访问域名或者目的IP地址。这就是一个坑，所以，需要对代码脚手架做一些改造，目标就是强制设置该host值。 实践以下是一个调用链的例子，我写了一个程序，通过环境变量传入微服务名称和工作模式，其中stub模式为叶子节点，ingress模式为流量入口（树根节点），需要指定其下一跳节点（各分支或叶子）。所有的服务起来之后，都会将自己注册到consul上面，consul会基于注册的心跳时间来对各个服务发起健康检查。当服务A要访问服务B的时候，会在consul上查询存活的B列表，并访问B服务。 调用链拓扑为： 12a - &gt; c - &gt; b - &gt; d 我们实际部署后，在kiali上看到的流量转发图是这样的： 除了业务的调用链之外，我们看到consul有往各个service做健康检查的流量。由于consul发往各个service的流量的host我们没法做修改，因此看到了匹配passthrough cluster的力量。这个是Istio上如果流量没法匹配对应的路由，就会发送到passthrough cluster，最终基于报文的目标IP来转发（当然也可以配置成黑洞模式，这样没法匹配istio中路由规则的流量就会全部被干掉）。 下面是k8s上，启动整个应用各微服务的yaml文件和istio config文件： services.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368apiVersion: v1items:- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: run: centos template: metadata: labels: run: centos spec: containers: - args: - sleep - \"999999\" image: centos:7 imagePullPolicy: IfNotPresent name: centos- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: consul version: v1 name: consul namespace: default spec: replicas: 1 selector: matchLabels: app: consul version: v1 template: metadata: labels: app: consul version: v1 spec: containers: - env: - name: CONSUL_BIND_INTERFACE value: eth0 image: consul imagePullPolicy: Always name: consul ports: - containerPort: 8500 protocol: TCP- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: service-a version: v1 name: service-a namespace: default spec: replicas: 1 selector: matchLabels: app: service-a version: v1 template: metadata: labels: app: service-a version: v1 spec: containers: - env: - name: CONSUL_ADDR value: consul - name: CONSUL_PORT value: \"8500\" - name: app value: service-a - name: mode value: ingress - name: next_services value: service-b,service-c image: ljchen/istio-demo imagePullPolicy: Always name: service-a ports: - containerPort: 9090 name: http protocol: TCP- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: service-b version: v1 name: service-b namespace: default spec: replicas: 1 selector: matchLabels: app: service-b version: v1 template: metadata: labels: app: service-b version: v1 spec: containers: - env: - name: CONSUL_ADDR value: consul - name: CONSUL_PORT value: \"8500\" - name: app value: service-b - name: next_services value: service-d image: ljchen/istio-demo imagePullPolicy: Always name: service-b ports: - containerPort: 9090 name: http protocol: TCP- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: service-c version: v1 name: service-c namespace: default spec: replicas: 1 selector: matchLabels: app: service-c version: v1 template: metadata: labels: app: service-c version: v1 spec: containers: - env: - name: CONSUL_ADDR value: consul - name: CONSUL_PORT value: \"8500\" - name: app value: service-c - name: mode value: stub image: ljchen/istio-demo imagePullPolicy: Always name: service-c ports: - containerPort: 9090 name: http protocol: TCP- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: service-d version: v1 name: service-d-v1 namespace: default spec: replicas: 1 selector: matchLabels: app: service-d version: v1 template: metadata: labels: app: service-d version: v1 spec: containers: - env: - name: CONSUL_ADDR value: consul - name: CONSUL_PORT value: \"8500\" - name: app value: service-d - name: mode value: stub - name: version value: v1 image: ljchen/istio-demo imagePullPolicy: Always name: service-d-v1 ports: - containerPort: 9090 name: http protocol: TCP- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: service-d version: v2 name: service-d-v2 namespace: default spec: progressDeadlineSeconds: 600 replicas: 1 selector: matchLabels: app: service-d version: v2 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: service-d version: v2 spec: containers: - env: - name: CONSUL_ADDR value: consul - name: CONSUL_PORT value: \"8500\" - name: app value: service-d - name: mode value: stub - name: version value: v2 image: ljchen/istio-demo imagePullPolicy: Always name: service-d-v2 ports: - containerPort: 9090 name: http protocol: TCP dnsPolicy: ClusterFirst restartPolicy: Always- apiVersion: v1 kind: Service metadata: labels: app: consul version: v1 name: consul namespace: default spec: externalTrafficPolicy: Cluster ports: - name: http nodePort: 31057 port: 8500 protocol: TCP targetPort: 8500 selector: app: consul version: v1 sessionAffinity: None type: NodePort- apiVersion: v1 kind: Service metadata: labels: component: apiserver provider: kubernetes name: kubernetes namespace: default spec: ports: - name: https port: 443 protocol: TCP targetPort: 6443 sessionAffinity: None type: ClusterIP- apiVersion: v1 kind: Service metadata: labels: app: service-a version: v1 name: service-a namespace: default spec: ports: - name: http port: 9090 protocol: TCP targetPort: 9090 selector: app: service-a sessionAffinity: None type: ClusterIP- apiVersion: v1 kind: Service metadata: labels: app: service-b version: v1 name: service-b namespace: default spec: ports: - name: http port: 9090 protocol: TCP targetPort: 9090 selector: app: service-b sessionAffinity: None type: ClusterIP- apiVersion: v1 kind: Service metadata: labels: app: service-c version: v1 name: service-c namespace: default spec: ports: - name: http port: 9090 protocol: TCP targetPort: 9090 selector: app: service-c sessionAffinity: None type: ClusterIP- apiVersion: v1 kind: Service metadata: labels: app: service-d version: v1 name: service-d namespace: default spec: ports: - name: http port: 9090 protocol: TCP targetPort: 9090 selector: app: service-d sessionAffinity: None type: ClusterIPkind: Listmetadata: resourceVersion: \"\" selfLink: \"\" istio-config.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150apiVersion: v1items:- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: consul namespace: default spec: hosts: - consul http: - route: - destination: host: consul subset: v1- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: service-a namespace: default spec: gateways: - gateway hosts: - ljchen.net http: - route: - destination: host: service-a subset: v1- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: service-b namespace: default spec: hosts: - service-b http: - route: - destination: host: service-b subset: v1- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: service-c namespace: default spec: hosts: - service-c http: - route: - destination: host: service-c subset: v1- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: service-d namespace: default spec: hosts: - service-d http: - route: - destination: host: service-d subset: v1 - destination: host: service-d subset: v2 weight: 100- apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: gateway namespace: default spec: selector: istio: ingressgateway servers: - hosts: - ljchen.net port: name: http number: 80 protocol: HTTP- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: consul namespace: default spec: host: consul subsets: - labels: version: v1 name: v1- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: service-a namespace: default spec: host: service-a subsets: - labels: version: v1 name: v1- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: service-b namespace: default spec: host: service-b subsets: - labels: version: v1 name: v1- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: service-c namespace: default spec: host: service-c subsets: - labels: version: v1 name: v1- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: service-d namespace: default spec: host: service-d subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2kind: Listmetadata: resourceVersion: \"\" selfLink: \"\"","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"istio","slug":"istio","permalink":"http://ljchen.net/tags/istio/"},{"name":"eureka","slug":"eureka","permalink":"http://ljchen.net/tags/eureka/"}]},{"title":"TiDB架构原理","slug":"TiDB架构原理","date":"2019-10-12T09:23:38.000Z","updated":"2020-04-06T09:24:01.469Z","comments":true,"path":"2019/10/12/TiDB架构原理/","link":"","permalink":"http://ljchen.net/2019/10/12/TiDB架构原理/","excerpt":"MySQL的一些集群方案和数据库中间件都在视图解决当表条目变大时，查询性能下降的问题。TiDB的设计原生适用于大规模数据量的存储和查询，重点学习了关于其计算、存储和调度的三篇文档，整理如下。","text":"MySQL的一些集群方案和数据库中间件都在视图解决当表条目变大时，查询性能下降的问题。TiDB的设计原生适用于大规模数据量的存储和查询，重点学习了关于其计算、存储和调度的三篇文档，整理如下。 架构 上图展示了TiDB的整体架构设计，重点来看TiDB、PD和TiKV的功能： TiDB 负责接收SQL请求、语法检查 做SQL到逻辑查询逻辑的转换 基于PD信息将逻辑查询转化为物理查询计划 从TiKV获取数据后整合并返回； PD（Placement Driver） 保存SQL逻辑表数据在物理TiKV中分布的映射关系 对TiKV集群做调度和负载均衡 分配全局事务ID TiKV KV存储引擎 计算(TiDB)整层架构如下图所示: 转换规则TiDB最终是将数据以KV的形势存储到分布式的TIKV上，因此TiDB的重要作用就是做SQL表到KV存储的转换。对于一个表来讲，需要存储的数据包括： 表的元信息 表中的条目 索引数据 元信息每个 Database/Table 都被分配了一个唯一的 ID，这个 ID 作为唯一标识，并且在编码为 Key-Value 时，这个 ID 都会编码到 Key 中，再加上 m_ 前缀。这样可以构造出一个 Key，Value 中存储的是序列化后的元信息。 表条目TiDB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID（如果表有整数型的 Primary Key，那么会用 Primary Key 的值当做 RowID）,行记录转换方式为： 12Key: tablePrefix&#123;tableID&#125;_recordPrefixSep&#123;rowID&#125;Value: [col1, col2, col3, col4] 实例: SQL表格式 1231, \"TiDB\", \"SQL Layer\", 102, \"TiKV\", \"KV Engine\", 203, \"PD\", \"Manager\", 30 Row转换后 123t10_r1 --&gt; [\"TiDB\", \"SQL Layer\", 10]t10_r2 --&gt; [\"TiKV\", \"KV Engine\", 20]t10_r3 --&gt; [\"PD\", \"Manager\", 30] 索引数据Unique Index 12Key: tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValueValue: rowID 非 Unique Index 12Key: tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValue_rowIDValue: null 实例 123t10_i1_10_1 --&gt; nullt10_i1_20_2 --&gt; nullt10_i1_30_3 --&gt; null 操作与查询过程对数据的操作包括： 对于 Insert 语句，需要将 Row 写入 KV，并且建立好索引数据。 对于 Update 语句，需要将 Row 更新的同时，更新索引数据（如果有必要）。 对于 Delete 语句，需要在删除 Row 的同时，将索引也删除。 执行查询语句Select count(*) from user where name=&quot;TiDB”;流程: 构造出 Key Range：一个表中所有的 RowID 都在 [0, MaxInt64) 这个范围内，那么我们用 0 和 MaxInt64 根据 Row 的 Key 编码规则，就能构造出一个 [StartKey, EndKey) 的左闭右开区间 扫描 Key Range：根据上面构造出的 Key Range，读取 TiKV 中的数据 过滤数据：对于读到的每一行数据，计算 name=”TiDB” 这个表达式，如果为真，则向上返回这一行，否则丢弃这一行数据 计算 Count：对符合要求的每一行，累计到 Count 值上面 这个方案肯定是可以 Work 的，但是并不能 Work 的很好，原因是显而易见的： 为了减少网络开销，会尽量将一些计算下沉到TiKV, 比如将上面的扫描KeyRange、过滤数据和计算Count都放到TiKV, 然后基于各个分布式TiKV计算之后，将结果再上报到TIDB做汇总。 存储(TiKV)TiKV的存储需要解决一下问题： 如何高效存储和访问； 数据冗余与一致性问题； 当数据量太大的时候，如何解决前面两个问题； 如何高效存储和访问 这是一个巨大的 Map，也就是存储的是 Key-Value pair（底层是采用RocksDB一个单机的 Key-Value Map存储） 这个 Map 中的 Key-Value pair 按照 Key 的二进制顺序有序，也就是我们可以 Seek 到某一个 Key 的位置，然后不断的调用 Next 方法以递增的顺序获取比这个 Key 大的 Key-Value MVCCTiKV 的 MVCC 实现是通过在 Key 后面添加 Version 来实现的。 key的排列变成这样： 123456789101112Key1-Version3 -&gt; ValueKey1-Version2 -&gt; ValueKey1-Version1 -&gt; Value....Key2-Version4 -&gt; ValueKey2-Version3 -&gt; ValueKey2-Version2 -&gt; ValueKey2-Version1 -&gt; Value....KeyN-Version2 -&gt; ValueKeyN-Version1 -&gt; Value.... TiKV 的事务采用乐观锁，事务的执行过程中，不会检测写写冲突，只有在提交过程中，才会做冲突检测，冲突的双方中比较早完成提交的会写入成功，另一方会尝试重新执行整个事务。 数据冗余与一致性问题 通过在多台物理节点上保存多个副本来做冗余； 使用raft协议来保障数据的一致性； 总结起来就是下图的架构设计，通过单机的 RocksDB，我们可以将数据快速地存储在磁盘上；通过 Raft，我们可以将数据复制到多台机器上，以防单机失效。数据的写入是通过 Raft 这一层的接口写入，而不是直接写 RocksDB。 Region 将整个 Key-Value 空间分成很多段，每一段是一系列连续的 Key，每一段叫做一个 Region； 尽量保持每个 Region 中保存的数据不超过一定的大小(默认是 64mb)； 每一个 Region 都可以用 StartKey 到 EndKey 这样一个左闭右开区间来描述。 当数据量大时，如何解决前两个问题 以 Region 为单位，将数据分散在集群中所有的节点上，并且尽量保证每个节点上服务的 Region 数量差不多； 以 Region 为单位做 Raft 的复制和成员管理； 调度(PD)Pd之间本身通过etcd来实现分布式高可用容错和数据的一致性； 信息收集 每个 TiKV 节点会定期向 PD 汇报节点的整体信息 每个 Raft Group 的 Leader 会定期向 PD 汇报信息 调度策略 一个 Region 的 Replica 数量正确 一个 Raft Group 中的多个 Replica 不在同一个位置 副本在 Store 之间的分布均匀分配 Leader 数量在 Store 之间均匀分配 访问热点数量在 Store 之间均匀分配 各个 Store 的存储空间占用大致相等 控制调度速度，避免影响在线服务 支持手动下线节点 调度实现 PD 不断的通过 Store 或者 Leader 的心跳包收集信息，获得整个集群的详细数据 根据这些信息以及调度策略生成调度操作序列 收到 Region Leader 发来的心跳包时，PD 都会检查是否有对这个 Region 待进行的操作，通过心跳包的回复消息，将需要进行的操作返回给 Region Leader，并在后面的心跳包中监测执行结果 binlog TiDB 实例连接到各个 Pump 节点并发送 binlog 数据到 Pump 节点。 Pump 集群连接到 Drainer 节点，Drainer 将接收到的更新数据转换到某个特定下游（例如 Kafka、另一个 TiDB 集群或 MySQL 或 MariaDB Server）指定的正确格式。 数据热点问题为解决热点问题，TiDB 引入了预切分 Region 的功能，即可以根据指定的参数，预先为某个表切分出多个 Region，并打散到各个 TiKV 上去。 1SPLIT TABLE table_name [INDEX index_name] BETWEEN (lower_value) AND (upper_value) REGIONS region_num BETWEEN lower_value AND upper_value REGIONS region_num 语法是通过指定上、下边界和 Region数量，然后在上、下边界之间均匀切分出 region_num 个 Region。 1SPLIT TABLE table_name [INDEX index_name] BY (value_list) [, (value_list)] ... BY value_list… 语法将手动指定一系列的点，然后根据这些指定的点切分 Region，适用于数据不均匀分布的场景 均匀切分 由于 row_id 是整数，所以根据指定的 lower_value、upper_value 以及 region_num，可以推算出需要切分的 key。TiDB 先计算 step（step = (upper_value - lower_value)/num），然后在 lower_value 和 upper_value 之间每隔 step 区间切一次，最终切出 num 个 Region。 1SPLIT TABLE t BETWEEN (-9223372036854775808) AND (9223372036854775807) REGIONS 16; 不均匀切分 如果已知数据不是均匀分布的，比如想要 -inf ~ 10000 切一个 Region，10000 ~ 90000 切一个 Region，90000 ~ +inf 切一个 Region，可以通过手动指定点来切分 Region，示例如下： 1SPLIT TABLE t BY (10000), (90000);","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"TiKV","slug":"TiKV","permalink":"http://ljchen.net/tags/TiKV/"},{"name":"TiDB","slug":"TiDB","permalink":"http://ljchen.net/tags/TiDB/"}]},{"title":"MySQL Innodb Cluster(MGR)压力测试","slug":"MySQL-Innodb-Cluster-MGR-集群搭建与压力测试","date":"2019-09-24T14:29:32.000Z","updated":"2020-04-06T09:24:01.467Z","comments":true,"path":"2019/09/24/MySQL-Innodb-Cluster-MGR-集群搭建与压力测试/","link":"","permalink":"http://ljchen.net/2019/09/24/MySQL-Innodb-Cluster-MGR-集群搭建与压力测试/","excerpt":"之前一直对腾讯TDSQL的实现原理比较感兴趣，号称使用Raft来保障数据的一致性。MySQL官方推荐使用MGR的方案来搭建高可用集群，基于Paxos的MGR毕竟是官方的解决方案，也很想知道其性能到底如何。","text":"之前一直对腾讯TDSQL的实现原理比较感兴趣，号称使用Raft来保障数据的一致性。MySQL官方推荐使用MGR的方案来搭建高可用集群，基于Paxos的MGR毕竟是官方的解决方案，也很想知道其性能到底如何。 另一方面，在同城两个容灾数据中心之间，由于测试出来只有4ms的时延，理论上将MGR的一个节点放到容灾数据中心也是没有问题的。这样可以解决异步复制和半同步复制都无法解决的数据一致性问题。 资源准备主机需要至少有四台主机，其中：三台作为集群节点，一台安装mysqlrouter（同时部署mysql-shell）机器的配置都是：4核16G 1234567891011# 修改主机名hostnamectl set-hostname sh-mrouter# 修改hosts文件，便于使用主机名互访cat /etc/hosts10.254.200.13 sh-mrouter10.254.200.5 sh-mgr110.254.200.8 sh-mgr2# 注意，最后一个节点的网段和其他节点不一样，因为它在同城的另一个机房10.254.201.70 sh-mgr4 yum源慢！真的很慢！官方的yum源安装起来能够让人崩溃。所以，我找了清华的源，然后自己改了配置。由于我只关注8.x的版本，因此只修改了这部分，要使用5.7的请自行修改。 123456789101112131415cat mysql.repo[mysql80-community]name=MySQL 8.0 Community Serverbaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql80-community-el7/enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-tools-community]name=MySQL Tools Communitybaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7/enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 安装服务mysqld12yum install -y mysql-serversystemctl start mysqld mysql-shell1yum install -y mysql-shell mysql-router1yum install -y mysql-router 配置集群密码与权限12345678910111213# 从日志从获取系统生成的grep 'A temporary password is generated for root@localhost' /var/log/mysqld.log | awk -F ' ' '&#123;print $(NF)&#125;'# 使用密码登录到mysqlmysql -uroot -p# 更改密码和权限ALTER USER 'root'@'localhost' IDENTIFIED BY 'Mysql,123';use mysql;update user set host='%' where user='root';# 注意这里的mysql_native_password，不修改会导致集群登录有问题ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'Mysql,123'; FLUSH PRIVILEGES; my.cnf配置更新12345678910# 在my.cnf添加一下配置，注意这里的server_id每台节点需要不一样cat /etc/my.cnfenforce_gtid_consistency=1gtid_mode=onserver_id=1# 当你有节点跨网段时，mysql自动的白名单没法加入跨网段的IP， 需要手动修改 group_replication_ip_whitelist group_replication_ip_whitelist=\"10.254.200.13/24,10.254.201.70/24\"systemctl restart mysqld mysqlsh创建集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 连接到节点\\connect root@sh-mgr1:3306# 检查当前配置是否能够加入集群dba.configureInstance('root@sh-mgr1:3306')# 创建集群dba.createCluster('cluster_1')# 加入其他节点var cluster = dba.getCluster()cluster.addInstance('root@sh-mgr2:3306')cluster.addInstance('root@sh-mgr4:3306')# 查看节点拓扑cluster.status()# 以下信息显示有三个节点ONLINE，当前集群可以容忍有一个节点挂掉&#123; \"clusterName\": \"cluster_1\", \"defaultReplicaSet\": &#123; \"name\": \"default\", \"primary\": \"sh-mgr1:3306\", \"ssl\": \"REQUIRED\", \"status\": \"OK\", \"statusText\": \"Cluster is ONLINE and can tolerate up to ONE failure.\", \"topology\": &#123; \"sh-mgr1:3306\": &#123; \"address\": \"sh-mgr1:3306\", \"mode\": \"R/W\", \"readReplicas\": &#123;&#125;, \"role\": \"HA\", \"status\": \"ONLINE\", \"version\": \"8.0.17\" &#125;, \"sh-mgr2:3306\": &#123; \"address\": \"sh-mgr2:3306\", \"mode\": \"R/O\", \"readReplicas\": &#123;&#125;, \"role\": \"HA\", \"status\": \"ONLINE\", \"version\": \"8.0.17\" &#125;, \"sh-mgr4:3306\": &#123; \"address\": \"sh-mgr4:3306\", \"mode\": \"R/O\", \"readReplicas\": &#123;&#125;, \"role\": \"HA\", \"status\": \"ONLINE\", \"version\": \"8.0.17\" &#125; &#125;, \"topologyMode\": \"Single-Primary\" &#125;, \"groupInformationSourceMember\": \"sh-mgr1:3306\"&#125; 配置router在安装了mysql-router的节点上直接执行一下命令，会提示输入密码，输入之后系统会告知已经创建配置文件。 1234567891011121314151617181920212223242526272829303132333435[root@mrouter ~]# mysqlrouter --bootstrap root@sh-mgr1:3306 --user=rootPlease enter MySQL password for root:# Bootstrapping system MySQL Router instance...- Checking for old Router accounts - No prior Router accounts found- Creating mysql account mysql_router1_175w87hctokv@'%' for cluster management- Storing account in keyring- Adjusting permissions of generated files- Creating configuration /etc/mysqlrouter/mysqlrouter.conf# MySQL Router configured for the InnoDB cluster 'cluster_1'After this MySQL Router has been started with the generated configuration $ /etc/init.d/mysqlrouter restartor $ systemctl start mysqlrouteror $ mysqlrouter -c /etc/mysqlrouter/mysqlrouter.confthe cluster 'cluster_1' can be reached by connecting to:## MySQL Classic protocol- Read/Write Connections: localhost:6446- Read/Only Connections: localhost:6447## MySQL X protocol- Read/Write Connections: localhost:64460- Read/Only Connections: localhost:64470Existing configuration backed up to '/etc/mysqlrouter/mysqlrouter.conf.bak' 不知道是什么原因，我通过系统命令启动服务一直失败，索性直接使用一下命令启动服务。 1nohup mysqlrouter -c /etc/mysqlrouter/mysqlrouter.conf &amp; 服务对应的读写端口都有自动列出来，大概也可以去手动修改，后面我们做压力测试主要使用R/W的6446端口。 压力测试sysbench安装12curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bashsudo yum -y install sysbench 压力测试测试主要针对于三节点在单个机房的测试，以及其中一个节点在另外一个机房的测试。对应的测试参数和命令如下，大概是先分别压了50万条记录到5张表里面，然后开启100个线程，总共花1分钟时间去读写数据。 12345sysbench /usr/share/sysbench/oltp_read_write.lua --db-driver=mysql --table_size=500000 --tables=5 --time=60 --threads=100 --mysql-host=sh-mgr1 --mysql-port=3306 --mysql-db=test --mysql-user=root --mysql-password=Mysql,123 preparesysbench /usr/share/sysbench/oltp_read_write.lua --db-driver=mysql --table_size=500000 --tables=5 --time=60 --threads=100 --mysql-host=sh-mgr1 --mysql-port=3306 --mysql-db=test --mysql-user=root --mysql-password=Mysql,123 runsysbench /usr/share/sysbench/oltp_read_write.lua --db-driver=mysql --table_size=500000 --tables=5 --time=60 --threads=100 --mysql-host=sh-mgr1 --mysql-port=3306 --mysql-db=test --mysql-user=root --mysql-password=Mysql,123 cleanup 测试结果 第一组 12345678910111213141516171819202122232425SQL statistics: queries performed: read: 803544 write: 229584 other: 114792 total: 1147920 transactions: 57396 (955.34 per sec.) queries: 1147920 (19106.81 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 60.0773s total number of events: 57396Latency (ms): min: 11.32 avg: 104.60 max: 505.75 95th percentile: 193.38 sum: 6003761.80Threads fairness: events (avg/stddev): 573.9600/29.45 execution time (avg/stddev): 60.0376/0.02 第二组 12345678910111213141516171819202122232425SQL statistics: queries performed: read: 793646 write: 226756 other: 113378 total: 1133780 transactions: 56689 (943.96 per sec.) queries: 1133780 (18879.11 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 60.0529s total number of events: 56689Latency (ms): min: 11.02 avg: 105.88 max: 384.31 95th percentile: 189.93 sum: 6002003.55Threads fairness: events (avg/stddev): 566.8900/13.72 execution time (avg/stddev): 60.0200/0.02 第三组 12345678910111213141516171819202122232425SQL statistics: queries performed: read: 804482 write: 229852 other: 114926 total: 1149260 transactions: 57463 (957.06 per sec.) queries: 1149260 (19141.25 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 60.0392s total number of events: 57463Latency (ms): min: 12.81 avg: 104.44 max: 450.94 95th percentile: 189.93 sum: 6001272.40Threads fairness: events (avg/stddev): 574.6300/20.70 execution time (avg/stddev): 60.0127/0.01","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"mysql","slug":"mysql","permalink":"http://ljchen.net/tags/mysql/"},{"name":"mgr","slug":"mgr","permalink":"http://ljchen.net/tags/mgr/"},{"name":"mysql-innodb-cluster","slug":"mysql-innodb-cluster","permalink":"http://ljchen.net/tags/mysql-innodb-cluster/"},{"name":"mysql-cluster","slug":"mysql-cluster","permalink":"http://ljchen.net/tags/mysql-cluster/"}]},{"title":"MySQL必须掌握的原理","slug":"MySQL必须掌握的原理","date":"2019-09-22T12:31:01.000Z","updated":"2020-04-06T09:24:01.467Z","comments":true,"path":"2019/09/22/MySQL必须掌握的原理/","link":"","permalink":"http://ljchen.net/2019/09/22/MySQL必须掌握的原理/","excerpt":"整理多数据中心容灾备份的方案的时候，必然绕不过应用的持久化的备份方案。MySQL经历了这么多年的发展，对应也衍生除了很多高可用的部署架构，能够适用于两地三中心或者同城多活的场景。但是，这里我准备系统性的粗略理一遍MySQL的原理，日后再慢慢填充各部分的内容。","text":"整理多数据中心容灾备份的方案的时候，必然绕不过应用的持久化的备份方案。MySQL经历了这么多年的发展，对应也衍生除了很多高可用的部署架构，能够适用于两地三中心或者同城多活的场景。但是，这里我准备系统性的粗略理一遍MySQL的原理，日后再慢慢填充各部分的内容。 主流的MySQL版本为5.7和当前的8.0，存储引擎都推荐使用innodb，下文就先来讲讲innodb的存储。 Innodb原理由外到内来分析，我们先从innodb引擎的文件类型，再到具体文件内部数据存储结构。 文件类型其实，无论使用什么样的存储引擎，最终都是以文件的形势保存到磁盘上的，这点和elasticsearch很像。我们在mysql中创建一个数据库之后，对应在其data目录下会创建一个与该database同名的目录。比如，这里我创建了一个名叫haha的db，然后在里面添加了一张表Person，查看目录结构大致是这样的： 1234567[root@aliyun-vm mysql]# tree hahahaha├── db.opt├── Persons.frm└── Persons.ibd0 directories, 3 files 接下来，我们来看看其中的两种扩展名文件: .frm 无论选择了哪种存储引擎，所有的MySQL表都会在硬盘上创建一个.frm文件用来描述表的格式（定义）。.frm文件的格式在不同的平台上都是相同的。 .ibd 文件当打开innodb_file_per_table选项时，.ibd文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。如果不指定每个表单独一个innodb文件，会统一将数据存到其data目录下的ibdata1文件中。 存储结构 如上图所示，Persons.ibd这个文件所存储的内容主要就是B+树（索引），一个表可以有多个索引。一个.ibd文件可以存储多个索引，ibd文件存储的就是一个表的所有索引数据。索引文件有段（segment）,簇（extends）,页面（page）组成。 段 (segment): 就是.ibd文件的各部分，包括数据段、索引段、回滚段等。 簇 (extends): 簇是由64个连续的页组成的，每个页大小为16KB，即每个簇的大小为1MB。簇是构成段的基本元素，一个段由若干个簇构成。 页面 (page): 页是InnoDB磁盘管理的最小单位。 一个页面的存储由图中的几部分组成，下面重点讲解关键的部分： 页头（Page Header）：记录页面的控制信息，共占150字节，包括页的左右兄弟页面指针、页面空间使用情况等； 最小虚记录、最大虚记录：两个固定位置存储的虚记录，本身并不存储数据。最小虚记录比任何记录都小，而最大虚记录比任何记录都大。 记录堆（record heap）：表示页面已分配的记录空间，也是索引数据的真正存储区域。记录堆分为：有效记录和已删除记录。有效记录就是索引正常使用的记录，而已删除记录表示索引已经删除，不在使用的记录，会组织成为自由空间链表。 未分配空间：指页面未使用的存储空间，随着页面不断使用，未分配空间将会越来越小。当新插入一条记录时，首先尝试从自由空间链表中获得合适的存储位置（空间足够），如果没有满足的，就会在未分配空间中申请。 页尾（Page Tailer）：页面最后部分，占8个字节，主要存储页面的校验信息。 页面中的页头，最大/最小虚记录以及页尾都是页面中有固定的存储位置。 当插入一条记录，会从Free Space中申请一个记录大小的空间划分到User Records部分，当Free Space部分的空间全部被User Records部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。 一张表中可以有成千上万条记录，一个页只有16KB，所以可能需要好多页来存放数据。不同页其实构成了一条双向链表，File Header是InnoDB页的第一部分，它的FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号，即链表的上一个以及下一个节点指针。 表空间碎片已删除数据的record heap会被新写入的数据覆盖，但是新的数据和老的数据长度不可能完全匹配，于是就产生了很多空间碎片。下面是最简单的空间碎片清理方式： 12345# 查看show table status like '%xxx%'\\G;# 清理alert table xxx engine=innodb; 各种logMySQL配置文件里面就需要指定binlog，errorlog，其实除了这之外，还有redolog，undolog，relaylog等等…. redologInnoDB在更新数据的时候会采用WAL(Write Ahead Logging)，这个日志就是redolog用来保证数据库宕机后可以通过该文件进行恢复(类似于elasticsearch的translog)。这个文件一般只会顺序写，只有在数据库启动的时候才会读取 redolog 文件看是否需要进行恢复。 redolog是几个文件组成类似一个环一样，循环写的。写入 redolog 的时候不能将没有同步到数据页上的记录覆盖，如果碰到这种情况会停下来先进行数据页同步然后在继续写入 redolog 。 其粒度为整个mysql，位于data目录下，在磁盘上默认文件名为：ib_logfile{n} 1234567[root@aliyun-vm mysql]# lltotal 126804drwxr-x--- 2 mysql mysql 4096 Sep 22 20:46 haha-rw-r----- 1 mysql mysql 12582912 Sep 22 20:57 ibdata1-rw-r----- 1 mysql mysql 50331648 Sep 22 20:57 ib_logfile0-rw-r----- 1 mysql mysql 50331648 Sep 3 21:05 ib_logfile1...... 其刷盘方式受该参数影响：innodb_flush_log_at_tx_commit: 0 : redo log thread 每1s刷新redo log和数据到磁盘； 1 : 每次事务提交时，刷新redo log和数据到磁盘； 2 : 每次事务提交时，只刷新redo log到磁盘，但不刷新数据到磁盘； undolog主要用来做事务回滚和MVCC, 这个文件存储在共享表空间中。undolog是逻辑日志(和binlog一样)，它不是记录的将物理的数据页恢复到之前的状态，而是记录的和原sql相反的sql, 比如insert对应delete, update对应另外一个update 。 另外undolog的写入也会有对应的redolog，因为undolog也需要持久化，通过WAL可以提高效率。 truncate 是DDL语言，无法回滚，操作之后，自增ID回归到0。delete 是DML，可以回滚。delete了之后，自增ID不会回到0。 binlogbinlog和redolog的区别是: 记录的是逻辑操作（也就是对应的sql）,而redolog记录的底层某个数据页的物理操作，redolog是循环写的，而binlog是追加写的，不会覆盖以前写的数据。 binlog的三种格式：statement, raw, mixed binlog 的写入页需要通过fsync来保证落盘，MySQL通过sync_binlog来控制是否需要同步刷盘。 1 : 不管sync问题，由系统决定何时刷盘； n : 每执行n个事务之后，执行sync binlog到磁盘； 事务提交流程在事务提交的时候要保证 redolog 写入到文件里，而这个 redolog 包含 主键索引上的数据页的修改，以及共享表空间的回滚段中 undolog 的插入。 在一次更改数据的事务提交中，几种log的刷盘流程如下： 分配事务ID ，开启事务，获取锁，没有获取到锁则等待； 执行器先通过存储引擎找到的数据页，如果缓冲池有则直接取出，没有则去主键索引上取出对应的数据页放入缓冲池； 在数据页内找到记录，取出，对数据做更改，然后写入内存； 生成redolog和undolog到内存，redolog状态为prepare； 将redolog和undolog写入文件并调用fsync； server层生成binlog并写入文件调用fsync； 事务提交； 将redolog的状态改为commited 释放锁。 总结起来就是： 开启事务-&gt; 找记录-&gt; 改字段 -&gt; 生成redolog、undolog并刷盘 -&gt; 生成binlog并刷盘 -&gt; 提交事务 索引关于B+ TREE相关的理论就不分析了，在之前的一片文章中有提到，有兴趣的可以参考这里 如图，索引分为聚集索引和非聚集索引，他们的区别是是否主键索引在叶子节点包含了数据。非聚集索引是将数据存储在别的地方，通过在索引的叶子节点中存储指针来实现的。 注意: 如图所示，innodb普通索引的叶子节点并不包含所有行的数据记录，只会在叶子节点存有自己本身的键值和主键的值； 使用索引的情况 主键自动建立唯一索引 经常作为查询条件在WHERE或者ORDER BY语句中出现的列要建立索引 用于聚合函数的列可以建立索引，例如使用了max(column_1)或者count(column_1)时的column_1就需要建立索引 查询中与其他表关联的字段，外键关系建立索引 高并发条件下倾向组合索引 注意: LIKE操作中，’%aaa%’不会使用索引，也就是索引会失效，但是‘aaa%’可以使用索引。 锁先来学习两个概念： 脏读：读到其他事务没有commit的数据； 幻读：在一次事务里面，多次查询之后，结果集的个数不一致的情况叫做幻读。而多或者少的那一行被叫做幻行。 根据加锁的范围，可以分为： 全局锁全局锁会把整个数据库实例加锁，命令为 flush tables withs read lock ，将使数据库处于只读状态，其他数据写入和修改表结构等语句会阻塞，一般在备库上做全局备份使用。 表级锁 表锁，命令为 lock table with read/write ，和读写锁一样。 元数据锁，也叫意向锁，不需要显示申明，当执行修改表结构，加索引的时候会自动加元数据写锁，对表进行增删改查的时候会加元数据读锁。 如果一条SQL语句用不到索引就不会使用行级锁的，会使用表级锁。 行锁通过索引条件检索数据，InnoDB才使用行级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。 还有一种行级锁称为间隙锁，他锁定的是两条记录之间的间隙，防止其他事务往这个间隙插入数据，间隙锁是隐式锁，是存储引擎自己加上的。 备份冷备需要停机做备份，该方式最稳妥，但是对业务影响很大，很多环境无法具备冷备条件。 热备也就是在线备份数据，这里又分为几种情况: 裸文件备份（XtraBackup）这种方式下，mysql的文件会被整体备份。XtraBackup主要使用了MySQL redolog的特征来实现该功能。 XtraBackup复制InnoDB 数据文件，这会导致内部不一致的数据，但是它会对文件执行崩溃恢复，以使其再次成为一个一致的可用数据库。 这样做是可行的，因为InnoDB维护一个 REDO日志，也称为事务日志。REDO日志包含了InnoDB数据每次更改的记录。当InnoDB启动时，REDO日志会检查数据文件和事务日志，并执行两个步骤。它将已提交的事务日志条目应用于数据文件，并对任何修改了数据但未提交的事务执行undo操作。 Percona XtraBackup会在启动时记住日志序列号(LSN)，然后复制数据文件。这需要一些时间来完成，如果文件正在改变，那么它们会在不同的时间点反映数据库的状态。同时，Percona XtraBackup运行一个后台进程，用于监视事务日志文件，并从中复制更改。PerconaXtra backup需要持续这样做，因为事务日志是以循环方式写人的，并且可以在一段时间后重新使用。 Percona XtraBackup开始执行后，需要复制每次数据文件更改对应的事务日志记录。 逻辑备份逻辑备份不是存mysql文件，而是将数据导出来存储。下面工具都可以用来做逻辑备份：mysqldump, mysqldumper, select ... into outfile。 数据复制GTID 引入 整个集群架构的节点,通常是master在工作,其他两个结点做备份。各个节点的机器,性能不可能完全一致,所以,在做备份时,备份的速度就不一样。当 master突然宕掉之后,马上会启用从节点机器,接管 master 的工作。当有多个从节点时,选择备份日志文件最接近 master的那个节点。现在就出现问题了,当salve1 变成主节点, 那slave2就应该从slave1 的什么位置开始同步数据呢？ 有了GTID全局的,将所有节点对于同一个event的标记完全一致,当master宕掉之后,slave2根据同一个GTID直接去读取slave1的日志文件,继续同步。 GTID原理 全局事务ID的格式： GTID=server_id:transaction_id transaction_id 是顺序化的序列号(sequence number)，在每台 MySQL 服务器上都是从1开始自增长的序列，是事务的唯一标识。 Master: gtid_next 是默认的 AUTOMATIC,即 GTID 在每次事务提交时自动生成。它从当前已执行的 GTID 集合(即 gtid_executed)中，找一个大于 0 的未使用的最小值作为下个事务 GTID。同时将 GTID 写入到 binlog(set gtid_next 记录)，在实际的更新事务记录之前。 Slave: 从 binlog 先读取到主库的 GTID(即 set gtid_next 记录)，而后执行的事务采用该 GTID。 异步复制这是MySQL最传统的复制方式，由主库负责binlog的线程将binlog发送给从库，主库不会去验证Binlog有没有成功复制到从库。 异步复制中，如果主库提交一个事务并写入Binlog中后，当从库还没有从主库得到Binlog时，主库宕机了或因磁盘损坏等故障导致该事务的Binlog丢失了，那从库就不会得到这个事务，也就造成了主从数据的不一致。 半同步复制为解决异步复制可能会丢事务的缺陷，半同步复制中，当主库每提交一个事务后，不会立即返回，而是等待任意一个从库接收到Binlog并成功写入Relay-log中才返回客户端。这就保证了一个事务至少有两份日志，一份保存在主库的Binlog，另一份保存在其中一个从库的Relay-log中。 如果在主库推送binlog到从库的过程中，从库宕机了或网络故障，导致从库没有接收到这个事务的Binlog；主库在等待（rpl_semi_sync_master_timeout）时间后，将半同步复制切换到异步复制模式；当从库恢复正常连接到主库后，主库又会自动切换回半同步复制。 组复制组复制由多个MySQL节点组成，组中的每一个节点都能够在任意时刻独立执行事务，但是事务的提交需要得到组内所有成员的冲突检测（certification）一致通过。 假设我们要修改一条记录，当处理MySQL请求的节点修改了数据并写入内存之后，它会向组内所有节点发起广播请求（包含了原始的数据和更新的内容）。group组内的通讯是采用基于paxos协议的xcom来实现的，它的一个特性就是消息是有序传送，每个节点接收到的消息顺序都是相同的。其他节点以相同的顺序接收到请求，此时会生成一个请求的顺序队列。由于各个节点都在处理各种事物，很有可能接收队列中的两条来自不同请求节点的事务之间存在冲突，此时需要根据其消息体中的原始记录以及更新内容来做冲突检测（certification）。 一旦发现半数以上节点返回存在冲突，解决过程也非常简单，直接按照进入队列的先后顺序，对晚提交的事务不予通过即可。此时需要在发起请求的节点上执行回滚操作，其他节点丢弃该事务请求即可。 总结起来就是：冲突检测在行级处理，首先执行的一方提交成功，晚提交的一方回滚。 集群方案MHA (MMM)需要部署MHA manager，用来探测master的健康状况。当发现master挂掉后，自动将具有最新数据的slave提升为master，并将其他slave的master改为新的master。 HMA node作为agent运行在每台mysql服务器节点上，通过监控具备解析和清理日志的脚本来加快故障转移速度。该方案中，MHA管理节点本身的HA无法保证。MMM方案和MHA类似。 MM + keepalive后端主从复制，在master故障的时候由keepalive的脚本来负责切换VIP和master，并修改主从关系。 Galera节点在接收sql请求后，对于DDL操作，在commit之前，由WSREP API调用galera库进行集群内广播，所有其他节点验证成功后事务在集群所有节点进行提交，反之rollback。至少三个节点，每个节点都可读写，不共享数据，集群通过wsrep自动将写入的数据在集群节点之间做同步。需要为原生MySQL节点打wsrep补丁。 基于galera的方案有两个： Percona XtraDB cluster mariadb galera cluster 这种方案是完全的同步复制多主方案，好处：是能够完全保证数据的一致性，坏处：是整个集群的性能受限于性能最差的节点。 DRBDDRBD(DistributedReplicatedBlockDevice)是一个基于块设备级别在远程服务器直接同步和镜像数据的软件，用软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。当用户将数据写入本地磁盘时，还会将数据发送到网络中另一台主机的磁盘上，这样的本地主机(主节点)与远程主机(备节点)的数据就可以保证实时同步。 高可用软件加DRBD其实在架构上跟SAN是相同的，唯一不同的是没有使用SAN网络存储，而是使用Local Disk实时复制磁盘数据，虽然没有MySQL Replication那样主从有数据不一致性，但是DRBD实时复制数据在性能上有很大的影响，网上有人测过大概是降40%性能。 DRBD是应用级别的磁盘IO同步工具，Linux内核提供了一个钩子，每一次IO都通知到DRBD，然后DRBD将该IO同步至备机，并且返回ACK，主机才会认为该IO操作完成。所以DRBD的方案优点就是数据可以保证完全的一致，缺点则是牺牲了MySQL的扩展性以及极大的降低了磁盘IO的性能。 MGR有两种模式：单主模式single-primary mode 和 多主模式multi-primary mode。 在前面组复制的章节已经讲到其冲突检测的原理，另外还涉及到选主、加入group以及底层通信的原理，这些都是通过Paxos协议来保障的。下面是MGR plugin的功能结构图。 NDB将MySQL的存储引擎和传统MySQL服务器分离，底层使用Ndbd来提供存储，MySQL层只负责缓存、优化等工作。 管理(MGM)节点 这类节点的作用是管理MySQL集群内的其他节点，如提供配置数据、启动并停止节点、运行备份等。由于这类节点负责管理其他节点的配置，应在启动其他节点之前首先启动这类节点。 数据节点 这类节点用于保存集群的数据。数据节点的数目与副本的数目相关，是片段的倍数。例如，对于两个副本，每个副本有两个片段，那么就有4个数据节点。 SQL节点 这是用来访问集群数据的节点。对于MySQL集群，客户端节点是使用NDB集群存储引擎的传统MySQL服务器。 Ndbd节点负责存储数据，其数据的存储类似于ES中Lucene的方案；","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"mysql","slug":"mysql","permalink":"http://ljchen.net/tags/mysql/"}]},{"title":"应用可靠性保障评估","slug":"应用可靠性保障评估","date":"2019-09-19T07:05:43.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2019/09/19/应用可靠性保障评估/","link":"","permalink":"http://ljchen.net/2019/09/19/应用可靠性保障评估/","excerpt":"最近整理应用可靠性保障评估建议，吸取之前做微服务的一些经验，同时也参考了许多书籍，大致整理了一些细则。分别从可靠性、高性能、伸缩性、容错和灾备、监控以及文档化的角度来评估应用是否具有高可靠保障。大致整理如下，有需要的同学，可以参考！","text":"最近整理应用可靠性保障评估建议，吸取之前做微服务的一些经验，同时也参考了许多书籍，大致整理了一些细则。分别从可靠性、高性能、伸缩性、容错和灾备、监控以及文档化的角度来评估应用是否具有高可靠保障。大致整理如下，有需要的同学，可以参考！ 可靠性 有遵照标准化的部署流程，包括功能测试阶段、UAT测试阶段再到生产上线 有遵照应用分类做高可用性部署 两地三中心（同城多活）部署 数据中心内部应用高可用部署（接入层、应用层、持久化层） 可靠的依赖关系管理，避免依赖项失效 有清晰的服务间或外部服务依赖列表； 每个依赖项，是否都有备份、回退方案或防御性缓存； 有微服务（或模块）接口变更列表； 有稳定可靠的路由和服务发现机制 服务配置了健康检查和探活机制； 服务的健康检查结果真实可靠； 服务失效后，路由能够准确切换流量； 有使用断路器来应对反复出现异常的实例； 有服务分级与降级设计 明确的服务（模块）分级，明确的降级条件 有服务（模块）的、前端页面的降级控制开关，预先定义的降级逻辑 预定义大规模流量压力时的降级控制流程 有流量控制设计 合适的限流算法（固定窗口、漏桶、令牌） 流控策略（API网关、服务入口、中间件服务） 高性能 应用在发布前已通过压力测试 已识别出单个微服务实例（单体应用）资源需求与吞吐量的平衡点 平衡点上的资源需求 CPU 内存 外部资源（数据库连接数、文件描述符限制、日志配额等） 平衡点上的吞吐量（QPS） 高效率的服务间通信 协议（报文封装效率） 路径 方式（阻塞、非租塞） 合理使用中间件，提升读写效率 MQ Cache 数据库网关 伸缩性 使用专门的硬件，或支持资源隔离 微服务配置了严格的CPU、内存限制 单体应用使用专门的VM或者物理机 已预估流量增长规模 &lt;质&gt;总请求用户数量级，业务流量预测 &lt;量&gt;具体的QPS、RPS数 服务可伸缩 清晰的流量拓扑 服务失效后，流量是否能够路由到其他IDC 依赖项可伸缩 清晰的依赖项列表 依赖项是否可伸缩或已预留足够资源 数据存储可伸缩 数据库HA（主备切换）方案 数据分库 数据分片（分表） 容错与灾备 清晰的各层级故障域隔离策略 线程 进程 集群 用户 识别潜在故障场景，并做好故障应对方案 基础设施故障 网络故障 平台层故障（中间件、日志、监控、负载均衡等） 服务内部故障 服务依赖项故障（SLA不达标等） 识别并消除单点故障问题 提供软件和部署架构图，确定是否存在单点故障 故障点是否能被移除，或者对它们进行缓解 故障演练，通过所有计划测试流程和探索式测试 单元测试 集成测试 端到端测试 压力测试 探索式测试 有实施故障探测和制定对应补救策略 回滚（部署引起） 备份（依赖项失效） 监控告警（探测预警） 服务都配置了故障恢复策略 遵照故障的事件响应机制 监控 确定关键性度量指标，以及是否已被监控 服务级别指标 基础设施级别指标 平台级别指标 中间件级别指标 适当的日志 日志等级合理 明确定义微服务日志配额，日志保存时间 可参考的调用链指标，以及调用链监控 易理解的仪表盘 已包含关键性度量指标 能准确反映服务健康状况 有效可操作的告警 每个度量指标都设置了告警 告警分级设置合理的阀值：正常、警告、严重 告警应该可操作（若T1团队无法立即对告警采取行动，就不设告警） 有告警处理的文档（包含常见问题的缓解、排查流程） 开发人员轮班待命 文档化 最新的、集中式的文档 服务描述文档 服务对外API接口列表 服务软件、部署架构图，架构评审记录 服务处理的数据流程图 服务依赖项，依赖项监控仪表盘链接 监控关键指标，仪表盘链接，告警阀值设置信息 服务T2开发人员排班信息，联系方式 服务常见问题，以及其诊断、缓解和解决方法","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"},{"name":"reliability","slug":"reliability","permalink":"http://ljchen.net/tags/reliability/"}]},{"title":"分布式事务协议","slug":"分布式事务协议","date":"2019-08-25T01:54:39.000Z","updated":"2020-04-06T09:24:01.486Z","comments":true,"path":"2019/08/25/分布式事务协议/","link":"","permalink":"http://ljchen.net/2019/08/25/分布式事务协议/","excerpt":"分布式系统的一致性是特别伤脑筋的事情，当然也可以使用一些现成的协议，比如Raft之类的。今天我们准备来聊下两阶段提交（2PC）和三阶段提交（3PC），以及它们之间的关系。","text":"分布式系统的一致性是特别伤脑筋的事情，当然也可以使用一些现成的协议，比如Raft之类的。今天我们准备来聊下两阶段提交（2PC）和三阶段提交（3PC），以及它们之间的关系。 相对来说，我个人认为这些东西如果只是通过文字表达，若抽象思维不好，理解起来还是有些难度。于是，我画了几个图，尽量用图形的方式来展现它们的流程以及对彼此做一些比较。 预备知识在分布式系统中，我们都会采用一些分布式协调的工具来协调分布在各个地方的参与者的行为，保障事务结果的一致性。这里就有两个角色，分别是协调者和参与者。在现实中，协调者可能是你发送消息到各个系统的客户端，或者是外部中间件。 两阶段提交 先来看两阶段提交的过程，上图中包含了一个协调者和两个参与者，为了保障事务在整个系统中执行完成后的一致性，协调者分两个阶段来向各参与者提交事务。具体流程如下： prepare 协调者向各个参与者发送提交事务的请求； 参与者基于接收到的事务内容，在本地执行：校验、锁资源、执行、写redo和undo日志，但不提交; 并向协调者反馈yes/no commit/rollback 协调者在上一步中收集来自各个参与者的反馈，如果都是yes，那么就正式向各参与者发送commit请求；如果其中有的参与者反馈是no，为了整个系统的一致性，会向各参与者发送rollback请求； 参与者接收到请求后，执行log中的redo或者undo操作，然后返回结果。 这块是否很好理解呢？整个过程其实就类似于我们使用mysql的事务操作，一般是先提交一批的操作，然后在最后基于各个操作的执行情况来决定是执行commit还是rollback。只有在第二阶段命令提交之后，操作才会真正在DB中生效。 问题看起来很完美！但是，两阶段提交也存在着一些无法解决的问题，比如： prepare阶段做了锁定资源的操作；所以，要是在第一个阶段执行了之后，假设出现以下两种情况都会导致协调者无法提交第二阶段请求到参与者，那么参与者的资源就会一直被锁定； 网络中断 协调者挂掉 commit阶段的协调者到参与者的消息，有的发送成功，有的失败；这就会导致有的参与者执行了redo，另外一些什么都没处理，还在锁定资源，数据最终不一致。 三阶段提交其整个流程如下，接下来会分析为何要这样设计。 cancommit只负责对操作条件的校验，不做其它； precommit/abort负责锁资源、执行、写redo和undo日志，但不提交； docmmit/abort正常完成操作，释放资源 两阶段提交的改良三阶段提交是对两阶段提交的一种完善，为了解决前面提到的第一个问题，三阶段提交引入了两个解决方案。 解决资源锁定核心思想是：“增加流程，事前预防，胜过事后解决问题”。它将原来两阶段中的第一阶段prepare分为两个阶段： cancommit只负责对操作条件的校验，不做其他； precommit负责锁资源、执行、写redo和undo日志，但不提交； 通过上图可以清楚看到，从两阶段提交到三阶段提交，通过拆分了prepare后，可以在cancommit阶段就能最大概率快速确定该次事务是否能够执行。一旦所有参与者都返回yes后，才通知各个参与者锁定资源；这样就避免了两阶段提交中，一上来各自就开始锁定资源，然后发现某个参与者有问题后又通知各自在rollback操作中去释放资源的补救措施。 解决无法提交第二阶段请求前面已经提到，由于协调者故障或者网络异常、分片等异常，将导致commit消息无法提交到某些消费者；为了更大概率的解决该问题，在三阶段提交中，为precommit后的事务都设置了一个定时器。当在超时后没有接收到docommit消息后，参与者就会尝试正式执行操作。 当然，这种方法只是一种最大概率的保障，判断的依据是一旦参与者接收到了precommit操作之后，意味着它知道所有的参与者其实都同意修改了。所以，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 但是，如果协调者事实上在最后阶段不是提交docommit，而是abort的话，那就愿赌服输了！","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"event","slug":"event","permalink":"http://ljchen.net/tags/event/"},{"name":"2PC","slug":"2PC","permalink":"http://ljchen.net/tags/2PC/"},{"name":"3PC","slug":"3PC","permalink":"http://ljchen.net/tags/3PC/"}]},{"title":"Redis Cluster总结","slug":"Redis-Cluster总结","date":"2019-08-24T02:26:00.000Z","updated":"2020-04-06T09:24:01.468Z","comments":true,"path":"2019/08/24/Redis-Cluster总结/","link":"","permalink":"http://ljchen.net/2019/08/24/Redis-Cluster总结/","excerpt":"Redis是一个非常小而美的缓存中间件，官方早期没有集群方案的时候，社区采用哨兵的方式来做集群。官方在3.0版本后开始支持集群方案，但是依然需要至少6台节点，那处于对各种中间件研究的兴趣，我们就来聊聊Redis Cluster希望解决的问题，以及其实现原理。","text":"Redis是一个非常小而美的缓存中间件，官方早期没有集群方案的时候，社区采用哨兵的方式来做集群。官方在3.0版本后开始支持集群方案，但是依然需要至少6台节点，那处于对各种中间件研究的兴趣，我们就来聊聊Redis Cluster希望解决的问题，以及其实现原理。 梗概 Redis集群的方案解决的终极问题无非三个，大致总结如下： 提高整体的可用性 横向看，每个集群的master节点都对应有slaves，master挂掉后，新master会从slaves中选举出来； 纵向看，每个集群可以手动扩容和缩容，添加减少master节点的数量； 支持更大规模的容量(key) 采用分片的思想，两层抽象；key被分布到固定数量的slots中，集群masters各服务一部分slots，横向扩容master节点，可以支持更大容量的key； 提高整体的吞吐量和性能 分片后的，客户端直接计算出key对应slot所在的master节点，直接访问目标master，实现了分布式，从而提高集群整体吞吐量； 集群原理通过对于问题和解决方式的简单描述，相信大家已经迫不及待的想要知道整个集群的实现原理了。这一章节，就分别来介绍各自的实现。 数据分片寻址方式需要注意的是，redis cluster并没有使用一致性哈希来计算key对应的节点，而是通过记录一张映射表 hash slots的方式。由于分层两层结构，从key到slotId这一步还是使用了哈希算法的。 哈希算法（或手动指定）为了提高客户端快速找到key所在的slot，这里采用了哈希的方式，计算公式是: slotId = crc16(key) % 16384。 当然，系统也提供了通过手动的方式来为每一个key指定一个固定的slotId，通过该方式指定的key，在读取的时候也需要指定之前指定的slotId, 否则客户端会默认通过CRC16去计算slotId，导致找不到数据。具体如下： 12345&gt; set mykey1:&#123;100&#125; 1&gt; set mykey2 2 (error) MOVED 14119 192.168.99.172:7005&gt; get mykey2:&#123;100&#125; \"2\" hash slots通过上一步计算得到的slotId快速定位到节点，这里是通过记录一张映射表（slotId-&gt;nodeId）的方式来实现的。节点之间通过gossip协议来广播这个关系，可以迅速收敛。 如果客户端将不属于该node的一个slot请求发送到该node上，redis会通过查表，快速找到该slot请求本应该去的node，然后向client返回MOVED消息。 客户端实现单节点redis和redis集群的客户端的实现方式不一样，redis集群客户端的大致处理流程如下: 初始化和正常访问 客户端初始化，随机选择一个node通信，获取hashslot-&gt;node映射表和nodes信息; 向cluster中所有node建立连接，并为每个node创建一个连接池; 发送请求的时候，先在本地计算key的hashslot，再在本地映射表找到对应node; 若目标node正好持有那个hashslot，那么正常处理. Slot正在迁移 若客户端请求过来是，目标node正在迁移slot，就会返回ASK重定向给客户端（包含了重定向目标node）; 客户端基于节点信息，向重定向node请求key；但是由于slot正在迁移，客户端不更新新本地的hashslot-&gt;node映射表; 正在接受迁移的node接收到请求，查询key信息，并返回结果. Slot已迁移 若经过reshard或者slot迁移，hashslot已经不在目标node，就会返回MOVED; 客户端收到node返回MOVED，就更新本地的hashslot-&gt;node映射表; 客户端基于moved node信息请求key。 分片局限性由于集群采用了将slot分片的方式，因此一些单机版的功能无法适用在集群中。下面是典型的场景： key的批量操作支持有限； 事务操作支持有限； 大的键值对象，如list，hash不能映射到不同节点，只能基于key来映射； 只能支持一个db； slave的复制只允许有一层； 高可用性高可用性是集群要达到的一大目标，接下来分别从集群的副本、容错、扩展性几个方面来做介绍。 副本与容错在Redis集群方案中，每一个master都有对应的slave，当master节点故障的时候，集群能够从备选的slave中选举出新的master节点，从而继续对master上的slots提供对外的服务能力。 节点故障下线 通过从Gossip消息中获取到由其他节点判断到的，主观下线的节点列表（需要判断来源是否为主节点，如果一个节点认为某个节点主观下线了，那么会在Gossip ping消息中，ping给其他节点）； 基于该信息，更新本地的clusterNodes下线列表； 基于本地的下线列表，尝试对目标节点进行客观下线；（如果超过半数的节点都认为其主观下线了，就会变成客观下线） 向其他节点通告目标节点客观下线消息；（通知其他节点故障节点立即下线，通知slave节点切换为master）。 故障转移 slave节点在感知到master节点挂了之后，该master的所有slave节点会参与选举投票，选举胜出的节点会变为新的master，对外提供服务。 选举流程 确认选举资格，没有资格的slave不能参与选举；判断slave节点与主节点失联的时间是否超过指定的时间（每一个从节点都可以设置一个因子来决定其切换的优先级）； 准备选举时间（延迟触发），每个节点的时间受offset的影响，越接近master的offset越快； 投票选举，集群中的主节点对发起投票申请的slave进行投票； slave如果获取到绝大多数master的投票，就切换为master； slave角色切换 取消slaveof，并变为master 使用clusterDelSlot取消之前的master上的slot，使用clusterAddSlot将slot委派给自己； 向集群通告自己变为master 一致性Redis集群中的epoch有两种：currentEpoch 和 configEpoch。 currentEpoch（整个集群）currentEpoch代表了整个集群的拓扑版本信息。初始化的时候从0开始，如果节点接收到来自其他节点的包，发送者的currentEpoch大于当前节点的currentEpoch，当前节点就更新 currentEpoch为发送者的currentEpoch。当前currentEpoch只用于slave的故障转移流程。 slave发现其master下线，就会试图发起故障转移流程。首先增加currentEpoch的值，然后向所有节点发起拉票请求。 其他节点收到包后，发现发送者的currentEpoch比自己的大，就会更新自己的currentEpoch，并投票。 configEpoch（单个分片内 master+slaves）configEpoch代表了单个节点配置（所负责的slots）的版本信息。master对外通告其所负责的slots列表，会对应一个configEpoch，所有slave的configEpoch与之一致。一个节点如果收到两个master同时宣称冲突的slot映射消息，就会分别判断两个消息中的configEpoch，并相信更大configEpoch消息中的映射关系。 slave发起选举，成功当选后，会试图替代其已经下线的旧master slave增加它自己的configEpoch，使其成为当前所有集群节点的configEpoch中的最大值 slave向所有节点发送广播包，强制其他节点更新相关slots的负责节点为自己 扩展性即修改master节点的数量，一般是因为机器数量不够（缩容），或者是希望基于实际业务情况做对外吞吐量的响应调整。需要使用命令： CLUSTER SETSLOT &lt;slot&gt; (importing|migrating|stable|node &lt;node-id&gt;) 其目的是将slot从一个节点拷贝到另一个节点，具体命令如下： 在目标节点执行：cluster setslot {slot} importing {sourceNodeId} 在源节点执行：cluster setslot {slot} migrating {targetNodeId} 常见问题 穿透缓存空值、使用布隆过滤器; 无底洞问题分布式缓存中，更多的机器并不一定代表更高的性能； 雪崩问题缓存层高可用、客户端降级； 节点迁移 如果节点没有importing flag, 它会直接设置槽位, 但不会增加自己的node epoch。这样当他告诉别的节点对这个槽位的所有权时, 其他节点并不认可。所以实在要在迁移slot以外的地方用这个命令, 必须要给它发一次cluster bumpepoch。 创建与运维集群的创建和维护，包含集群握手、slave节点添加、节点slot迁移等操作，具体对应下面三种场景: 集群建立 集群节点之间，通过使用服务端口+10000的端口，基于Gossip协议通信握手；握手执行命令: cluster meet {ip} {port}可以通过命令: cluster nodes或者cluster info查看 分配slot到新加的节点在目标节点上执行命令: cluster addslots {0..12345} 集群副本 设置从节点在slave节点上执行: cluster replicate {master-nodeid}, 指定其属于哪一个master节点的slave 集群扩容 先通过cluster meet将node加入集群 在新加入的（迁移目标）节点执行：cluster setslot {slot} importing {sourceNodeId} 在原有的源节点执行：cluster setslot {slot} migrating {targetNodeId} 在源节点循环执行：cluster getkeysinslot {slot} {count}， count为slot的key数量 向其他节点执行：cluster setslot {slot} node {targetNodeId}，更新节点的slot映射列表 调试Keydebug object &lt;key&gt; 监控面板 1docker run --name redis-stat -p 8080:63790 -d insready/redis-stat --server &#123;ip-1:port&#125; &#123;ip-2:port&#125; &#123;ip-3:port&#125; 配置信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245[root@vm ~]# redis-cli127.0.0.1:6379&gt; info# Serverredis_version:3.2.12redis_git_sha1:00000000redis_git_dirty:0redis_build_id:7897e7d0e13773fredis_mode:standaloneos:Linux 3.10.0-693.2.2.el7.x86_64 x86_64arch_bits:64multiplexing_api:epollgcc_version:4.8.5process_id:2048run_id:908bef9f0253572db49ea248a172b4842d81ed35tcp_port:6379uptime_in_seconds:2781uptime_in_days:0hz:10lru_clock:6028016executable:/usr/bin/redis-serverconfig_file:/etc/redis.conf# Clientsconnected_clients:3client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:2911368used_memory_human:2.78Mused_memory_rss:5410816used_memory_rss_human:5.16Mused_memory_peak:61110496used_memory_peak_human:58.28Mtotal_system_memory:1928695808total_system_memory_human:1.80Gused_memory_lua:37888used_memory_lua_human:37.00Kmaxmemory:0maxmemory_human:0Bmaxmemory_policy:noevictionmem_fragmentation_ratio:1.86mem_allocator:jemalloc-3.6.0# Persistenceloading:0rdb_changes_since_last_save:0rdb_bgsave_in_progress:0rdb_last_save_time:1566308627rdb_last_bgsave_status:okrdb_last_bgsave_time_sec:0rdb_current_bgsave_time_sec:-1aof_enabled:0aof_rewrite_in_progress:0aof_rewrite_scheduled:0aof_last_rewrite_time_sec:-1aof_current_rewrite_time_sec:-1aof_last_bgrewrite_status:okaof_last_write_status:ok# Statstotal_connections_received:18370total_commands_processed:3600032instantaneous_ops_per_sec:0total_net_input_bytes:192800920total_net_output_bytes:2634507410instantaneous_input_kbps:0.01instantaneous_output_kbps:6.06rejected_connections:0sync_full:2sync_partial_ok:0sync_partial_err:0expired_keys:0evicted_keys:0keyspace_hits:1000004keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:300migrate_cached_sockets:0# Replicationrole:masterconnected_slaves:0master_repl_offset:131203668repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:130155093repl_backlog_histlen:1048576# CPUused_cpu_sys:24.04used_cpu_user:31.68used_cpu_sys_children:0.01used_cpu_user_children:0.01# Clustercluster_enabled:0# Keyspacedb0:keys=6,expires=0,avg_ttl=0127.0.0.1:6379&gt; config get *1) \"dbfilename\"2) \"dump.rdb\"3) \"requirepass\"4) \"\"5) \"masterauth\"6) \"\"7) \"unixsocket\"8) \"\"9) \"logfile\"10) \"/var/log/redis/redis.log\"11) \"pidfile\"12) \"/var/run/redis_6379.pid\"13) \"slave-announce-ip\"14) \"\"15) \"maxmemory\"16) \"0\"17) \"maxmemory-samples\"18) \"5\"19) \"timeout\"20) \"0\"21) \"auto-aof-rewrite-percentage\"22) \"100\"23) \"auto-aof-rewrite-min-size\"24) \"67108864\"25) \"hash-max-ziplist-entries\"26) \"512\"27) \"hash-max-ziplist-value\"28) \"64\"29) \"list-max-ziplist-size\"30) \"-2\"31) \"list-compress-depth\"32) \"0\"33) \"set-max-intset-entries\"34) \"512\"35) \"zset-max-ziplist-entries\"36) \"128\"37) \"zset-max-ziplist-value\"38) \"64\"39) \"hll-sparse-max-bytes\"40) \"3000\"41) \"lua-time-limit\"42) \"5000\"43) \"slowlog-log-slower-than\"44) \"20000\"45) \"latency-monitor-threshold\"46) \"0\"47) \"slowlog-max-len\"48) \"1000\"49) \"port\"50) \"6379\"51) \"tcp-backlog\"52) \"511\"53) \"databases\"54) \"16\"55) \"repl-ping-slave-period\"56) \"10\"57) \"repl-timeout\"58) \"60\"59) \"repl-backlog-size\"60) \"1048576\"61) \"repl-backlog-ttl\"62) \"3600\"63) \"maxclients\"64) \"10000\"65) \"watchdog-period\"66) \"0\"67) \"slave-priority\"68) \"100\"69) \"slave-announce-port\"70) \"0\"71) \"min-slaves-to-write\"72) \"0\"73) \"min-slaves-max-lag\"74) \"10\"75) \"hz\"76) \"10\"77) \"cluster-node-timeout\"78) \"15000\"79) \"cluster-migration-barrier\"80) \"1\"81) \"cluster-slave-validity-factor\"82) \"10\"83) \"repl-diskless-sync-delay\"84) \"5\"85) \"tcp-keepalive\"86) \"300\"87) \"cluster-require-full-coverage\"88) \"yes\"89) \"no-appendfsync-on-rewrite\"90) \"no\"91) \"slave-serve-stale-data\"92) \"yes\"93) \"slave-read-only\"94) \"yes\"95) \"stop-writes-on-bgsave-error\"96) \"yes\"97) \"daemonize\"98) \"no\"99) \"rdbcompression\"100) \"yes\"101) \"rdbchecksum\"102) \"yes\"103) \"activerehashing\"104) \"yes\"105) \"protected-mode\"106) \"yes\"107) \"repl-disable-tcp-nodelay\"108) \"no\"109) \"repl-diskless-sync\"110) \"no\"111) \"aof-rewrite-incremental-fsync\"112) \"yes\"113) \"aof-load-truncated\"114) \"yes\"115) \"maxmemory-policy\"116) \"noeviction\"117) \"loglevel\"118) \"notice\"119) \"supervised\"120) \"systemd\"121) \"appendfsync\"122) \"everysec\"123) \"syslog-facility\"124) \"local0\"125) \"appendonly\"126) \"no\"127) \"dir\"128) \"/var/lib/redis\"129) \"save\"130) \"900 1 300 10 60 10000\"131) \"client-output-buffer-limit\"132) \"normal 0 0 0 slave 268435456 67108864 60 pubsub 33554432 8388608 60\"133) \"unixsocketperm\"134) \"0\"135) \"slaveof\"136) \"\"137) \"notify-keyspace-events\"138) \"\"139) \"bind\"140) \"127.0.0.1\"","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"redis","slug":"redis","permalink":"http://ljchen.net/tags/redis/"},{"name":"redis-cluster","slug":"redis-cluster","permalink":"http://ljchen.net/tags/redis-cluster/"},{"name":"cache","slug":"cache","permalink":"http://ljchen.net/tags/cache/"}]},{"title":"CRD Operator要点","slug":"CRD-operator要点","date":"2019-07-24T14:05:31.000Z","updated":"2020-04-06T09:24:01.465Z","comments":true,"path":"2019/07/24/CRD-operator要点/","link":"","permalink":"http://ljchen.net/2019/07/24/CRD-operator要点/","excerpt":"头阵子基于kubebuilder实现了一套用于大数据交换平台底层调度的operator。在过程中遇到一些坑，平时学习的时候不太容易注意到的点，简单记录一下。","text":"头阵子基于kubebuilder实现了一套用于大数据交换平台底层调度的operator。在过程中遇到一些坑，平时学习的时候不太容易注意到的点，简单记录一下。 要点多个CRD级联删除我自定义了一个CRD PipelineRun，基于PipelineRun的逻辑会自动创建其子资源deployment。我希望在删除PipelineRun的时候，能够自动删除其对应的所有deployment实例。 这里只需要在PipelineRun创建deployment的时候，指定deployment的ObjectMet.OwnerReferences的值为PipelineRun自己即可。具体见: 1234567891011121314151617181920func MakeDeploysAndServices(pipeline *v1.Pipeline, run *v1.PipelineRun) ([]appsv1.Deployment, []corev1.Service) &#123; ...... // deployment deployment := appsv1.Deployment&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: makeDeployName(pipeline.Name, svc.Name), Namespace: pipeline.Namespace, Labels: podLabels, Annotations: CopyMap(pipeline.GetAnnotations()), OwnerReferences: []metav1.OwnerReference&#123; // 指定其OwnerReferences为run的属性 *metav1.NewControllerRef(run.GetObjectMeta(), run.GroupVersionKind()), &#125;, &#125;, Spec: appsv1.DeploymentSpec&#123; ...... &#125;, &#125; ...... return deployments, services&#125; 订阅删除前的事件如果在etcd中已经删除了资源后operator才watch到该事件，此时由于资源已经不复存在，很多逻辑操作无法得到足够的参数来执行处理。因此，因此我们需要的是在执行最终删除之前就能够watch到该事件，并执行一些销毁资源的操作。好在k8s api-server已经为我们提供了finalizer机制。 如果某个资源的finalizers不为空，当执行删除之前，会被operator watch到操作。此时，其meta.DeletionTimestamp不为null，对应operator应该在该次事件的handler中删除掉其注册上来的finalizer对象；并执行其他业务逻辑handler。 finalizer定义方式如下：123456789apiVersion: controller.xxx.cn/v1kind: Pipelinemetadata: name: pipeline-test-1 finalizers: # 可以指定多个finalizers数组 - finalizer.xxx.cnspec: batchJob: ...... 以下是reconcile的一个示例: 12345678910111213141516171819func (r *PipelineReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) &#123; ...... // delete if p.ObjectMeta.GetDeletionTimestamp() != nil &#123; r.CronHandler.release(p) if result := resources.RemoveFinalizer(p, CustomFinalizer); result == resources.FinalizerRemoved &#123; if err := r.Update(ctx, p); err != nil &#123; r.Log.Error(err, \"Failed to remove finalizer\") return reconcile.Result&#123;Requeue: true&#125;, nil &#125; &#125; return ctrl.Result&#123;&#125;, nil &#125; ...... return ctrl.Result&#123;&#125;, nil&#125; 处理事件风暴因为k8s operator本来就是一个循环，即: 当资源变化时，operator的reconcile会被调用。如果此时在代码中又update了资源，那么对资源的update操作又会触发下一轮reconcile。如果在reconcile中没做好基于状态来终结循环的逻辑，循环就会无休止的进行，产生事件风暴。 假设你实现的CRD A的状态依赖于其子资源B的状态，在未走到最终状态之前，operator需要不断读取B的状态来同步给A。想想这个要如何实现？ 既然k8s operator的机制本身就是一个循环，因此，我们可以利用这种循环来不断读取子资源的状态，并同步给A。这个循环的控制逻辑需要满足： 在B的状态未到最终状态时，循环必须一直执行下去； 在B进入不可迁移状态时，A的operator需要同步B的状态，并终结循环。 但是，这里依然存在一个问题！在B处于中间状态的时候，A的operator就一直循环，处于时间风暴中浪费资源吗？其实kubebuilder为我们提供了解决方案。 12345678910111213141516func (r *PipelineRunReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) &#123; ...... //update status if equality.Semantic.DeepEqual(originState, run.Status) &#123; logs.Debug(\"[pipelineRun] status are equal... \", run.Namespace, run.Name) return reconcile.Result&#123;RequeueAfter: SyncBuildStatusInterval&#125;, nil &#125; // 除非有特殊需求，否则劲量在Reconcile层更新状态（少在更深层逻辑中更新状态） if err := r.Status().Update(ctx, run); err != nil &#123; logs.Error(\"failed to update pipelineRun status, err: %s\", err.Error()) return reconcile.Result&#123;&#125;, err &#125; return ctrl.Result&#123;&#125;, nil&#125; 上面的reconcile.Result{RequeueAfter: SyncBuildStatusInterval}不会执行到下面的状态更新操作，而是直接返回。operator会将该资源变动的event重新放入队列，然后等到RequeueAfter参数指定的时间间隔之后重新取出来再调用reconcile处理。这样的优点是，到达的效果一样，但不会频繁的写etcd，从而保障k8s集群不受影响。 利弊另外，kubebuilder自动生成了operator的代码框架，同时生成CRD的yaml文件，这减少了不少工作量，但是它不生成client侧的代码。因此，这也比较坑，需要重新基于CRD类型文件来生成代码。 项目介绍项目名称: pipeline-operator项目代码: https://github.com/chenleji/pipeline-operator","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"operator","slug":"operator","permalink":"http://ljchen.net/tags/operator/"},{"name":"CRD","slug":"CRD","permalink":"http://ljchen.net/tags/CRD/"}]},{"title":"kubeedge之EdgeSite","slug":"kubeedge之EdgeSite","date":"2019-07-22T13:05:54.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2019/07/22/kubeedge之EdgeSite/","link":"","permalink":"http://ljchen.net/2019/07/22/kubeedge之EdgeSite/","excerpt":"kubeedge默认在云端部署edgeController和deviceController，然后通过websocket/quic隧道连接云端和边缘端，通过云端一个中心来统一调度应用到特定edge node上运行。但是，就像Rancher K3S的应用场景，有些时候边缘端也希望运行一套完整的k8s集群。K3S的方案只是提供了一套精简的k8s集群，而kubeedge的edgesite模式，除了运行k8s集群之外，还提供了对IOT设备的适配和支持。","text":"kubeedge默认在云端部署edgeController和deviceController，然后通过websocket/quic隧道连接云端和边缘端，通过云端一个中心来统一调度应用到特定edge node上运行。但是，就像Rancher K3S的应用场景，有些时候边缘端也希望运行一套完整的k8s集群。K3S的方案只是提供了一套精简的k8s集群，而kubeedge的edgesite模式，除了运行k8s集群之外，还提供了对IOT设备的适配和支持。 架构 如上图所示，edgesite与传统的部署模式的差异在于edgeController与edged等之前边缘端的组件部署在一起；另外，k8s master虽然与下边的组件组没有画在一起，但是它们也被部署在边缘端。所以，没有了cloudHub和edgeHub之间基于隧道的通信。 这里可以看到，k8s master，也就是上面部分几乎是原生的k8s，不需要做任何改动；而下面部署就是kubeedge除cloudHub和edgeHub之外的所有组件。那代码能够直接复用？这在最后原理部分再来分析。 实验这种架构，特别适合于已部署了一个k8s server端，在本地开发调试kubeedge组件的情况。在代码中加一些调试信息，本地快速编译，然后执行查看，方便快速理解代码逻辑。这里，我就要演示一把如何在本地运行kubeedge来对接到远端的k8s集群。 云端我依然是使用在aliyun-vm上部署好的k3s集群，唯一需要注意的是，我在启动命令行中指定了参数，让api-server暴露了insecure-port 8080（这样本地就不需要指定各种证书信息了，当然为了安全，你也可以严格使用证书），具体如下: 123456789101112~ $ cat /etc/systemd/system/k3s.service[Unit]Description=Lightweight KubernetesDocumentation=https://k3s.ioAfter=network-online.target[Service]...ExecStart=/usr/local/bin/k3s \\ server \\ --kube-apiserver-arg insecure-bind-address=0.0.0.0 \\ # 指定监听地址 --kube-apiserver-arg insecure-port=8080 # 指定insecure-port 等待k3s启动之后，通过下面的node.yaml文件在k3s上kubectl apply -f node.yaml添加节点，注意节点的名称，后续edge端的配置文件需要设置为该名字。 1234567apiVersion: v1kind: Nodemetadata: labels: name: edge-site node-role.kubernetes.io/edge: \"\" name: edge-site 边缘端也就是我的本地笔记本，在kubeedge源码包中，有edgesite目录。需要进入到cmd目录中编译代码（这需要golang环境，这块不再介绍）、准备配置文件，并执行。 1234567891011~ $ lsMakefile README.md cmd conf~ $ cd cmd# 如果是golang 1.12版本，需要手动关闭go mod模式，因为该项目是基于go vendor的~ $ GO111MODULE=off go build # 从上级目录复制conf目录中的配置文件到cmd目录中，并修改配置~ $ cp -rf ../conf ./~ $ ls confedgeSite.yaml logging.yaml modules.yaml 这里只需要修改edgeSite.yaml, 具体配置修改如下:1234567891011121314151617181920212223242526272829303132333435363738394041mqtt: server: tcp://127.0.0.1:1883 # external mqtt broker url. internal-server: tcp://127.0.0.1:1884 # internal mqtt broker url. mode: 0 # 0: internal mqtt broker enable only. 1: internal and external mqtt broker enable. 2: external mqtt broker enable only. qos: 0 # 0: QOSAtMostOnce, 1: QOSAtLeastOnce, 2: QOSExactlyOnce. retain: false # if the flag set true, server will store the message and can be delivered to future subscribers. session-queue-size: 100 # A size of how many sessions will be handled. default to 100.controller: kube: master: http://$&#123;k3s-api-server&#125;:8080 # 这里是k3s的insecure-port的访问URL namespace: \"default\" content_type: \"application/vnd.kubernetes.protobuf\" qps: 5 burst: 10 node_update_frequency: 10 node-id: edge-site node-name: edge-site # 该名称为预先在k8s上创建的node名称 context: send-module: metaManager receive-module: controller response-module: metaManageredged: register-node-namespace: default hostname-override: edge-site interface-name: eth0 node-status-update-frequency: 10 # second device-plugin-enabled: false gpu-plugin-enabled: false image-gc-high-threshold: 80 # percent image-gc-low-threshold: 40 # percent maximum-dead-containers-per-container: 1 docker-address: unix:///var/run/docker.sock runtime-type: docker version: 2.0.0metamanager: context-send-group: controller context-send-module: controller edgesite: true 然后就在cmd目录下直接运行生成的cmd二进制文件了（记得要使用sudo，因为需要创建pods目录）。 1~ $ sudo ./cmd 原理前面有讲到，从kubeedge的云边模式，切换到纯边缘部署，代码方面是否可以复用？答案是肯定的，这块kubeedge设计的非常好。要理解这块的原理，需要理解beehive的消息转发。 模块与消息 模块 module name group name deviceTwin twin twin edged edged edged cloudHub cloudhub cloudhub edgeHub websocket hub eventbus eventbus bus servicebus servicebus bus metaManager metaManager meta edgeController controller controller deviceController devicecontroller controller 模块之间通信的每一个消息都有如下关键组成： 消息头信息 消息ID 消息父ID（请求的返回消息也有同样的该ID） 时间戳（消息创建时间） 同步消息标识 消息路由信息 消息来源模块 消息目的广播组 操作 操作资源名称 在之前的架构中，metaManager的消息发送到controller需要经过edgeHub再到cloudHub，然后才能够送到controller。而当前在edgesite中直接取消掉了edgeHub和cloudHub，也就意味着，controller现在是直接将消息送到metaManager。 我们在来看一段controller读取配置文件时候的代码: 12345678910111213141516171819202122232425262728const( DefaultContextSendModuleName = \"cloudhub\" DefaultContextReceiveModuleName = \"controller\" DefaultContextResponseModuleName = \"cloudhub\")func init() &#123; if smn, err := config.CONFIG.GetValue(\"controller.context.send-module\").ToString(); err != nil &#123; ContextSendModule = constants.DefaultContextSendModuleName // cloudhub &#125; else &#123; ContextSendModule = smn &#125; log.LOGGER.Infof(\" send module name: %s\", ContextSendModule) if rmn, err := config.CONFIG.GetValue(\"controller.context.receive-module\").ToString(); err != nil &#123; ContextReceiveModule = constants.DefaultContextReceiveModuleName // controller &#125; else &#123; ContextReceiveModule = rmn &#125; log.LOGGER.Infof(\"receive module name: %s\", ContextReceiveModule) if rmn, err := config.CONFIG.GetValue(\"controller.context.response-module\").ToString(); err != nil &#123; ContextResponseModule = constants.DefaultContextResponseModuleName // cloudhub &#125; else &#123; ContextResponseModule = rmn &#125; log.LOGGER.Infof(\"response module name: %s\", ContextResponseModule)&#125; 云边模式的情况下使用默认配置，也就是controller发送消息以及发送响应都是往cloudHub发送，而接收都是订阅模块名为controller的消息。在edgesite模式下，配置文件有所变化，我截取edgeSite.yaml的关键部分： 12345678910controller: kube: ...... context: send-module: metaManager # controller消息不再中转，直接发送到metaManager receive-module: controller # 订阅controller模块消息 response-module: metaManager # response消息也一样metamanager: context-send-group: controller # 边缘端消息发送到controller group context-send-module: controller # 边缘端消息发送到controller module 至此，我们当值了解了edgesite的大概架构设计，适用场景，以及其代码的巧妙设计。当然，kubeedge还有更多目标，比如对serverless以及serviceMesh的支持等，待后续再慢慢分解。","categories":[{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/categories/edge-compute/"}],"tags":[{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"kubeedge","slug":"kubeedge","permalink":"http://ljchen.net/tags/kubeedge/"},{"name":"edgesite","slug":"edgesite","permalink":"http://ljchen.net/tags/edgesite/"},{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/tags/edge-compute/"}]},{"title":"Kubeedge实现原理","slug":"kubeedge实现原理","date":"2019-07-21T01:27:28.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2019/07/21/kubeedge实现原理/","link":"","permalink":"http://ljchen.net/2019/07/21/kubeedge实现原理/","excerpt":"可能由于各自的定位不同，K3S更像是一个kubernetes厂商的一个发行版，在边缘计算方面其实是没有摄入细节的。相比起来，kubeedge目标更明确，除了在kubernetes的方面做了各种异步通信通道，保障offline后的业务连续性之外；还定义了一系列的设备抽象，用来管理边缘设备。而且，其v1.0版本正朝着边缘端服务网格，以及函数式计算等方向发展。","text":"可能由于各自的定位不同，K3S更像是一个kubernetes厂商的一个发行版，在边缘计算方面其实是没有摄入细节的。相比起来，kubeedge目标更明确，除了在kubernetes的方面做了各种异步通信通道，保障offline后的业务连续性之外；还定义了一系列的设备抽象，用来管理边缘设备。而且，其v1.0版本正朝着边缘端服务网格，以及函数式计算等方向发展。 也是为了切入这个方向，真正理解kubedge的目标和定位，我专门在华为云上申请开通了还在公测阶段的IEF服务。总体感觉是基本功能可用，但是更深入的功能有待开发，比如函数式计算这块貌似只有界面，但是使用手册和功能都缺失等。 这里记录一下最近学习kubeedge v1.0分支代码的一些理解，便于后续快速查看。其实华为在该项目中文档还算写的比较完备的，官方文档可以参考这里。 架构 整体架构图比较明了，在不考虑edgesite的情况下，其架构分为了云端和边缘端。其实可以理解为kubernetes的管理侧和kubelet节点侧（对应edge端）。我记得很多年前rancher就采用这种架构，通过隧道网络通过一个机头来统一纳管位于多个云服务提供商上的计算节点。但是请注意，这里的场景是边缘计算，意味着edge端的网络环境难以保障。 云边通信于是就衍生出了cloud端的cloud Hub与edge端的Edge Hub。这两个模块之间通过websocket或者quic通信，相当于建立了一条底层通信隧道，供k8s和其他应用通信。当然，使用什么协议通信不是重点，重点是如何保障当着之间的链路都无法保障的时候，业务不受到影响，这就是MetaManager的要解决的问题了。 CloudHub前面提到cloud端的cloudHub就是一个隧道的server端，用于大量的edge端基于websocket或者quic协议连接上来；没错，这货才是正儿八经的二传手，每天就负责拉皮条。 EdgeHub位于edge端运行，是隧道的client端，负责将接收到的信息转发到各edge端的模块处理；同时将来自个edge端模块的消息通过隧道发送到cloud端。 边缘端 MetaManagerMetaManager模块后端对应一个本地的数据库（sqlLite），所有其他模块需要与cloud端通信的内容都会被保存到本地DB种一份，当需要查询数据时，如果本地DB中存在该数据，就会从本地获取，这样就避免了与cloud端之间频繁的网络交互；同时，在网络中断的情况下，本地的缓存的数据也能够保障其稳定运行（比如你的智能汽车进入到没有无线信号的隧道中），在通信恢复之后，重新同步数据。 Edged之前提到过kubernetes的kubelet，它相当于k8s的核心。这块其实简单做了一些裁剪，去掉一些用不上的功能，然后就成为Edged模块，该模块就是保障cloud端下发的pod以及其对应的各种配置、存储（后续会支持函数式计算）能够在edge端稳定运行，并在异常之后提供自动检测、故障恢复等能力。当然，由于k8s本身运行时的发展，该模块对应支持各种CRI应该也比较容易。 EventBus/ServiceBus/Mappper前面讲到的模块都与k8s直接或间接相关，接下来说下与设备（或者说真正IOT业务）相关的设备管理侧。外部设备的接入当前支持MQTT和Rest-API，这里分别对应EventBus和ServiceBus。EventBus就是一个MQTT broker的客户端，主要功能是将edge端各模块通信的message与设备mapper上报到MQTT的event做转换的组件；而ServiceBus就是对应当外部是Rest-API接入时的转换组件。说道这里，就有必要提一下MQTT broker，其实搞互联网的基本都用过类似于rabbitmq、activeMQ之类的消息中间件，其实他们就支持MQTT协议啦（可以理解为AMQP的精简版）。IOT的各种设备可能直接支持MQTT，但有的只支持蓝牙或者其他近场通信的协议。没关系，Mappper可以实现将各种协议转换为对MQTT的订阅与发布，从而实现与edge端的通信。当然，ServiceBus对应就适用于支持http协议的服务了。 DeviceTwinedge端最后就剩下一个DeviceTwin模块了，要理解这个名词，就得提一下数字孪生这个概念。这里来科幻一下，假设人类要实现乾坤大挪移，但是有点难度的是，这下是要把你移到火星上。怎么办？这里有一个解决方案：在地球上通过扫描你的所有生物信息，生成拥有你完整生物特征的数据包之后，然后在地球上就把你毁灭了。再将描述你完整信息的数据包通过电波光速发送到火星上，让火星的设备再使用接收到的生物特征造出一个你。是不是挺可行！^_^ 回个头来，我们要说的数字孪生就是那个用来传输到火星的用于描述你所有生物特征的数据包；当然，这里对应就是接入设备信息。所以，DeviceTwin就是将这些信息保存到本地DB中，并处理基于cloud端的操作来修改device的某些属性（也就是操作设备）；同时，将设备基于eventBus上报的状态信息同步到本地DB和cloud端的中间人。 云端 Controller然后再说controller，其实准确的说controller是包括了用于edge端与API-Server同步信息的edgeController与用于DeviceTwin与API-Server同步device CRD信息的deviceController组成。这两个模块相对也比较简单，后面具体讲解。 各模块实现边缘端入口与beehivebeehive模块在整个kubeedge中扮演了非常重要的作用，它实现了一套Module管理的接口，程序中各个模块的启动、运行、模块间的通信等都是由其统一封装管理。下图是kubeedge的edge端代码的main启动流程，这里涉及到的modules就是由beehive提供。 可以看到，在初始化的时候，分别加载了各个edge端modules的init函数，用来注册其modules到heehive框架中。然后在core.Run中遍历启动（StartModules）。 另外，值得提及的是，用于模块间通信，发送message到group/module的功能，在beehive中，其实是通过channel来通信的。这也是golang推荐的goroutine间通信的方式。 EdgeHub 重点是启动了两个go routine，用来实现往两个方向的消息接收和分发。这里go ehc.routeToEdge对应从隧道端点接收cloud端发往edge端的消息，然后调用ehc.dispatch解析出消息的目标module并基于heehive模块module间消息的通信机制来转发出去。 同理，go ehc.routeToCloud实现将edge端消息基于隧道转发到cloud端的cloudHub模块处理。当然，该模块中实现了对同步消息的response等到超时处理的逻辑，当在未超时期间收到response消息，会转发到消息发送端模块。比较暴力的是，一旦发送消息到cloud失败，该goroutine会退出，通知所有模块，当前与cloud端是未连接状态，然后重新发起连接。 metaManager在与cloud的连接断开期间，会使用本地DB中的数据，不会发起往cloud端的查询。 Edged 这块基本是调用kubelet的代码，实现较多的是启动流程。另外，将之前kubelet的client作为fake的假接口，转而将数据都通过metaClient来存储数据到metaManager，从而代理之前直接访问api-server的操作。这块的学习可以参考之前分析kubelet架构的一篇文章Kubelet源码架构简介。 这里差异化的一块代码在e.syncPod的实现，通过读取metaManager和EdgeController的pod任务列表，来执行对本地pod的操作。同时，这些pod关联的configmap和secret也会随着处理pod的过程而一并处理。对pod的操作也是基于一个操作类别的queue，比如e.podAddWorkerRun就启动了一个用于消费添加pod的queue的goroutine。外部的封装基本就这样，内部完全通过引用kubelet原生的包来处理。 MetaManager 从代码架构看起来，该模块比较简单，首先在外层按照一定周期给自己发送消息，触发定时同步pod状态到cloud端。另外，在mainLoop中启动一个独立的goroutine接收外部消息，并执行处理逻辑。 处理逻辑基于消息类型分类，分别包括： cloud端发起的增、删、查、改 edge端模块发起的查询请求（前面提到，当状态为disconnect的时候不发起remote查询） cloud端返回的查询响应的结果 edgeHub发来的用于更新与cloudHub见连接状态的消息 自己给自己发送的，定期同步edge端pod状态到cloud端的消息 函数式计算相关的消息 重点来说增删查改，拿添加举例。当接收到要添加某个资源时，会将资源解析出来，组织成为key、type、value的三元组，以一种类似于模拟NoSQL的方式保存到本地的SqlLite数据库中。这样保存的目的也是为了方便快速检索和增删。保存完之后，需要对应发送response消息到请求消息的源模块。 EventBus与ServiceBus EventBus eventBus用于对接MQTT Broker与beehive，MQTT broker有几种启动模式，从代码实现的角度分为： 使用内嵌MQTT broker 使用外部MQTT broker 在内嵌MQTT broker模式下，eventBus启动了golang实现的broker包gomqtt用来作为外部MQTT设备的接入，具体用法请参考其github项目首页。两种模式下eventBus都做了一些共性的操作，具体包括： 向broker订阅关注的topic，如下： 1234567 SubTopics = []string&#123; \"$hw/events/upload/#\", \"$hw/events/device/+/state/update\", \"$hw/events/device/+/twin/+\", \"$hw/events/node/+/membership/get\", \"SYS/dis/upload_records\",&#125; 当接收到对应的event时，触发回调函数onSubscribe 回调函数中，对event做了简单的分类，分别发送到不同的目的地（DeviceTwin或EventHub） 所有$hw/events/device/+/twin/+和$hw/events/node/+/membership/gettopic的的event发送到DeviceTwin，其他的event直接发送到EventHub再同步到Cloud端。 当然，该部分也包括了创建客户端，往MQTT broker发布events的接口，这里就不展开了。 ServiceBus ServiceBus启动一个goroutine来接受来自beehive的消息，然后基于消息中带的参数，通过调用http client将消息通过REST-API发送到本地127.0.0.1上的目标APP。这相当于一个客户端，而APP是一个http Rest-API server，所有的操作和设备状态都需要客户端调用接口来下发和获取。 DeviceTwin DeviceTwin包含一下几个部分的功能： 数据存储方面，将设备数据存储到本地存储sqlLite，包括三张表：device、deviceAttr和deviceTwin。 处理其他模块发送到twin module的消息，然后调用 dtc.distributeMsg来处理消息。在消息处理逻辑里面，消息被分为了四个类别，并分别发送到这四个类别的action执行处理（每一个类别又包含多个action）： membership device communication twin 由于这个部分和设备更紧密相关，为何要分着几个类别，都是如何抽象，这块的理解方面还不够透彻，我们暂时只关注其主业务逻辑，官方文档对这块有比较详细的描述devicetwin。 云端入口 这里重点关注在init中加载了cloudHub、controller（也就是edgeController）和devicecontroller三个部分。然后和edge端一样，都是beehive的套路，调用StartModules来启动所有的模块。 CloudHub handler.WebSocketHandler.ServeEvent在websocket server上接收新边缘node的连接，然后为新node分配channel queue。再进一步将消息交给负责内容读写的逻辑处理。 channelq.NewChannelEventQueue为每一个边缘node维护了一个对应的channel queue（这里默认有10个消息的缓存），然后调用go q.dispatchMessage 来接收由controller发送到clouHub的消息，基于消息内容解析其目的node，然后将消息发送到node对应的channel排队处理。 clouHub的核心逻辑包括这两部分，读和写： 前面讲到，需要发送到边缘node的消息会发送到了node对应的channel队列上，这里通过handler.WebSocketHandler.EventWriteLoop在channel中读取到，并负责基于隧道发送处理（这里也很多判断，比如如果找不到对应的node节点，或者该node节点为offline状态等都会终止发送）。 另一方面，handler.WebSocketHandler.EventReadLoop函数从隧道上读取来自于edge端的消息，然后将消息发送到controller模块处理（如果是keepalive的心跳消息直接忽略）。 如果cloudHub往node发送消息失败，就会触发EventHandler的CancelNode操作；如果结合edgeHub端的行为的话，我们知道edgeHub会重新发起到cloud端的新连接，然后重新走一遍同步流程。 Controller（EdgeController） controller的核心逻辑是upstream和downstream。 upstream接收由beehive发送到controller的消息，然后基于消息资源类型，通过go uc.dispatchMessage转发到不同的的goroutine处理。这里包括nodeStatus、podStatus、queryConfigMap、querySecret、queryService、queryEndpoints等；各种类别的操作都是调用k8s的client代码来将节点状态写到API-Server。 downstream通过调用k8s client代码来监听各资源的变化情况，比如对于pod是通过 dc.podManager.Events来读取消息，然后调用dc.messageLayer.Send将消息发送到edge端处理。这里也同upstream，包括pod、configmap、secret、nodes、services和endpoints这些资源。 DeviceController deviceController同edgeController，只是其关心的资源不再是k8s的workload的子资源，而是为device定义的CRD，包括：device和deviceModel。由于主要逻辑都通edgeControler，这里不再做详细介绍。","categories":[{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/categories/edge-compute/"}],"tags":[{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"kubeedge","slug":"kubeedge","permalink":"http://ljchen.net/tags/kubeedge/"},{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/tags/edge-compute/"}]},{"title":"基于K3s部署kubeedge","slug":"基于k3s部署kubeedge","date":"2019-07-15T13:11:18.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2019/07/15/基于k3s部署kubeedge/","link":"","permalink":"http://ljchen.net/2019/07/15/基于k3s部署kubeedge/","excerpt":"在边缘计算云原生领域，Rancher开源的k3s侧重于在边缘运行完整的k8s集群，而华为开源的kubeedge则是在边缘运行可以脱机的k8s节点。刚好手上有aliyun和腾讯云上各一台配置比较差的虚拟机，平时用来跑下基本的k8s服务，做个调试验证用的，一跑应用就卡死，内存严重不足。这场景似乎与边缘计算资源局限的窘境很类似；于是，大胆的想像在其中一台机器上跑k3s，同时用其作为kubeedge的k8s server侧；在另外一台上跑kubeedge的edge节点，组成一个集群。","text":"在边缘计算云原生领域，Rancher开源的k3s侧重于在边缘运行完整的k8s集群，而华为开源的kubeedge则是在边缘运行可以脱机的k8s节点。刚好手上有aliyun和腾讯云上各一台配置比较差的虚拟机，平时用来跑下基本的k8s服务，做个调试验证用的，一跑应用就卡死，内存严重不足。这场景似乎与边缘计算资源局限的窘境很类似；于是，大胆的想像在其中一台机器上跑k3s，同时用其作为kubeedge的k8s server侧；在另外一台上跑kubeedge的edge节点，组成一个集群。 K3Sk3s就不介绍了，具体可以参考其首页; 另外，这里是项目的Readme信息。可以关注一下其架构图，图上将server和agent分开，我们今天部署的方式是将server和仅有的一个agent部署到同一个服务器上。 Linux如何翻墙为啥突然讲到linux翻墙？是因为这几天我发现在阿里和腾讯云的服务器上下载github的资源特别慢，几次通过脚本安装k3s都提示超时，实在是崩溃了！于是在linux上设置了ss client，然后速度飕飕的…. 我是使用shadowsocks，这里默认你已经有墙外的shadowsocks服务器。主要告诉你如何在linux上配置ss的client，解决下载镜像以及一些限制资源较慢的问题。 123456789101112131415161718192021# 安装sspip install shadowsocks# 设置配置文件cat /etc/shadowsocks.json&#123; \"server\":\"x.x.x.x\", # 替换为你的ss server \"server_port\":0, # 替换为你的ss server port \"local_address\": \"127.0.0.1\", \"local_port\":10800, \"password\":\"password\", # 密码 \"timeout\":300, \"method\":\"aes-256-cfb\", \"workers\": 5&#125;# 启动服务sslocal -c /etc/shadowsocks.json -d start# 设置代理export http_proxy=socks5://127.0.0.1:10800export https_proxy=socks5://127.0.0.1:10800 快速安装12345678910# 自动安装（有时候墙内特别慢，会timeout，建议先翻墙）curl -sfL https://get.k3s.io | sh -# 如果已经下载了k3s，可以手动启动# 服务端，Kubeconfig 位于 /etc/rancher/k3s/k3s.yamlk3s server &amp;# agent端， NODE_TOKEN 位于 /var/lib/rancher/k3s/server/node-tokenk3s agent --server https://myserver:6443 --token $&#123;NODE_TOKEN&#125; k3s参数科普 指定启动参数 1234567k3s server --disable-agentk3s server --no-deploy traefik k3s server --no-deploy servicelbk3s server --no-deploy corednsk3s agent -u $&#123;SERVER_URL&#125; -t $&#123;NODE_TOKEN&#125; --no-flannel &amp;k3s agent -s $&#123;SERVER_URL&#125; -t $&#123;NODE_TOKEN&#125; --docker &amp; 指定k8s参数 1234567891011# Adding extra argument can be done by passing the following flags to server or agent:--kube-apiserver-arg value--kube-scheduler-arg value--kube-controller-arg value--kubelet-arg value--kube-proxy-arg value#For example to add the following arguments -v=9 and log-file=/tmp/kubeapi.log to the kube-apiserver, you should pass the following:k3s server --kube-apiserver-arg v=9 --kube-apiserver-arg log-file=/tmp/kubeapi.log kubeedge场景配置通过上面的参数配置介绍，结合kubeedge的需求，以及我的机器的配置有限，裁减掉一些没用的服务，最终的启动文件如下： 123456789101112131415161718192021222324252627282930# cat /etc/systemd/system/k3s.service[Unit]Description=Lightweight KubernetesDocumentation=https://k3s.ioAfter=network-online.target[Service]Type=notifyEnvironmentFile=/etc/systemd/system/k3s.service.envExecStartPre=-/sbin/modprobe br_netfilterExecStartPre=-/sbin/modprobe overlayExecStart=/usr/local/bin/k3s \\ server \\ --no-deploy traefik \\ # 没用，干掉 --docker \\ # 之所以选择使用docker的原因是发现当前k3s集成的containerd有报错，没时间去慢慢查询；另外，docker处理镜像比较方便 --no-deploy servicelb \\ # 没用，干掉 #--kube-apiserver-arg insecure-bind-address=0.0.0.0 \\ #--kube-apiserver-arg insecure-port=8080KillMode=processDelegate=yesLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityTimeoutStartSec=0Restart=always[Install]WantedBy=multi-user.target 记得修改之后需要执行systemctl daemon-reload来生效。 pause镜像问题由于k3s默认是使用containerd，而非docker的；而且containerd的sock文件与docker的containerd的sock文件不一样，所以通过docker拉取pause镜像的方式行不通（我在上一步的启动参数中指定了使用docker，如果你要想尝试继续使用containerd，就会遇到该问题）。这里需要这样操作： 1234567891011# 先在安装了docker的机器上拉取镜像$ docker pull ljchen/k8s_gcr_io_pause:3.1# tag为k3s中pause image的label$ docker tag ljchen/k8s_gcr_io_pause:3.1 k8s.gcr.io/pause:3.1# 保存镜像为tar包$ docker save k8s.gcr.io/pause:3.1 -o pause.tar# 在k3s机器上，load该tar包（注意这里指定的address）$ ctr --address=/run/k3s/containerd/containerd.sock cri load pause.tar kubeedgekubedege是华为开源的一个边缘计算的项目，下面是其架构图（后续再慢慢分析，今天只讲部署）。 首先需要将kubeedge项目拉取到本地，因为需要在该项目上构建镜像。 1234$ mkdir -p $GOPATH/src/github.com/kubeedge$ cd $GOPATH/src/github.com/kubeedge$ git clone https://github.com/kubeedge/kubeedge.git 完了之后，建议直接checkout到v1.0.0上来，其他版本之前有做过一些测试，发现不少问题。本文主要基于v1.0.0来操作。 cloud端cloud端直接部署到k3s之上，主要是controller服务。 构建镜像如果使用的不是v1.0.0版本，或者希望自己构建，可以执行以下操作。12$ cd $GOPATH/src/github.com/kubeedge/kubeedge$ make cloudimage 如果只是希望部署，可以使用我已经构建好的镜像 ljchen/kubeedge_edgecontroller:v1.0.0 准备yaml生成证书信息到secret中。 12345$ cd build/cloud; ls01-namespace.yaml 03-clusterrole.yaml 05-configmap.yaml 07-deployment.yaml Dockerfile README_zh.md02-serviceaccount.yaml 04-clusterrolebinding.yaml 08-service.yaml README.md$ ../tools/certgen.sh buildSecret | tee ./06-secret.yaml 另外更改deployment的yaml镜像名称为ljchen/kubeedge_edgecontroller:v1.0.0，或者你自己构建的镜像。 最后就是执行yaml文件来部署cloud。1$ for resource in $(ls *.yaml); do kubectl create -f $resource; done 问题诊断查看kubeedge命名空间下的deployment/edgecontroller的日志，会发现很多报错；这里涉及到两个地方需要做修改。 报证书错误，当前证书无法用在kubernetes.default.svc.cluster.local需要手动修改configmap/edgecontroller，删除服务后面的cluster.local 1234567891011121314apiVersion: v1items:- apiVersion: v1data: controller.yaml: | controller: kube: master: https://kubernetes.default.svc:443 # 这里 kubeconfig: /etc/kubeedge/cloud/kubeconfig.yaml namespace: \"\" content_type: \"application/vnd.kubernetes.protobuf\" qps: 5 burst: 10 node_update_frequency: 10 发现controller报没有权限操作services和endpoints，需要在对应的role中添加 12345678910111213141516171819202122232425262728293031323334apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:creationTimestamp: \"2019-07-12T07:11:27Z\"labels: k8s-app: kubeedge kubeedge: edgecontrollername: edgecontrollerresourceVersion: \"4847\"selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/edgecontrolleruid: 43c656b8-a474-11e9-b3d9-00163e149cf2rules:- apiGroups:- \"\"resources:- nodes- nodes/status- configmaps- pods- pods/status- secrets- services # 这里services- endpoints # 这里endpointsverbs:- get- list- watch- update- apiGroups:- \"\"resources:- podsverbs:- delete edgecontroller服务无法被edge访问 由于edgecontroller是clusterIP类型，无法被集群外的edge访问到，这里可以直接在部署前或者部署之后手动修改service的类型为nodePort类型。 12345678910111213141516apiVersion: v1kind: Servicemetadata:name: edgecontrollernamespace: kubeedgelabels: k8s-app: kubeedge kubeedge: edgecontrollerspec:type: NodePortports:- port: 10000 name: cloudhubselector: k8s-app: kubeedge kubeedge: edgecontroller edge端edge端通过与edgecontroller通信来上报上报状态，以及获取k8s配置并执行。这里主要讲述通过脚本部署的方式，具体部署流程如下。官方文档见这里。 准备镜像首先是镜像准备，如果你希望自己构建镜像，请执行以下脚本。如果不想再构建，可以直接使用我构建好的镜像: ljchen/kubeedge_edgecore:v1.0.0 1234567# 检查环境是否ready$ cd $GOPATH/src/github.com/kubeedge/kubeedge/build/edge$ ./run_daemon.sh prepare# 构建镜像$ cd $GOPATH/src/github.com/kubeedge/kubeedge$ make edgeimage 拷贝证书信息您需要去拷贝edge certs文件包括edge.crt和edge.key到您想要部署edge part的k8s节点上的/etc/kubeedge/certs/文件夹中。这里我是直接将cloud端主机上的/etc/kubeedge/目录打包并scp到edge端主机上解压的。 cloudhub信息由于edge端是通过cloudhub来访问k8s api-server的，edge端的edgehub会与cloudhub进行websocket通信，需要确保该目标服务器端口可以被访问。该端口的查看方式如下: 123$ kce get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEedgecontroller NodePort 10.43.161.49 &lt;none&gt; 10000:30744/TCP 2d3h 就是edgecontroller的10000端口映射到主机nodePort的端口号。 启动脚本123456789$GOPATH/src/github.com/kubeedge/kubeedge/build/edge/run_daemon.sh only_run_edge \\ image=\"kubeedge/edgecore:v1.0.0\" \\ cloudhub=$&#123;k3s-server-IP&#125;:30744 \\ edgename=edgenode \\ arch=amd64 \\ qemu_arch=x86_64 \\ certpath=/etc/kubeedge/certs \\ certfile=/etc/kubeedge/certs/edge.crt \\ keyfile=/etc/kubeedge/certs/edge.key 创建node节点在k3s服务器节点上通过以下yaml文件来创建一个node，保存为node.yaml之后，执行kubectl apply -f node.yaml。 1234567apiVersion: v1kind: Nodemetadata: name: edgenode labels: name: edge-node node-role.kubernetes.io/edge: \"\" # 切忌！必须带上这个label，否则你就慢慢查问题吧 然后，查看node状态可以看到edge已经上报状态到k3s了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ kc get nodesNAME STATUS ROLES AGE VERSIONaliyun-vm Ready master 3d7h v1.14.3-k3s.1edgenode Ready edge 3h26m v1.10.9-kubeedge-v1.0.0$ kc describe node edgenodeName: edgenodeRoles: edgeLabels: name=edge-nodeAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;\"apiVersion\":\"v1\",\"kind\":\"Node\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"labels\":&#123;\"name\":\"edge-node\"&#125;,\"name\":\"edgenode\"&#125;&#125; node.alpha.kubernetes.io/ttl: 0CreationTimestamp: Mon, 15 Jul 2019 18:18:37 +0800Taints: &lt;none&gt;Unschedulable: falseConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- Ready True Mon, 15 Jul 2019 21:45:01 +0800 Mon, 15 Jul 2019 21:45:01 +0800 EdgeReady edge is posting ready statusAddresses: InternalIP: 172.18.0.2 Hostname: bc9fc8d547f7Capacity: cpu: 1 memory: 991Mi pods: 110Allocatable: cpu: 1 memory: 891Mi pods: 110System Info: Machine ID: System UUID: Boot ID: Kernel Version: 3.10.0-862.14.4.el7.x86_64 OS Image: Alpine Linux v3.10 Operating System: linux Architecture: amd64 Container Runtime Version: docker://3.10.0 Kubelet Version: v1.10.9-kubeedge-v1.0.0 Kube-Proxy Version:PodCIDR: 10.42.1.0/24Non-terminated Pods: (1 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default nginx-7db9fccd9b-shkw7 0 (0%) 0 (0%) 0 (0%) 0 (0%) 44mAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 0 (0%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%)Events: &lt;none&gt; 接下来，就可以像使用普通k8s一样部署你的应用到edge节点（你可以把节点换成你的树莓派或者其他开发板）了。enjoy yourself ^_^","categories":[{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/categories/edge-compute/"}],"tags":[{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"},{"name":"kubeedge","slug":"kubeedge","permalink":"http://ljchen.net/tags/kubeedge/"},{"name":"edge-compute","slug":"edge-compute","permalink":"http://ljchen.net/tags/edge-compute/"},{"name":"k3s","slug":"k3s","permalink":"http://ljchen.net/tags/k3s/"}]},{"title":"Knative Eventing必知原理","slug":"knative-eventing必知原理","date":"2019-07-09T02:39:04.000Z","updated":"2020-04-06T09:24:01.483Z","comments":true,"path":"2019/07/09/knative-eventing必知原理/","link":"","permalink":"http://ljchen.net/2019/07/09/knative-eventing必知原理/","excerpt":"serverless需要有一套广泛兼容多种事件的时间触发框架，从而支持在不同应用场景下基于事件来创建server，对外提供服务。knative的eventing的目标就是提供这样一套框架。当前knative原生支持的event类型还比较有限，但是系统提供了开放的container source，可以基于需求自定义实现。本篇的示例主要基于系统sample文档的in-memory-channel来论述其实现原理。","text":"serverless需要有一套广泛兼容多种事件的时间触发框架，从而支持在不同应用场景下基于事件来创建server，对外提供服务。knative的eventing的目标就是提供这样一套框架。当前knative原生支持的event类型还比较有限，但是系统提供了开放的container source，可以基于需求自定义实现。本篇的示例主要基于系统sample文档的in-memory-channel来论述其实现原理。 概念理解由于整个knative都是建立在k8s之上的，knative的实现完全基于k8s原生的编程框架。所以，要理解eventing的整个原理，我们先来分别了解其定义的CRD资源，以及它们之间的联动关系。 CRD资源通过在k8s上过滤eventing的crd资源，可以看到knative v0.7.0提供了一下的资源定义。他们都有什么作用，以及彼此之间存在何种依赖关系？接下来将做分析。 12345678910~  kc get crd | grep eventingapiserversources.sources.eventing.knative.dev 2019-07-06T14:34:04Zbrokers.eventing.knative.dev 2019-07-06T14:34:04Zchannels.eventing.knative.dev 2019-07-06T14:34:04Zclusterchannelprovisioners.eventing.knative.dev 2019-07-06T14:34:04Zcontainersources.sources.eventing.knative.dev 2019-07-06T14:34:04Zcronjobsources.sources.eventing.knative.dev 2019-07-06T14:34:04Zeventtypes.eventing.knative.dev 2019-07-06T14:34:04Zsubscriptions.eventing.knative.dev 2019-07-06T14:34:04Ztriggers.eventing.knative.dev 2019-07-06T14:34:04Z 可以按照功能，将其分为两大类： source相关抽象事件类型，适用于每一种sourceeventtypes.eventing.knative.dev系统支持的三种sourceapiserversources.sources.eventing.knative.devcontainersources.sources.eventing.knative.devcronjobsources.sources.eventing.knative.dev broker-trigger相关顶层抽象brokers.eventing.knative.devtriggers.eventing.knative.dev逻辑层channels.eventing.knative.devsubscriptions.eventing.knative.dev物理实现clusterchannelprovisioners.eventing.knative.dev 关联关系 上图列出了所有的CRD，他们的作用和关联关系如下： 三个source CRD创建之后，其controller会主动部署对应的deployment; deployment实现了从producer收集event，并转发到下一跳的逻辑，相当于event进入knative-eventing系统的入口； 当创建namespace的时候，如果指定了label knative-eventing-injection=enabled，knative会在该namespace自动创建default broker； broker和trigger抽象了event转发的逻辑，为了实现broker，controller会分别创建该broker的ingress-channel和trigger-channel以及用于租户业务namespace与eventing system namespace之间event转发的deployment和service； trigger是依赖于broker的，如果trigger不指定broker，会自动使用default broker，trigger中明确定义了subscription； subscription在这里涉及到两部分，一部分是用户创建trigger时指定的业务相关的订阅和SINK；另一部分是系统内置的，trigger channel在fanout了消息之后，需要reply通道，ingress-channel和ingress-subscription就是为了实现该通道的转发逻辑； clusterchannelprovisioners在当前版本中还保留，但是后续会被废弃掉，转而采用各个provisioner对应的CRD。这里可以理解为channel之下对应的物理实现，通过解析channel中的subscription信息来fanout事件。 broker-trigger实例按照官网的步骤来部署一个ApiServerSource类型的source采集k8s的event，并经过一些列的channel之后，最终达到ksvc来展示出来。具体步骤详见这里。忽略一些非关键的步骤，我们来重点看看几个核心概念都是如何定义的，以及他们之间的关联关系。 consumer先准备好最终展示所接收event的ksvc，该资源受autoscale的控制，在没有请求的情况下，pod实例数会自动缩容到0值。查看yaml配置文件的最后一段是knative service containers的定义，这里的镜像的功能是显示接收到的event信息 (为了墙内拉取方便，镜像已经被替换地址)。 12345678910apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata: name: event-display namespace: defaultspec: template: spec: containers: - image: ljchen/knative_eventing-sources_cmd_event_display:v0.7.0 部署好之后，可以看到对应的pod和service信息如下： 123456789101112 ~  kc get deploy,svc,podNAME READY UP-TO-DATE AVAILABLE AGEdeployment.extensions/event-display-xnztn-deployment 1/1 1 1 15hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/event-display ExternalName &lt;none&gt; istio-ingressgateway.istio-system.svc.cluster.local &lt;none&gt; 15hservice/event-display-xnztn ClusterIP 10.111.128.248 &lt;none&gt; 80/TCP 15hservice/event-display-xnztn-m5j2x ClusterIP 10.110.215.179 &lt;none&gt; 80/TCP 15hservice/event-display-xnztn-xqtmr ClusterIP 10.105.98.248 &lt;none&gt; 9090/TCP,9091/TCP 15hNAME READY STATUS RESTARTS AGEpod/event-display-xnztn-deployment-797c9bbcd8-hd8l4 2/2 Running 0 96s source首先是event的来源，这里由于是ApiServerSource，只需直接指定对应的配置文件。 123456789101112131415apiVersion: sources.eventing.knative.dev/v1alpha1kind: ApiServerSourcemetadata: name: testevents namespace: defaultspec: serviceAccountName: events-sa mode: Resource resources: - apiVersion: v1 kind: Event sink: apiVersion: eventing.knative.dev/v1alpha1 kind: Broker name: default 注意，这里配置的sink指定了使用default broker。当该yaml被应用到k8s之后，在对应的namespace下可以看到创建了一个deployment。 123456789101112# apiserversource~  kc get apiserversourceNAME AGEtestevents 4h27m# deployment, pod~  kc get deploy,podNAME READY UP-TO-DATE AVAILABLE AGEdeployment.extensions/apiserversource-testevents-57knn 1/1 1 1 4h26m 9090/TCP 37dNAME READY STATUS RESTARTS AGEpod/apiserversource-testevents-57knn-95dfb87bd-s48p4 1/1 Running 0 4h26m 如果查看service，会发现找不到该deploy对应的service。原因是其只watch k8s api-server，然后将k8s event信息收集到之后发送到对应的channel；因此，该服务并不对外提供其他服务。另外，该服务貌似并没有sidecar，我们知道对于knative的service都是有sidecar的，为啥呢？还是同样的原因，这是一个系统服务，不是业务的ksvc。 brokerdefault broker是自动创建的，创建的时候需要为namespace配置对应的label（如果不配置的话，会导致broker无法被自动创建，整个event链路不通），具体如下： 1kubectl label namespace default knative-eventing-injection=enabled 在执行完命令之后，最直观的是可以查看到已经创建的broker default（通过查看该broker的status可以看到address，即外部的访问地址）；在default namespace下可以看到，已经自动部署了两个deployment。但是如果我们通过查看knative的eventing CRD可以发现更多的资源被创建了出来。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# broker~  kc get brokerNAME READY REASON HOSTNAME AGEdefault True default-broker.default.svc.cluster.local 10h# broker详情~  kc get broker default -o yamlapiVersion: eventing.knative.dev/v1alpha1kind: Brokermetadata: ......status: IngressChannel: # ingress channel apiVersion: eventing.knative.dev/v1alpha1 kind: Channel name: default-kn-ingress namespace: default address: # 对外访问的地址信息(并不是指向channel，而是deployment) hostname: default-broker.default.svc.cluster.local url: http://default-broker.default.svc.cluster.local ...... triggerChannel: # trigger channel apiVersion: eventing.knative.dev/v1alpha1 kind: Channel name: default-kn-trigger namespace: default# channels~  kc get channelsNAME READY REASON AGEdefault-kn-ingress True 10h # 用于reply的channeldefault-kn-trigger True 10h # 主channel# channel provisioners (后面会废弃该方式)~  kc get clusterchannelprovisioners.eventing.knative.devNAME READY REASON AGEin-memory True 11h# subscriptions #（属于ingress侧的订阅, 将ingressChannel事件订阅到default-broker-ingress上，用于reply）~  kc get subscriptions.eventing.knative.devNAME READY REASON AGEinternal-ingress-default-fv8bg True 10h# deployment, service and pod （将流量引入in-memory channel的业务端组件）~  kc get deploy,svc,podNAME READY UP-TO-DATE AVAILABLE AGEdeployment.extensions/default-broker-filter 1/1 1 1 96sdeployment.extensions/default-broker-ingress 1/1 1 1 96sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE# 业务端组件ingress和filter的serviceservice/default-broker ClusterIP 10.101.17.164 &lt;none&gt; 80/TCP,9090/TCP 10hservice/default-broker-filter ClusterIP 10.97.228.135 &lt;none&gt; 80/TCP,9090/TCP 10h# 真正channel的service（都映射到dispatcher上）service/default-kn-ingress-channel-4xxz2 ExternalName &lt;none&gt; in-memory-dispatcher.knative-eventing.svc.cluster.local &lt;none&gt; 10hservice/default-kn-trigger-channel-zbrlw ExternalName &lt;none&gt; in-memory-dispatcher.knative-eventing.svc.cluster.local &lt;none&gt; 10hNAME READY STATUS RESTARTS AGEpod/default-broker-filter-744ff96759-x4nbt 1/1 Running 0 97spod/default-broker-ingress-96cd4b769-qswm2 1/1 Running 0 97s trigger下面是trigger的配置： 1234567891011apiVersion: eventing.knative.dev/v1alpha1kind: Triggermetadata: name: testevents-trigger namespace: defaultspec: # 未指定broker，默认使用default broker subscriber: # 指定trigger侧订阅到consumer ref: apiVersion: serving.knative.dev/v1alpha1 kind: Service name: event-display 在subscriber.ref可以看到，其将event内容发送到了event-display这一个knative service。 1234567891011# trigger ~  kc get triggerNAME READY REASON BROKER SUBSCRIBER_URI AGEtestevents-trigger True default http://event-display.default.svc.cluster.local 12s# subscriptions 多出来了一条记录 default-testevents-trigger-rrhzn#（属于trigger侧的订阅, 固定将triggerChannel事件订阅到default-broker-filter上，event订阅的URI中会带上详细trigger信息，用于反查trigger中用户指定的subscription内容） ~  kc get subscriptionsNAME READY REASON AGEdefault-testevents-trigger-rrhzn True 7m31s#internal-ingress-default-fv8bg True 10h 数据平面前面是整个创建流程，以及对应的资源属性，接下来分析一下event转发的数据面流程，先来看官方的一张控制面与转发面的图。 由于该图较老，只体现了channel与scription这一层的概念，且数据面较抽象。我重新基于broker&amp;trigger以及in-memory-channel整理了该图，具体如下。 其中实现部分为真正event的转发流程，broker-ingress、broker-filter与上面的channel之间通过虚线连接的部分是逻辑链路。黑色的连线为event转发，红色为反向的reply链路。in-memory-dispatcher是imc的底层实现，如果是使用kafka就应该替换为kafaka deployment。这里先不做详细描述，具体见下文逐步分析。 source源前面讲到，当我们将apiserversource配置下发后，controller会在namespace下部署出一个deployment，该deployment的作用是收集k8s的evnet并发送到指定的SINK。由于我们指定的是default broker，经controller处理之后，其参数变为了default-broker这个service的访问地址（即，broker的总入口）；具体见下面操作中对应的注释信息。 12345678910111213141516171819202122232425262728293031 ~  kc get deployment.extensions/apiserversource-testevents-57knn -o yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: ......spec: ...... template: ...... spec: containers: - env: - name: SINK_URI # 重点关注该value，为default-broker这个service value: http://default-broker.default.svc.cluster.local - name: MODE value: Resource - name: API_VERSION value: v1 - name: KIND value: Event - name: CONTROLLER value: \"false\" - name: SYSTEM_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: ljchen/knative_eventing_cmd_apiserver_receive_adapter:v0.7.0 ......status: ...... 当event接收到后，直接转发到SINK_URL指定的default-broker地址。自此，source部分的工作已经完结，接下来在看看default-broker。 broker &amp; trigerbroker在逻辑层面包含ingress-channel和trigger-channel，对应在数据面位于eventing的system namespace下有对应的dispatcher。同时，位于业务的namespace中会有broker-ingress和broker-filter两个deployment用来负责dispatcher与业务层之间event的转发。 broker-ingress既然已经知道流量是转发给default-broker这个service的，直接查看该service以及对应的endpoint的地址，确定其对应pod和deployment。 1234567# service, endpoint~  kc get svc,ep default-brokerNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/default-broker ClusterIP 10.101.17.164 &lt;none&gt; 80/TCP,9090/TCP 15hNAME ENDPOINTS AGEendpoints/default-broker 10.244.0.69:9090,10.244.0.69:8080 15h 根据endpoint的IP地址，查找对应pod，deployment。 12345678# pod~  kc get pod -o wide | grep 10.244.0.69default-broker-ingress-96cd4b769-qswm2 1/1 Running 0 4h46m 10.244.0.69 k8s-master &lt;none&gt; &lt;none&gt;# deployment~  kc get deploy default-broker-ingressNAME READY UP-TO-DATE AVAILABLE AGEdefault-broker-ingress 1/1 1 1 4h48m 该deployment的配置信息如下，这里面关键信息已经在注释中标明。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152~  kc get deploy default-broker-ingress -o yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: ......spec: ...... selector: matchLabels: eventing.knative.dev/broker: default # broker的名称为default eventing.knative.dev/brokerRole: ingress # 这里指定了broker的角色为ingress，难道还有其他的角色？是的，就是trigger role ...... template: ...... spec: containers: - env: ...... - name: FILTER - name: CHANNEL # 该deployment接受到event之后，发送到下一个channel的名称 value: default-kn-trigger-channel-zbrlw.default.svc.cluster.local - name: BROKER # 该deploy所属的broker value: default image: ljchen/knative_eventing_cmd_broker_ingress:v0.7.0 # 镜像已经替换 ......status: ......# 下一跳channel的service表明发送到knative-eventing ns，ingress-channel和trigger-channel都发到同一个external-ip，即imc的底层in-memory-dispatcher~  kc get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE#default-kn-ingress-channel-4xxz2 ExternalName &lt;none&gt; in-memory-dispatcher.knative-eventing.svc.cluster.local &lt;none&gt; 16hdefault-kn-trigger-channel-zbrlw ExternalName &lt;none&gt; in-memory-dispatcher.knative-eventing.svc.cluster.local &lt;none&gt; 16h# knative-eventing system namespace中查看服务~  kke get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEin-memory-dispatcher ClusterIP 10.98.22.109 &lt;none&gt; 80/TCP 17h# endpoint~  kke get ep in-memory-dispatcherNAME ENDPOINTS AGEin-memory-dispatcher 10.244.0.43:8080 17h# pod~  kke get pod -o wide | grep 10.244.0.43in-memory-channel-dispatcher-ffd969cd9-vhblf 1/1 Running 0 17h 10.244.0.43 k8s-master &lt;none&gt; &lt;none&gt;# deployment~  kke get deployNAME READY UP-TO-DATE AVAILABLE AGEin-memory-channel-dispatcher 1/1 1 1 17h dispatcher自此，我们似乎再也没有线索，不知道event到了in-memory-dispatcher之后是如何转发的。但是通过查看dispatcher代码了解到，该deployment转发event的逻辑是基于channel的subscription配置信息。由于当前对应为triggerChannel，通过查询其channel default-kn-trigger可知： 12345678910111213141516171819202122232425# channel description \"default-kn-trigger\"~  kc describe channels default-kn-triggerName: default-kn-triggerKind: Channel....Spec: Provisioner: API Version: eventing.knative.dev/v1alpha1 Kind: ClusterChannelProvisioner Name: in-memory Subscribable: Subscribers: Generation: 1 Ref: Name: default-testevents-trigger-rrhzn Namespace: default UID: 9985ef4e-a05a-11e9-84ef-525400ff729a Reply URI: http://default-kn-ingress-channel-4xxz2.default.svc.cluster.local Subscriber URI: http://default-broker-filter.default.svc.cluster.local/triggers/default/testevents-trigger/99849cd0-a05a-11e9-84ef-525400ff729a # 找到其fanout的下一跳URL了。注意，这个URI除了host之外，还有一串信息 UID: 9985ef4e-a05a-11e9-84ef-525400ff729aStatus: Address: # 对外通过该地址来接收发往该channel的event Hostname: default-kn-trigger-channel-zbrlw.default.svc.cluster.local URL: http://default-kn-trigger-channel-zbrlw.default.svc.cluster.local .... 报文会被dispacher fanout到subscriber URI，也就是default-broker-filter这个服务。 broker-filter1234567891011121314~  kc get svc default-broker-filterNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault-broker-filter ClusterIP 10.97.228.135 &lt;none&gt; 80/TCP,9090/TCP 17h~  kc get ep default-broker-filterNAME ENDPOINTS AGEdefault-broker-filter 10.244.0.68:9090,10.244.0.68:8080 17h~  kc get pod -o wide | grep 10.244.0.68default-broker-filter-744ff96759-x4nbt 1/1 Running 0 7h 10.244.0.68 k8s-master &lt;none&gt; &lt;none&gt;~  kc get deploy default-broker-filterNAME READY UP-TO-DATE AVAILABLE AGEdefault-broker-filter 1/1 1 1 7h1m 到这里似乎又卡壳了，因为通过查看default-broker-filter的配置，得不到任何有关其下一跳的信息。通过分析代码，发现代码中通过报文URI中的路径信息（http://default-broker-filter.default.svc.cluster.local/triggers/default/testevents-trigger/99849cd0-a05a-11e9-84ef-525400ff729a）来获取到trigger名称（default/testevents-trigger），然后再提取trigger的subscriber来定位到下一跳，最终将event发送给knative-service即event-display。 1234567891011apiVersion: eventing.knative.dev/v1alpha1kind: Triggermetadata: name: testevents-trigger namespace: defaultspec: subscriber: ref: apiVersion: serving.knative.dev/v1alpha1 kind: Service name: event-display","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"},{"name":"eventing","slug":"eventing","permalink":"http://ljchen.net/tags/eventing/"}]},{"title":"Tekton原理","slug":"tekton原理","date":"2019-06-30T03:54:29.000Z","updated":"2020-04-06T09:24:01.486Z","comments":true,"path":"2019/06/30/tekton原理/","link":"","permalink":"http://ljchen.net/2019/06/30/tekton原理/","excerpt":"Tekton作为google开源出来的一个CD工具，与knative之前的build有很多相似的地方。build的功能相对来将较单一，且社区提出了很多改进的地方都由于其现有的架构设计局限而找不到解决方案。那么tekton基于build，都有哪些提升？","text":"Tekton作为google开源出来的一个CD工具，与knative之前的build有很多相似的地方。build的功能相对来将较单一，且社区提出了很多改进的地方都由于其现有的架构设计局限而找不到解决方案。那么tekton基于build，都有哪些提升？ Tekton VS. Build相对于build，tekton主要有以下这些改进，可能归纳的不够完整，后续再逐步补充。 提供了更高层抽象 knative-build只提供了build和build-template这两层，但是如果要做更高级的流水线编排就比较局限。tekton既有现有资源的映射：task-&gt;buildTemplate, taskRun-&gt;build; 又提供了更高层的抽象，pipeline和pipelineRun。pipeline可以包含多个task，这样就可以支持更加复杂的流水线逻辑。 支持DAG调度 tekton的pipeline中，各个task之间可以指定其运行的依赖顺序。通过解析这些依赖生成一个DAG图，在调度的时候基于该图的顺序来逐个调度task。 Task不再使用initContainer knative-build使用initContainer来做流水线的调度，这就导致了所有的step必须是串行的。在tekton中，task不再使用initContainer来实现各个step之间的顺序，而是通过在entrypoint中指定volume依赖来控制同一个pod中不同container的调用顺序。 实现原理 首先创建了pipelineRun之后，reconcile流程会分析里面包含的所有task，并生成DAG图。经过一系列对resources的替换处理之后，从pipeline的DAG中调度出待运行的task，并创建taskRun。pipeline reconcile逻辑会及时处理status，更新状态。 taskRun的reconcile通过查询该task的steps，也是解析resources并生成并部署pod。它也有对应的timeout逻辑，用于在执行超时的时候更新taskRun的状态。 Task调度下面通过一个实例，手动创建一个task和taskrun，然后分析其生成的pod的yaml文件来了解其调度逻辑。 先创建一个task和taskRun。 123456789101112131415161718192021222324252627282930313233343536373839404142# task.yaml 和taskrun.yaml ~/Desktop/tekton  cat task.yamlapiVersion: tekton.dev/v1alpha1kind: Taskmetadata: name: echo-hello-worldspec: steps: - name: echo-1 image: ubuntu command: - echo args: - \"hello world\" - name: echo-2 image: ubuntu command: - echo args: - \"hello world\" ~/Desktop/tekton  cat taskrun.yamlapiVersion: tekton.dev/v1alpha1kind: TaskRunmetadata: name: echo-hello-world-task-runspec: taskRef: name: echo-hello-world# 查看CRD资源实例~/Desktop/tekton  kc get taskNAME AGEecho-hello-world 34m~/Desktop/tekton  kc get taskrunNAME SUCCEEDED REASON STARTTIME COMPLETIONTIMEecho-hello-world-task-run True 33m 33m# 调度的pod，显示已经执行完成~/Desktop/tekton  kc get podNAME READY STATUS RESTARTS AGEecho-hello-world-task-run-pod-01d042 0/3 Completed 0 2m10s 执行之后，可以在dashboad看到对应的两个step的状态。 接下来，分析pod的yaml文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125~/Desktop/tekton  kc get pod echo-hello-world-task-run-pod-01d042 -o yamlapiVersion: v1kind: Podmetadata: ......spec: containers: - args: - -wait_file # 指定其不依赖任何文件，可以在initContainer执行完成后首先执行” - \"\" - -post_file - /builder/tools/0 # 指定其运行完成后，才生成“/builder/tools/0” - -entrypoint # 在step中指定的，真正用户需要执行的args在最后 - echo - -- - hello world command: - /builder/tools/entrypoint env: - name: HOME value: /builder/home image: ubuntu name: build-step-echo-1 ...... volumeMounts: - mountPath: /builder/tools name: tools - mountPath: /workspace name: workspace - mountPath: /builder/home name: home - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v254r readOnly: true workingDir: /workspace - args: - -wait_file # 指定其依赖于 “/builder/tools/0” - /builder/tools/0 - -post_file - /builder/tools/1 # 指定其运行完成后，才生成“/builder/tools/1” - -entrypoint # 在step中指定的，真正用户需要执行的args在最后 - echo - -- - hello world command: - /builder/tools/entrypoint env: - name: HOME value: /builder/home image: ubuntu name: build-step-echo-2 ...... volumeMounts: - mountPath: /builder/tools name: tools - mountPath: /workspace name: workspace - mountPath: /builder/home name: home - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v254r readOnly: true workingDir: /workspace - args: - -wait_file # 指定其依赖于 “/builder/tools/1” - /builder/tools/1 - -post_file - /builder/tools/2 - -entrypoint - /ko-app/nop - -- command: - /builder/tools/entrypoint image: ljchen/tekton_pipeline_cmd_nop:v0.4.0 name: nop # 这是所有step执行完成之后才执行的nop container ...... volumeMounts: - mountPath: /builder/tools name: tools - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v254r readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true initContainers: - command: - /ko-app/creds-init #该容器主要用于拉取镜像是秘钥处理 env: - name: HOME value: /builder/home image: ljchen/tekton_pipeline_cmd_creds-init:v0.4.0 name: build-step-credential-initializer-rgt72 volumeMounts: - mountPath: /workspace name: workspace - mountPath: /builder/home name: home - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v254r readOnly: true workingDir: /workspace - args: - -c - cp /ko-app/entrypoint /builder/tools/entrypoint #通过挂卷的方式，将entrypoint复制到用户容器目录 command: - /bin/sh env: - name: HOME value: /builder/home image: ljchen/tekton_pipeline_cmd_entrypoint:v0.4.0 name: build-step-place-tools volumeMounts: - mountPath: /builder/tools name: tools - mountPath: /workspace name: workspace - mountPath: /builder/home name: home - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v254r readOnly: true workingDir: /workspace ......status: ...... 上面添加了较多的注释，应该比较好理解。通过编排pod，修改用户指定step的entrypoint，在启动用户commands前指定依赖文件，从而控制同一个pod中多个container的执行顺序。这就是task的调度中不依赖于initContainer来控制多step顺序的方案。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"},{"name":"tekton","slug":"tekton","permalink":"http://ljchen.net/tags/tekton/"}]},{"title":"Tekton安装镜像","slug":"tekton安装镜像","date":"2019-06-29T13:09:52.000Z","updated":"2020-04-06T09:24:01.486Z","comments":true,"path":"2019/06/29/tekton安装镜像/","link":"","permalink":"http://ljchen.net/2019/06/29/tekton安装镜像/","excerpt":"knative自v0.7.0开始废弃现有的build而拥抱tekton，由于均是基于google的项目，所以镜像下载成为一个大问题。本文采用类似于knative墙内安装 的方式来处理镜像和安装脚本，适用于墙内安装体验。","text":"knative自v0.7.0开始废弃现有的build而拥抱tekton，由于均是基于google的项目，所以镜像下载成为一个大问题。本文采用类似于knative墙内安装 的方式来处理镜像和安装脚本，适用于墙内安装体验。 安装流程主要是替换yaml中无法墙内无法拉取到的gcr镜像的流程，具体如下。 生成所需yaml下面是下载tekton安装yaml文件，并替换其中的gcr.io的镜像为ljchen提供的镜像的脚本。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105TEKTON_PATH='tekton'TEKTON_VER='v0.4.0'REGISTRY_URL='ljchen'rm -rf $TEKTON_PATHmkdir $TEKTON_PATH# download yamlecho \"download yaml files ...\"cd $TEKTON_PATHwget -q https://storage.googleapis.com/tekton-releases/latest/release.yamlcd ..# get images listecho \"collect image to tmp file ...\"cd $TEKTON_PATHrm -rf image.tmpfor line in `grep -RI \" image: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/tekton-releases/github.com/tektoncd' ]] then sub_line1=$&#123;line##gcr.io/tekton-releases/github.com/tektoncd/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=tekton_$&#123;sub_line2//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#image:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=tekton_$&#123;sub_line3//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp; fi fidonefor line in `grep -RI \" - \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/tekton-releases/github.com/tektoncd' ]] then sub_line1=$&#123;line##gcr.io/tekton-releases/github.com/tektoncd/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=tekton_$&#123;sub_line2//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#value:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=tekton_$&#123;sub_line3//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp; fi fidonecd ..# replace file imagecd $TEKTON_PATHcounter=0for file in *.yamldo echo \"开始处理文件 \" $file while read line do origin_image=`echo $&#123;line&#125; | awk '&#123;print $1&#125;'` new_image=`echo $&#123;line&#125; | awk '&#123;print $2&#125;'` tmp=$&#123;origin_image//\\//__&#125; origin_image=$&#123;tmp//__/\\\\/&#125; tmp2=$&#123;new_image//\\//__&#125; new_image=$&#123;tmp2//__/\\\\/&#125; sed -i \"s/$&#123;origin_image&#125;/$&#123;new_image&#125;/g\" $&#123;file&#125; #上面这行，如果是MacOS/UNIX请替换为: sed -i \" \" \"s/$&#123;origin_image&#125;/$&#123;new_image&#125;/g\" $&#123;file&#125; done &lt; image.tmp counter=`expr $&#123;counter&#125; + 1`doneecho \"共处理文件数：\" $&#123;counter&#125;rm -rf *.yaml.1cd ..# finishecho \"completed...\" 部署执行完脚本之后，会生成对应的文件目录如下图所示： 123456~/Desktop/tekton  tree.├── install.sh├── tekton ├── image.tmp └── release.yaml 其中的release.yaml文件就是我们所需的安装文件，直接kubectl apply -f release.yaml即可安装。安装完后，在tekton-pipelines命名空间下，会生成对应的pod，看名称和knative的build很像。 1234567891011121314# 查看pod，同build一致，均包含controller和webhook~/Desktop/tekton  kct get podNAME READY STATUS RESTARTS AGEtekton-pipelines-controller-6cf456fc89-sfjzs 1/1 Running 0 30mtekton-pipelines-webhook-58c9896d9b-6qd7v 1/1 Running 0 30m# 对应的CRD列表，看到资源名称不同于原有build中的build和build-template~/Desktop/tekton  kct get crd | grep tektonclustertasks.tekton.dev 2019-06-29T12:44:38Zpipelineresources.tekton.dev 2019-06-29T12:44:38Zpipelineruns.tekton.dev 2019-06-29T12:44:38Zpipelines.tekton.dev 2019-06-29T12:44:38Ztaskruns.tekton.dev 2019-06-29T12:44:38Ztasks.tekton.dev 2019-06-29T12:44:38Z 安装dashboard 下载dashboard的yaml文件： 1wget https://raw.githubusercontent.com/tektoncd/dashboard/master/config/release/gcr-tekton-dashboard.yaml 替换gcr-tekton-dashboard.yaml 中的镜像 gcr.io/tekton-nightly/dashboard:latest 为 ljchen/tekton-nightly_dashboard:latest; 部署到k8s，并映射端口出来。1234567891011121314~  kct get podNAME READY STATUS RESTARTS AGEtekton-dashboard-9bddff6bb-rzj6d 1/1 Running 0 6m43stekton-pipelines-controller-6cf456fc89-sfjzs 1/1 Running 1 12htekton-pipelines-webhook-58c9896d9b-6qd7v 1/1 Running 1 12h~  kct get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtekton-dashboard ClusterIP 10.99.93.239 &lt;none&gt; 9097/TCP 6m45stekton-pipelines-controller ClusterIP 10.100.85.20 &lt;none&gt; 9090/TCP 12htekton-pipelines-webhook ClusterIP 10.97.218.191 &lt;none&gt; 443/TCP 12h~  kct port-forward --address 0.0.0.0 pod/tekton-dashboard-9bddff6bb-rzj6d 8888:9097 Forwarding from 0.0.0.0:8888 -&gt; 9097 以下是打开后的界面，主要从tekton抽象概念的视角分别展现内容。另外，还提供了对resources引入的快捷操作。 配套镜像生成脚本正常情况下，你不需要关注该脚本，除非己希望在墙外生成镜像并推送到自己的镜像仓库。对于只希望安装tokton的读者，请直接使用上一步中的yaml文件部署。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495TEKTON_PATH='tekton'TEKTON_VER='v0.4.0'REGISTRY_URL='ljchen'rm -rf $TEKTON_PATHmkdir $TEKTON_PATH# download yamlecho \"download yaml files ...\"cd $TEKTON_PATHwget -q https://storage.googleapis.com/tekton-releases/latest/release.yamlcd ..# get images listecho \"collect image to tmp file ...\"cd $TEKTON_PATHrm -rf image.tmpfor line in `grep -RI \" image: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/tekton-releases/github.com/tektoncd' ]] then sub_line1=$&#123;line##gcr.io/tekton-releases/github.com/tektoncd/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=tekton_$&#123;sub_line2//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#image:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=tekton_$&#123;sub_line3//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp; fi fidonefor line in `grep -RI \" - \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/tekton-releases/github.com/tektoncd' ]] then sub_line1=$&#123;line##gcr.io/tekton-releases/github.com/tektoncd/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=tekton_$&#123;sub_line2//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#value:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=tekton_$&#123;sub_line3//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;TEKTON_VER&#125; &gt;&gt; image.tmp; fi fidonecd ..download image, tag, pushcd $TEKTON_PATHwhile read line do origin_image=`echo $&#123;line&#125; | awk '&#123;print $1&#125;'` new_image=`echo $&#123;line&#125; | awk '&#123;print $2&#125;'` echo \"old:\" $&#123;origin_image&#125; echo \"new:\" $&#123;new_image&#125; docker pull $&#123;origin_image&#125; docker tag $&#123;origin_image&#125; $&#123;new_image&#125; docker push $&#123;new_image&#125;done &lt; image.tmpcd ..# doneecho \"completed...\" gcr镜像映射表老问题，排版有点儿难看，见谅！ gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller@sha256:80e040a58ce6c4d58ae893eb934777bce013ef8be079967dc3db783d76fa5aaa=&gt; ljchen/tekton_pipeline_cmd_controller:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook@sha256:da75fbdaeb800813d85b99f7f54b665e8d0edbb2c5a7ffc6a99d66aede0291a3=&gt; ljchen/tekton_pipeline_cmd_webhook:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/kubeconfigwriter@sha256:2000fdb77fd830719533756afe246c460949b46eb0c7fc1841de17656d6f5114=&gt; ljchen/tekton_pipeline_cmd_kubeconfigwriter:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/creds-init@sha256:b4877c99d928fad3cf26c995d171674b34d206178d6f9f0efb337ebff01bb34b=&gt; ljchen/tekton_pipeline_cmd_creds-init:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init@sha256:4b91c31560f18a8f09c68d5288f2261797b6df31522a57a9d7350bc0060a1284=&gt; ljchen/tekton_pipeline_cmd_git-init:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/nop@sha256:9160ed41b20b2822d06e907d89f6398ea866c86a971f83371efb9e147fba079f=&gt; ljchen/tekton_pipeline_cmd_nop:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/bash@sha256:0355a9b21a7c0cc9466bf75071648e266de07b5e13fbfd271ec791c45a818bdb=&gt; ljchen/tekton_pipeline_cmd_bash:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/gsutil@sha256:6b6b8e02f6f03fb33cf3007b6b501e07bf2f435a0309482b868712a20f1dfd78=&gt; ljchen/tekton_pipeline_cmd_gsutil:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint@sha256:4d1fe990ca06ecc671370dfeab31d857efa8ccf81d632a672561c60482fd9aae=&gt; ljchen/tekton_pipeline_cmd_entrypoint:v0.4.0 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/imagedigestexporter@sha256:3d36cb375da0e4b5b5cf8b0964ed3d80f4645142ac609679c3b26a369f3ed340=&gt; ljchen/tekton_pipeline_cmd_imagedigestexporter:v0.4.0 gcr.io/tekton-nightly/dashboard:latest=&gt; ljchen/tekton-nightly_dashboard:latest (该镜像构建时间为2019-06-30)","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"},{"name":"tekton","slug":"tekton","permalink":"http://ljchen.net/tags/tekton/"}]},{"title":"Knative流量的秘密","slug":"knative流量的秘密","date":"2019-06-29T07:08:17.000Z","updated":"2020-04-06T09:24:01.483Z","comments":true,"path":"2019/06/29/knative流量的秘密/","link":"","permalink":"http://ljchen.net/2019/06/29/knative流量的秘密/","excerpt":"knative在service里面实现了serverless的功能，其中最重要的莫过于按需来启动服务，并基于流量来弹性伸缩。在社区的文档里面找到这样一张架构设计图，先初略理解一下，详细介绍一下其流量转发流程。 该文基于knative v0.6.0版本。","text":"knative在service里面实现了serverless的功能，其中最重要的莫过于按需来启动服务，并基于流量来弹性伸缩。在社区的文档里面找到这样一张架构设计图，先初略理解一下，详细介绍一下其流量转发流程。 该文基于knative v0.6.0版本。 模型抽象这里说的模型，其实就是knative的service里面关于CRD的定义。其中声明在外的莫属service、configuration、revision以及route。但是要真正搞明白整个模型的原作原理，理解清楚所有CRD的关联关系是非常有必要的。 上图展示了从service衍生出来的所有CRD的血缘关系，下面简述其流程和个CRD的作用。 当用户创建一个knative的service的时候，其controller会对应创建出configuration和route；这一块较简单，因为service的spec里面其实是包含了对configuration和traffic的定义的。 花开两支，configuration一方面基于配置创建对应的revision；另一方面，route除了创建externalService类型的service用于将流量指向istio网关外，同时还创建了clusterIngress（作用很重要）。 clusterIngress是对各种可用于knative流量入口组件的抽象。对于底层是istio的环境，networkController会将clusterIngress资源转化为istio的virtualService配置，从而提供将外部流量转发到集群内部的功能。 对于revision来讲，其controller一方面基于资源描述创建出deployment、imageCache等资源；另一方面，为了提供serverless功能，controller还对应创建了podAutoScaler。需要知道的是podAutoScaler有两种实现，分别对应kpa和hpa。 autoscaler controller又实现了基于podAutoScaler创建对应的sks(serverlessService)。如果策略是基于kpa的，就需要一套监控流量和并发请求量的机制，于是又创建了private和public的service专用于访问实例上监控组件sidecar的端口。 代码实现 上面的分析，具体的代码实现流程见下图。 流量转发当外部流量需要访问内部服务时，其流量的转发流程如何？接下来分小节介绍主要的流量转发逻辑。 流量入口服务映射之前在将CRD资源的时候有提到clusterIngress资源，以及对应的的ExternalService类型的service，这里是一个简单的实例。 1234567891011[root@k8s-master knative]# kc describe svc autoscale-goName: autoscale-goNamespace: defaultLabels: serving.knative.dev/route=autoscale-goAnnotations: &lt;none&gt;Selector: &lt;none&gt;Type: ExternalNameIP:External Name: istio-ingressgateway.istio-system.svc.cluster.localSession Affinity: NoneEvents: &lt;none&gt; 该service的作用是，当请求autoscale-go.default.example.com时（cluster名字换成环境的cluster name），DNS会直接返回external nameistio-ingressgateway.istio-system.svc.cluster.local作为响应。相当于就将对autoscale-go服务的访问，重定向到了istio的ingressgateway上。而Istio上早已按照clusterIngress的要求，配置好了流量转发规则。 转发规则serverlessservice有两种模式，proxy和service。其中proxy会将流量转发到activator上，而service则会将流量转发到对应的后端实例上真正处理业务。 我们先假设此时后端的deployment并没有启动起来，或者是很久没有请求流量，pod已经被autoscaler出于节约资源消耗的目的干掉了，即serverlessservice处于proxy模式。此时istio的配置如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master ~]# istioctl pc listener istio-ingressgateway-67cbb7f6c6-bqv2h.istio-systemADDRESS PORT TYPE0.0.0.0 80 HTTP0.0.0.0 15090 HTTP[root@k8s-master ~]# istioctl pc route istio-ingressgateway-67cbb7f6c6-bqv2h.istio-system -o json[ &#123; \"name\": \"http.80\", \"virtualHosts\": [ &#123; \"name\": \"autoscale-go.default.example.com:80\", \"domains\": [ \"autoscale-go.default.example.com\", \"autoscale-go.default.example.com:80\" ], \"routes\": [ &#123; \"match\": &#123; \"prefix\": \"/\", \"headers\": [ &#123; \"name\": \":authority\", \"regexMatch\": \"^autoscale-go\\\\.default(?::\\\\d&#123;1,5&#125;)?$\" &#125; ] &#125;, \"route\": &#123; \"cluster\": \"outbound|80||autoscale-go-52f52.default.svc.cluster.local\", \"timeout\": \"600s\", \"retryPolicy\": &#123; \"retryOn\": \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\", \"numRetries\": 3, \"perTryTimeout\": \"600s\", \"retryHostPredicate\": [ &#123; \"name\": \"envoy.retry_host_predicates.previous_hosts\" &#125; ], \"hostSelectionRetryMaxAttempts\": \"3\", \"retriableStatusCodes\": [ 503 ] &#125;, \"maxGrpcTimeout\": \"600s\" &#125;, ... &#125; ] &#125; ] &#125;] 查看EDS的配置信息，发现其对应的路由为IP10.244.0.234的8012端口。 123456789101112131415161718[root@k8s-master ~]# istioctl pc endpoint istio-ingressgateway-67cbb7f6c6-bqv2h.istio-system -o json&#123; ... \"name\": \"outbound|80||autoscale-go-52f52.default.svc.cluster.local\", \"addedViaApi\": true, \"hostStatuses\": [ &#123; \"address\": &#123; \"socketAddress\": &#123; \"address\": \"10.244.0.234\", \"portValue\": 8012 &#125; &#125;, ... &#125; ]&#125; 在来看此时该IP是activator对应的pod IP地址。 12[root@k8s-master ~]# kc get pod -o wide --all-namespaces | grep 10.244.0.234knative-serving activator-5b7d897458-xv4tp 1/1 Running 0 132m 10.244.0.234 k8s-master &lt;none&gt; &lt;none&gt; 查看service上面的流量以及对应的转发目标变为activator。 12345678910111213141516[root@k8s-master knative]# kc get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 7d2hautoscale-go ExternalName &lt;none&gt; istio-ingressgateway.istio-system.svc.cluster.local &lt;none&gt; 148mautoscale-go-52f52 ClusterIP 10.108.134.58 &lt;none&gt; 80/TCP 148mautoscale-go-52f52-metrics ClusterIP 10.109.214.47 &lt;none&gt; 9090/TCP 148mautoscale-go-52f52-priv ClusterIP 10.106.251.157 &lt;none&gt; 80/TCP 148mkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 66dprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 7d2h[root@k8s-master knative]# kc get epNAME ENDPOINTS AGEautoscale-go-52f52 10.244.0.234:9090,10.244.0.234:8012,10.244.0.234:8013 148mautoscale-go-52f52-metrics &lt;none&gt; 148mautoscale-go-52f52-priv &lt;none&gt; 148m 所以，我们可以判断：当很久没有流量请求的时候，serverlessservice会切换到proxy模式，其autoscaler逻辑会将pod全干掉；而此时，为了检测到外部的流量请求，Istio将流量转发到activator上面。 弹性伸缩通过上面的分析，相信大家对serverlessservice的弹性伸缩都有了一个直观的了解。接下来该部分通过两个小节介绍弹性伸缩的工作原理。 弹性伸缩分为hpa和kpa: hpa：基于一些监控指标，比如CPU、memory等的情况来决定是否需要扩缩容pod的数量，另外hpa不支持将实例数降到0个。 kpa：基于监控到的并发请求数来自动弹性伸缩实例数，当并发搞的时候就扩容，当持续一段时间没有请求的时候，就让pod休假，将流量转给看门人activator来中转。一旦有新请求，立马召回pod，并对外服务。 触发器前面提到的activator就是触发器，它的作用就是为那些几乎没有流量访问的服务充当看门人。显然这个看门人并不是只为某个VIP客户服务，而是为大家一起服务的。那activator都做了些什么？当流量到了的时候，它又是如何通知正处于放空状态的服务实例的呢？ 作用 当service的实例数为0的时候，activator代替service接收流量，统计流量和并发数量，并通过websocket上报到autoscaler服务。 当service的实例启动起来后，activator通过获取revision和serverlessservice来找到对应的service，并探测该service的服务是否已经可以访问，一旦准备好，就发送流量。 通过一系列的handler链来打印日志，记录trace信息，限速，零时存储请求内容，响应probe和healthcheck等。 核心工作流程 作为代理看门人，activator收到外部访问service的请求后，第一件事请就是去除报文中的header信息，按照revision name作为key产生一条event。 如果从来没有改key的event，就可以判断这是到某一个客户的第一条请求，此时看门人需要立刻通过websocket上报到autoscaler（autoscaler的逻辑会创建service的实例）。 当然，activator并非只做这些，它还记录日志，trace信息等，最重要的一点，activator会基于报文header中的信息来知道其所请求的revision以及sks。 通过查询sks的privateService来找到该service的healthCheck probe地址，探测其服务是否ready。一旦probe成功就会将缓存的报文发送到目标服务。 代码实现 其代码住逻辑分为三块，也对应三个goroutine，分别是： 监控上报 监控数据统计 报文处理（包含多个处理链） 弹性伸缩逻辑在触发器的流程中，我们提到当触发器发现某个报文是对某个revision的第一个请求时，会通过websocket上报到autoscaler controller。接下来，我们看看autoscaler的处理流程。 功能 autoscalr的核心作用就是做弹性伸缩的决策，为了做决策，它需要实现对上报监控指标的采集，这又分为两种形式。 通过websocket上报来收集activator的报文请求指标； 通过定时pull的形势来从metrics service获取queue中统计的指标； 基于metric指标来做弹性伸缩决策并下发。 代码实现 由于这里重点讲流量，该代码基本是按照功能来实现，这里就不再做讲解。 这里重点讲一下autoscaler将service的pod拉起来了之后的动作，通过代码里面可以看到，ks.applyScale最后会走到c.reconcileSKS。这里其实是更改serverlessservice的模式，从proxy切换到service，而接下来的又做了什么？请参考service controller的逻辑。 当pod启动起来之后，对应的private Service会基于selector而选中新起来的pod，从而监控流量将从pod的8012端口采集。 流量监控上面提到，当pod启动之后，autoscaler会从pod的8012端口采集监控指标。这里的监控指标是如何产生的？应用需要关注吗？这些问题在这一节来解答。 作用 首先，knative中的revision在创建deploy的时候，会为其自动加入一个sidecar，这个container就是queue-proxy;在整个平台中，queue-proxy的作用非常重要。 基于一层反向代理，收集访问业务container的流量情况，对外暴露9090(即：metrics的访问端口) 在反向代理上，基于配置的并发数来限制外部访问的请求速率； 对外提供对主容器进行healthcheck以及drain的接口（admin的端口:8022)，(业务代理端口: http1:8012，http2:8013）; 代码实现 实验 autoscaler通过周期性的抓取每一个业务容器对应queue-proxy的metrics来感知实时流量情况的。当外部往service发包的时候，在pod的queue container网卡上抓包可以发现有很多的prometheus metrics的报文，collector拉取指标的频率为每秒4次，具体指标见下图的抓包细节： 在报文里面发现来收集metrics的IP地址为10.244.0.235, 通过在k8s里面查询，发现该IP为autoscaler的地址。 12[root@k8s-master ~]# kc get pod -o wide --all-namespaces | grep 10.244.0.235knative-serving autoscaler-74f47bfff8-7znzv 1/1 Running 0 15d 10.244.0.235 k8s-master &lt;none&gt; &lt;none&gt; 当服务启动后，各服务的具体内容如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master ~]# kc get serverlessserviceNAME SERVICENAME PRIVATESERVICENAME READY REASONautoscale-go-52f52 autoscale-go-52f52 autoscale-go-52f52-priv True[root@k8s-master ~]# kc get revisionsNAME SERVICE NAME GENERATION READY REASONautoscale-go-52f52 autoscale-go-52f52 1 True[root@k8s-master ~]# kc get deployNAME READY UP-TO-DATE AVAILABLE AGEautoscale-go-52f52-deployment 1/1 1 1 141m[root@k8s-master ~]# kc get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEautoscale-go ExternalName &lt;none&gt; istio-ingressgateway.istio-system.svc.cluster.local &lt;none&gt; 142mautoscale-go-52f52 ClusterIP 10.108.134.58 &lt;none&gt; 80/TCP 142mautoscale-go-52f52-metrics ClusterIP 10.109.214.47 &lt;none&gt; 9090/TCP 142mautoscale-go-52f52-priv ClusterIP 10.106.251.157 &lt;none&gt; 80/TCP 142m[root@k8s-master ~]# kc get epNAME ENDPOINTS AGEalertmanager-operated &lt;none&gt; 7d2hautoscale-go-52f52 10.244.0.18:8012 142mautoscale-go-52f52-metrics 10.244.0.18:9090 142mautoscale-go-52f52-priv 10.244.0.18:8012 142mkubernetes 10.200.204.76:6443 66dprometheus-operated &lt;none&gt; 7d2h[root@k8s-master ~]# istioctl psNAME CDS LDS EDS RDS PILOT VERSIONistio-ingressgateway-67cbb7f6c6-bqv2h.istio-system SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-75984f55cc-5brpc 1.1.3[root@k8s-master ~]# istioctl pc endpoint istio-ingressgateway-67cbb7f6c6-bqv2h.istio-system -o json&#123; \"name\": \"outbound|80||autoscale-go-52f52.default.svc.cluster.local\", \"addedViaApi\": true, \"hostStatuses\": [ &#123; \"address\": &#123; \"socketAddress\": &#123; \"address\": \"10.244.0.18\", \"portValue\": 8012 &#125; &#125;, ... &#125; ]&#125;","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"},{"name":"autoscale","slug":"autoscale","permalink":"http://ljchen.net/tags/autoscale/"},{"name":"serverless","slug":"serverless","permalink":"http://ljchen.net/tags/serverless/"}]},{"title":"Knative Build实现","slug":"knative-build实现","date":"2019-06-19T13:46:35.000Z","updated":"2020-04-06T09:24:01.483Z","comments":true,"path":"2019/06/19/knative-build实现/","link":"","permalink":"http://ljchen.net/2019/06/19/knative-build实现/","excerpt":"knative-build提供将源码编译、构建、打包的整个流水线流程。其核心原理是基于pod的init contaier来定义一系列串行执行的stage。由于每一个stage都可以由用户自定义，若加以扩展，还可以用于更多场景。","text":"knative-build提供将源码编译、构建、打包的整个流水线流程。其核心原理是基于pod的init contaier来定义一系列串行执行的stage。由于每一个stage都可以由用户自定义，若加以扩展，还可以用于更多场景。 资源模型 build一条具体可运行的pipeline任务，里面包括多个stage（第一个stage为从代码仓库拉取源码） step组成pipeline的一个执行阶段，其底层通过使用init container来实现。 build-template对同一类pipeline任务流程的抽象，通常用户指定参数的形势来实例化为具体的build。 Builder就是build中的每一个step所使用的构建基础工具或环境，通常通过基础镜像的形势来指定。这里对常用的集中builder做简单介绍。 bazel在buildTemplate中，对应使用的builder为镜像 gcr.io/cloud-builders/bazel。 12345678910apiVersion: build.knative.dev/v1alpha1kind: BuildTemplatemetadata: name: bazelspec: ... steps: - name: build-and-push image: gcr.io/cloud-builders/bazel args: ['run', '$&#123;TARGET&#125;'] kanikokaniko貌似是通过dockerfile来生成镜像的，这里需要指定dockerfile目录以及目标镜像的名称。其builder使用的image为 gcr.io/kaniko-project/executor，除此之外，google来提供了很多的工具，可以在各个阶段中用到，也可以用来扩展buildTemplate的steps。 12345678910111213141516apiVersion: build.knative.dev/v1alpha1kind: BuildTemplatemetadata: name: kanikospec: parameters: ... steps: - name: build-and-push image: gcr.io/kaniko-project/executor args: - --dockerfile=$&#123;DOCKERFILE&#125; - --destination=$&#123;IMAGE&#125; env: - name: DOCKER_CONFIG value: /builder/home/.docker buildkitbuildkit是docker提供出来的，运行这个buildTemplate的build-and-push step其实是运行了一个buildkit的client。该client向运行在远端的buildkit server发送构建的任务，其具体流程可以参考docker buildkit项目。 1234567891011121314151617181920apiVersion: build.knative.dev/v1alpha1kind: BuildTemplatemetadata: name: buildkitspec: parameters: ... - name: BUILDKIT_DAEMON_ADDRESS description: The address of the BuildKit daemon (buildkitd) service default: \"tcp://buildkitd:1234\" steps: - name: build-and-push image: $&#123;BUILDKIT_CLIENT_IMAGE&#125; workingDir: $&#123;DIRECTORY&#125; command: [\"buildctl\", \"--addr=$&#123;BUILDKIT_DAEMON_ADDRESS&#125;\", \"build\", \"--progress=plain\", \"--frontend=dockerfile.v0\", \"--opt\", \"filename=$&#123;DOCKERFILE&#125;\", \"--local\", \"context=.\", \"--local\", \"dockerfile=.\", \"--output\", \"type=image,name=$&#123;IMAGE&#125;,push=$&#123;PUSH&#125;\"] 代码实现knative-build的代码主要实现对三种CRD资源的controller。三种CRS资源分别是： build buildTemplate clusterBuildTemplate 下面是核心代码的主流程，如下图所示： Controller在后两种template资源的controller代码中，到检测到有新的template创建时，会创建对应的builder镜像（通过get image可以查询）的操作。template资源的功能主要是为实例化build提供一种快速生成的模板和工具。 再来说build，build的operator首先判断该build是否基于buildTemplate创建。如果是，就将输入参数应用到模板，再将包含多个steps的完整build资源转化为由多个initContainer串行的pod。值得注意的是，build的前两个阶段一般是挂载秘钥volume（后面会再讲到）和拉取source code的initContainer，但该字段并非必须。 对于拉取代码，这里有三种情况，见下表： 代码拉取方式 image 备注 git git-init 基于git拉取代码 gcs gcs-fetcher google专用 custom 无 该情况适用于用户通过volume挂载的方式来获取源码 12345678910111213141516171819202122232425for i, source := range sources &#123; switch &#123; case source.Git != nil: git, err := gitToContainer(source, i) if err != nil &#123; return nil, err &#125; initContainers = append(initContainers, *git) case source.GCS != nil: gcs, err := gcsToContainer(source, i) if err != nil &#123; return nil, err &#125; initContainers = append(initContainers, *gcs) case source.Custom != nil: cust, err := customToContainer(source.Custom, source.Name) if err != nil &#123; return nil, err &#125; // Prepend the custom container to the steps, to be augmented later with env, volume mounts, etc. build.Spec.Steps = append([]corev1.Container&#123;*cust&#125;, build.Spec.Steps...) &#125; // webhook validation checks that only one source has subPath defined workspaceSubPath = source.SubPath&#125; 基于远端仓库拉取代码的方式，大多的代码仓库都需要做认证。这块knative是使用 creds-init 来实现的，该镜像运行起来后会生成一个initContainer。该initContainer通过找出serviceaccount下匹配builder的sercret，然后通过挂载volume的形势，将该secret挂载到pod对应的目录下，供builder在拉取代码的时候使用。 Imageknative启动时，指定了了四个镜像，其中的三个在前面已经提到过。nopImage用于在所有initContainer执行完之后运行的pod，该container并不做实质性的工作，只用来结束串行的initContainer；其代码很简单，只是在main函数中打印了成功执行完build的提示消息。 12345678910111213var ( // The container used to initialize credentials before the build runs. credsImage = flag.String(\"creds-image\", \"override-with-creds:latest\", \"The container image for preparing our Build's credentials.\") // The container with Git that we use to implement the Git source step. gitImage = flag.String(\"git-image\", \"override-with-git:latest\", \"The container image containing our Git binary.\") // The container that just prints build successful. nopImage = flag.String(\"nop-image\", \"override-with-nop:latest\", \"The container image run at the end of the build to log build success\") gcsFetcherImage = flag.String(\"gcs-fetcher-image\", \"gcr.io/cloud-builders/gcs-fetcher:latest\", \"The container image containing our GCS fetcher binary.\")) 下面是启动后四个images的CRD信息。 123456[root@k8s-master ~]# kkb get imagesNAME AGEcreds-init 13dgcs-fetcher 13dgit-init 13dnop 13d","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"}]},{"title":"K8s Api-Server流程简述","slug":"k8s-api-server流程简述","date":"2019-06-16T01:06:16.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2019/06/16/k8s-api-server流程简述/","link":"","permalink":"http://ljchen.net/2019/06/16/k8s-api-server流程简述/","excerpt":"最近开发环境的k8s出现过一次读取不出来configmap，并导致api-server OOM重启的现象，供应商说是因为configmap条目太多，导致api-server大量decode etcd的数据为yaml所致。在原生k8s上反复重试过多次，均无法复现该现象，所以又看了一遍api-server大致的代码逻辑，这里简单记录一下。","text":"最近开发环境的k8s出现过一次读取不出来configmap，并导致api-server OOM重启的现象，供应商说是因为configmap条目太多，导致api-server大量decode etcd的数据为yaml所致。在原生k8s上反复重试过多次，均无法复现该现象，所以又看了一遍api-server大致的代码逻辑，这里简单记录一下。 核心业务逻辑在看代码之前，基于对k8s架构的理解，我猜测：api-server应该就是对etcd的一层代理。其中需要实现了对各种资源的路由、准入控制、限速、资源的格式转换以及各种操作的接口封装等功能。具体代码逻辑又实现了些什么呢？下面来简单介绍。 其实和猜测的差不多，从入口之后，api-server就调用CreateServerChain来创建了一系列的服务，包括： KubeApiServer也就是为k8s定义的抽象资源（比如workload，service，configmap等）提供服务； ApiExtensionsServer主要负责CRD相关的服务； AggregatorServer这个不太熟，但是顾名思义，应该就是服务api aggregator的。 代码框架先上一张大图，也是为了之后看图就能够快速定位代码逻辑。 前面说的那三种server，其实就在大图中左上角部分，下面在 #总入口 中要重点讲下router这部分的逻辑。 总入口接下来，我们找熟悉的KubeApiServer分析。函数CreateKubeAPIServer调用了kubeAPIServerConfig.Complete().New。通过该函数，基本看出了端倪，api-server是通过webserver的形势对外提供api服务的；该函数首先准备了一堆的资源组，然后将这些RESTStorageProvider安装到了Master，这里可以理解为准备router信息（较抽象）。 1234567891011121314151617181920212223242526func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*Master, error) &#123; // 准备所有的资源组 restStorageProviders := []RESTStorageProvider&#123; auditregistrationrest.RESTStorageProvider&#123;&#125;, authenticationrest.RESTStorageProvider&#123;Authenticator: c.GenericConfig.Authentication.Authenticator, APIAudiences: c.GenericConfig.Authentication.APIAudiences&#125;, authorizationrest.RESTStorageProvider&#123;Authorizer: c.GenericConfig.Authorization.Authorizer, RuleResolver: c.GenericConfig.RuleResolver&#125;, autoscalingrest.RESTStorageProvider&#123;&#125;, batchrest.RESTStorageProvider&#123;&#125;, certificatesrest.RESTStorageProvider&#123;&#125;, coordinationrest.RESTStorageProvider&#123;&#125;, extensionsrest.RESTStorageProvider&#123;&#125;, networkingrest.RESTStorageProvider&#123;&#125;, noderest.RESTStorageProvider&#123;&#125;, policyrest.RESTStorageProvider&#123;&#125;, rbacrest.RESTStorageProvider&#123;Authorizer: c.GenericConfig.Authorization.Authorizer&#125;, schedulingrest.RESTStorageProvider&#123;&#125;, settingsrest.RESTStorageProvider&#123;&#125;, storagerest.RESTStorageProvider&#123;&#125;, appsrest.RESTStorageProvider&#123;&#125;, admissionregistrationrest.RESTStorageProvider&#123;&#125;, eventsrest.RESTStorageProvider&#123;TTL: c.ExtraConfig.EventTTL&#125;, &#125; // 第二个重点, 安装资源handler了 m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...)&#125; 以上只是粗枝大叶的入门，如果要对资源router的挂载搞透彻，我们还需要理解进一步理解这里的资源，以及InstallAPIs的流程。 资源组织k8s api的定义中，对rest-api操作的对象都按照资源组，版本的形势做了组织。要查看所有支持的资源组和版本信息可以执行以下操作： 1234567~ $ kc get apiservicesNAME SERVICE AVAILABLE AGEv1. Local True 3d11hv1.apps Local True 3d11hv1beta1.apps Local True 3d11hv1beta2.apps Local True 3d11h... 下面会涉及到两个概念：资源组和资源。 资源组这里，我们就使用 appsrest.RESTStorageProvider{} 这个最成熟、大家最熟悉的资源组（apps）来分析。 首先看下图目录结构，位于k8s项目pkg/registry/目录下的这些子目录都是k8s支持的资源组类型。这里的apps就是其中之一，apps下面又包含了常见的deployment等workloads。 每一个资源组目录下都包含了一个rest目录，里面放的不是子资源，而是该资源组的RESTStorageProvider接口实现。该接口的NewRESTStorage方法将资源组下的所有资源的storage都以版本信息作key填充到apiGroupInfo.VersionedResourcesStorageMap字典中。前面提到的m.InstallAPIs就是遍历字典，将所有资源组下的资源逐一安装到master router中。 顶层storage到此，我们已经清楚，每添加一个资源组，在api-server里面都需要把它加入到apiGroupInfo.VersionedResourcesStorageMap中，这样才会被安装到router里面。顶层的storage就是干这个事情的。 来看rest目录中storage_apps.go里面资源组的版本和子资源是如何组织的代码。下面可以看到，对于apps这个资源组，针对v1beta1这个版本，添加了deployments这个资源，而这个组织好的结构就是一个storage。 123456789101112131415161718192021// 该函数将各个版本的storage填充到字典中func (p RESTStorageProvider) NewRESTStorage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (genericapiserver.APIGroupInfo, bool) &#123; ... if apiResourceConfigSource.VersionEnabled(appsapiv1beta1.SchemeGroupVersion) &#123; apiGroupInfo.VersionedResourcesStorageMap[appsapiv1beta1.SchemeGroupVersion.Version] = p.v1beta1Storage(apiResourceConfigSource, restOptionsGetter) &#125; ... return apiGroupInfo, true&#125;// 该函数在准备单个版本所有的子资源func (p RESTStorageProvider) v1beta1Storage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) map[string]rest.Storage &#123; storage := map[string]rest.Storage&#123;&#125; //这里要再看NewStorage的代码，注意它的参数，这在后面会提到 deploymentStorage := deploymentstore.NewStorage(restOptionsGetter) storage[\"deployments\"] = deploymentStorage.Deployment ... return storage&#125; 接下来，我们将进入到deployment资源相关的子目录分析。 资源通过目录结构图可以知道，每一个资源下都包含了storage和strategy目录。 storage其中storage中定义了NewREST，还通过REST对外统一提供操作资源的一些操作接口（如New/Get/Update等）；这里将NewStorage和DeploymentStorage都一并附上。 123456789101112131415161718type DeploymentStorage struct &#123; Deployment *REST // 这些类型都是包含了 *genericregistry.Store Status *StatusREST // 这些类型都是包含了 *genericregistry.Store Scale *ScaleREST // 这些类型都是包含了 *genericregistry.Store Rollback *RollbackREST // 这些类型都是包含了 *genericregistry.Store&#125;func NewStorage(optsGetter generic.RESTOptionsGetter) DeploymentStorage &#123; // 这里调用了 NewREST，参数还是前面 NewStorage 的参数一如既往的往下传 deploymentRest, deploymentStatusRest, deploymentRollbackRest := NewREST(optsGetter) return DeploymentStorage&#123; Deployment: deploymentRest, Status: deploymentStatusRest, Scale: &amp;ScaleREST&#123;store: deploymentRest.Store&#125;, Rollback: deploymentRollbackRest, &#125;&#125; 在NewREST返回了三个REST，每一个REST里面都包含了 *genericregistry.Store， 该store类型重点实现下面三个方法： 12345678910111213func NewREST(optsGetter generic.RESTOptionsGetter) (*REST, *StatusREST, *RollbackREST) &#123; store := &amp;genericregistry.Store&#123; NewFunc: func() runtime.Object &#123; return &amp;apps.Deployment&#123;&#125; &#125;, NewListFunc: func() runtime.Object &#123; return &amp;apps.DeploymentList&#123;&#125; &#125;, DefaultQualifiedResource: apps.Resource(\"deployments\"), CreateStrategy: deployment.Strategy, // 重点 UpdateStrategy: deployment.Strategy, // 重点 DeleteStrategy: deployment.Strategy, // 重点 ... &#125; ...&#125; strategy接下来就将注意力转移到deployment.Stategy上来了。deploymentStrategy里面主要实现了对资源做增删改前后的各种校验和准备工作，这里就不再详细的讲述。 etcd/cache前面讲NewREST的时候，提到了其参数，其实该参数的最终值是这样赋值得到的： 1genericConfig.RESTOptionsGetter = &amp;genericoptions.SimpleRestOptionsFactory&#123;Options: etcdOptions&#125; 获取该值的时候，通过调用该factory的GetRESTOptions方法. 1234567891011121314151617func (f *SimpleRestOptionsFactory) GetRESTOptions(resource schema.GroupResource) (generic.RESTOptions, error) &#123; ... // 重要 if f.Options.EnableWatchCache &#123; sizes, err := ParseWatchCacheSizes(f.Options.WatchCacheSizes) if err != nil &#123; return generic.RESTOptions&#123;&#125;, err &#125; cacheSize, ok := sizes[resource] if !ok &#123; cacheSize = f.Options.DefaultWatchCacheSize &#125; // 重要 ret.Decorator = genericregistry.StorageWithCacher(cacheSize) &#125; return ret, nil&#125; 这里第一个要点是使用了一个api-server的参数，确定是否需要使用watchCache，这里一并把api-server与cache有关的两个参数都列出来。 –watch-cache Default: true Enable watch caching in the apiserver –watch-cache-sizes stringSlice Watch cache size settings for some resources (pods, nodes, etc.), comma separated. The individual setting format: resource[.group]#size, where resource is lowercase plural (no version), group is omitted for resources of apiVersion v1 (the legacy core API) and included for others, and size is a number. It takes effect when watch-cache is enabled. Some resources (replicationcontrollers, endpoints, nodes, pods, services, apiservices.apiregistration.k8s.io) have system defaults set by heuristics, others default to default-watch-cache-size 第二个要点是真正的初始化一个storageWithCacher的decorator，也就是在这个函数里面，初始化了backstorage之上的一层cache。该函数显示调用generic.NewRawStorage来new出一个backupstore，然后再基于此封装一层cache。 12345func NewRawStorage(config *storagebackend.Config) (storage.Interface, factory.DestroyFunc) &#123; s, d, err := factory.Create(*config) ... return s, d&#125; 这里的factory其实就是在backstore里面对etcd2和etcd3的一层抽象，默认是使用etcd3。另外，对于etcd3，除了封装kv操作之外，这里还引入了自动定时compact etcd已删除version数据的操作，当前默认是5分钟一次。这里就不再一一粘贴，可以去查一下代码的实现。 12345678910func Create(c storagebackend.Config) (storage.Interface, DestroyFunc, error) &#123; switch c.Type &#123; case \"etcd2\": return nil, nil, fmt.Errorf(\"%v is no longer a supported storage backend\", c.Type) case storagebackend.StorageTypeUnset, storagebackend.StorageTypeETCD3: return newETCD3Storage(c) default: return nil, nil, fmt.Errorf(\"unknown storage type: %s\", c.Type) &#125;&#125; 安装路由讲完资源组和资源的内容，我们再来看安装router的时候都做了什么。 123456789101112131415161718192021func (m *Master) InstallAPIs(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter, restStorageProviders ...RESTStorageProvider) &#123; apiGroupsInfo := []*genericapiserver.APIGroupInfo&#123;&#125; for _, restStorageBuilder := range restStorageProviders &#123; groupName := restStorageBuilder.GroupName() ... // 注意这里调用了 NewRESTStorage apiGroupInfo, enabled := restStorageBuilder.NewRESTStorage(apiResourceConfigSource, restOptionsGetter) if !enabled &#123; klog.Warningf(\"Problem initializing API group %q, skipping.\", groupName) continue &#125; ... apiGroupsInfo = append(apiGroupsInfo, &amp;apiGroupInfo) &#125; // 这里又进一层函数，到里层去安装router if err := m.GenericAPIServer.InstallAPIGroups(apiGroupsInfo...); err != nil &#123; klog.Fatalf(\"Error in registering group versions: %v\", err) &#125;&#125; 该函数里面调用的 NewRESTStorage 有没有似曾相识？对，就是我们在资源组的RESTStorageProvider里面实现的方法。看到这里，才将各资源组的实现与安装router结合起来。绕了一大圈，其实是为了干这事！ 真正将api资源安装到router的代码，看m.GenericAPIServer.InstallAPIGroups。 12345678910111213func (s *GenericAPIServer) InstallAPIGroups(apiGroupInfos ...*APIGroupInfo) error &#123; ... for _, apiGroupInfo := range apiGroupInfos &#123; // 看这里 if err := s.installAPIResources(APIGroupPrefix, apiGroupInfo, openAPIModels); err != nil &#123; return fmt.Errorf(\"unable to install api resources: %v\", err) &#125; ... // 看这里 s.Handler.GoRestfulContainer.Add(discovery.NewAPIGroupHandler(s.Serializer, apiGroup).WebService()) &#125; return nil&#125; 到此，api-server的基本代码流程就简单过了一遍。当然，这只是其中一个面，带着不同的问题去看代码，梳理的线索不一样，应该有不同的收获。先就写到这里吧，有时间再看看梳理下其他方面。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"CKA","slug":"CKA","permalink":"http://ljchen.net/tags/CKA/"},{"name":"api-server","slug":"api-server","permalink":"http://ljchen.net/tags/api-server/"}]},{"title":"K8s Client代码自动生成","slug":"K8s-client代码自动生成","date":"2019-06-14T13:50:06.000Z","updated":"2020-04-06T09:24:01.467Z","comments":true,"path":"2019/06/14/K8s-client代码自动生成/","link":"","permalink":"http://ljchen.net/2019/06/14/K8s-client代码自动生成/","excerpt":"在Istio和Knative，以及众多中间件operator中都会经常看到有使用informer去watch自定义CRD资源，然后reconsile的代码。其实每一个controller/operator都是使用api-server的客户端在操作资源。对于k8s支持的核心资源类型可以直接引用client-go，对于CRD，当前主要有三种方案。","text":"在Istio和Knative，以及众多中间件operator中都会经常看到有使用informer去watch自定义CRD资源，然后reconsile的代码。其实每一个controller/operator都是使用api-server的客户端在操作资源。对于k8s支持的核心资源类型可以直接引用client-go，对于CRD，当前主要有三种方案。 使用原生k8s的code-generator生成client代码 直接使用kubebuilder，生成controller框架的同时自动生成client代码 使用coreos和redhat提供的operator framework（有点类似于第二种） code-generator介绍项目目录： k8s.io/code-generator 对于初学者来讲，后两种方式都比较容易上手，开发者可以将关注点集中在怎么实现controller的业务上面，而不用去关心client的实现。但是，考虑到经常阅读源代码，时常接触到一些高度一致的目录结构而一头雾水；同时，便于更高效的阅读k8s相关项目的角度，我们接下来重点聊下k8s原生code-generator生成client库的过程，以及对应的目录结构的内容。 代码生成为啥要生成client库的代码？显然是因为我们定义了k8s core resource之外的CRD，那么在使用脚本自动生成cliet代码之前，很显然我们需要做一些对资源的定义（除了资源的定义之外，重点还有一系列的注释，这些注释有特定的语法，这里不做详述，具体可以参考后记中推荐的官方文档），这样脚本才可能生成我们所需的代码。接下来分两步演示：1. 预定义资源 2. 生成客户端代码 定义资源准备controller项目路径，这一步需要考虑好controller的命名，CRD资源的版本号，这些信息需要用来命名目录。 12345# 创建目录 (在$GOPATH/k8s.io下执行命令)mkdir -p project/pkg/apis/controller/v1# 进入controller目录cd project/pkg/apis/controller 这里需要预先按照对应的结构生成几个文件, 目录结构如图所示： 12345678910~/gopath/src/k8s.io/project  tree.└── pkg └── apis └── controller ├── register.go └── v1 ├── doc.go ├── register.go └── types.go 最顶层的register.go定义了groupName； 在版本号目录下又有三个文件： doc.go定义了版本的包名； types.go定义资源的数据结构； register.go真正用于注册该版本下的资源。 顶层register.go 12345678# 生成顶层register.gocat &lt;&lt; EOF &gt;&gt; register.gopackage controllerconst ( GroupName = \"controller.ljchen.net\")EOF doc.go注意，类似于下面的注释语句 // +k8s:deepcopy-gen=package 和 // +groupName=controller.ljchen.net都是code-generator的语法，详情请参考 #后记 中推荐的文章。 1234567891011# 进入v1目录，生成doc.go/types.go/register.gocd v1# 生成doc.gocat &lt;&lt; EOF &gt;&gt; doc.go// +k8s:deepcopy-gen=package// +groupName=controller.ljchen.net// Package v1 is the v1 version of the API.package v1EOF types.go 1234567891011121314151617181920212223242526272829303132333435363738394041# 生成types.gocat &lt;&lt; EOF &gt;&gt; types.gopackage v1import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\")// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// Foo is a specification for a Foo resourcetype Foo struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec FooSpec `json:\"spec\"` Status FooStatus `json:\"status\"`&#125;// FooSpec is the spec for a Foo resourcetype FooSpec struct &#123; DeploymentName string `json:\"deploymentName\"` Replicas *int32 `json:\"replicas\"`&#125;// FooStatus is the status for a Foo resourcetype FooStatus struct &#123; AvailableReplicas int32 `json:\"availableReplicas\"`&#125;// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// FooList is a list of Foo resourcestype FooList struct &#123; metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []Foo `json:\"items\"`&#125;EOF register.go 12345678910111213141516171819202122232425262728293031323334353637383940# 生成register.gocat &lt;&lt; EOF &gt;&gt; register.gopackage v1import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" samplecontroller \"k8s.io/project/pkg/apis/controller\")// SchemeGroupVersion is group version used to register these objectsvar SchemeGroupVersion = schema.GroupVersion&#123;Group: samplecontroller.GroupName, Version: \"v1\"&#125;// Kind takes an unqualified kind and returns back a Group qualified GroupKindfunc Kind(kind string) schema.GroupKind &#123; return SchemeGroupVersion.WithKind(kind).GroupKind()&#125;// Resource takes an unqualified resource and returns a Group qualified GroupResourcefunc Resource(resource string) schema.GroupResource &#123; return SchemeGroupVersion.WithResource(resource).GroupResource()&#125;var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme)// Adds the list of known types to Scheme.func addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes(SchemeGroupVersion, &amp;Foo&#123;&#125;, &amp;FooList&#123;&#125;, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125;EOF 生成客户端代码自动生成client代码的脚本位于code-generator项目中，generate-groups.sh是对项目中多个二进制文件编排的整合。 脚本用法如下： generate-groups.sh {generators} {output-package} {apis-package} {groups-versions} … 下面就使用以上脚本来自动生成刚才我们所写资源的client库。请千万注意这里的执行路径和参数的路径！ 1234567891011~$ pwd~$ /gopath/src/k8s.io~$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all \\ k8s.io/project/pkg/generated \\ # 注意路径 k8s.io/project/pkg/apis \\ controller:v1Generating deepcopy funcsGenerating clientset for controller:v1 at k8s.io/project/pkg/generated/clientsetGenerating listers for controller:v1 at k8s.io/project/pkg/generated/listersGenerating informers for controller:v1 at k8s.io/project/pkg/generated/informers 生成后的目录结构如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 ~/gopath/src/k8s.io/project  tree.└── pkg ├── apis │ └── controller │ ├── register.go │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── generated ├── clientset │ └── versioned │ ├── clientset.go │ ├── doc.go │ ├── fake │ │ ├── clientset_generated.go │ │ ├── doc.go │ │ └── register.go │ ├── scheme │ │ ├── doc.go │ │ └── register.go │ └── typed │ └── controller │ └── v1 │ ├── controller_client.go │ ├── doc.go │ ├── fake │ │ ├── doc.go │ │ ├── fake_controller_client.go │ │ └── fake_foo.go │ ├── foo.go │ └── generated_expansion.go ├── informers │ └── internalversion │ ├── factory.go │ ├── generic.go │ ├── internalinterfaces │ │ └── factory_interfaces.go │ └── v1 │ ├── interface.go │ └── internalversion │ ├── foo.go │ └── interface.go └── listers └── v1 └── internalversion ├── expansion_generated.go └── foo.go 后记客户端代码生成后，接下来就是实现自己的controller代码了。这块可以这里就不再详述，具体可以参考k8s官方的sample-controller 代码。 code-generator有自己的语法，当然，如果你只是简单的要实现一个operator可以直接复制上面的代码来修改，否则请参考这里。 另外，对于controller中的原理，这篇文章有讲的比较详细，可以参考。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"operator","slug":"operator","permalink":"http://ljchen.net/tags/operator/"},{"name":"controller","slug":"controller","permalink":"http://ljchen.net/tags/controller/"}]},{"title":"K8s认证详解","slug":"K8s认证详解","date":"2019-06-13T12:28:04.000Z","updated":"2020-04-06T09:24:01.467Z","comments":true,"path":"2019/06/13/K8s认证详解/","link":"","permalink":"http://ljchen.net/2019/06/13/K8s认证详解/","excerpt":"以前在基于k8s做应用开发的时候，都是使用admin来使用k8s，基本不用去关注授权的问题。但是，当我们将k8s作为PaaS平台的容器编排引擎，并引入多租户时，就涉及到权限管理相关的问题了。那么，如果你刚好是公司的系统管理员，当你部署完一套k8s，准备将其提供给多个部门使用的时候（他们希望彼此互不影响），接下来的内容就特别适合你了。","text":"以前在基于k8s做应用开发的时候，都是使用admin来使用k8s，基本不用去关注授权的问题。但是，当我们将k8s作为PaaS平台的容器编排引擎，并引入多租户时，就涉及到权限管理相关的问题了。那么，如果你刚好是公司的系统管理员，当你部署完一套k8s，准备将其提供给多个部门使用的时候（他们希望彼此互不影响），接下来的内容就特别适合你了。 概要介绍K8s的安全问题越来越成为大家都在关注的重点，挑战一方面来自于运行时容器层面宿主机的安全，另一方面来自于k8s本身的安全。 认证方式k8s的授权默认是基于RBAC的方式，这个比较好理解，但是认证却支持好几种方式: 客户端证书 Bearer Tokens Service Account Token BootStrap Token Static Token HTTP Basic Auth Authenticating Proxy 接下来我们重点分析前三种类型，至于Authenticating Proxy之类，这里暂不做分析。 权限控制原理要解决文章开始时提到的问题，按照常规业务系统的设计，不外乎以下几步： 将系统中模块的子功能的操作权限赋予角色； 将用户与角色绑定，让用户拥有角色对应的操作权限； 认证用户的登录； k8s系统中基于RBAC的授权刚好就是前面两步做的事情： 首先创建role，在role中指定该role所拥有的权限，能够允许执行的所有操作。 然后创建rolebinding，这一步binding的不止是用户；在k8s中，允许binding到role的可以是user,group,service account。其中user就是用户，group是用户组，service account可以理解为系统为内部服务分配的一个账户。 如果希望role和rolebinding不止是局限于对应的namespace，可以使用clusterrole和clusterrolebinding，这将使role的操作权限扩大到集群层面，而不是在单独某一个namespace内。 那么，第三步又是如何实现的呢？下面就介绍几种方法。 客户端证书认证客户端证书认证就是客户端向k8s提交带有用户名、用户组等信息的CSR，然后管理员在k8s上为该客户签发证书。以后，用户使用该证书去访问k8s，api-server就能够辨识出用户。结合RBAC的配置，如果该用户或者用户组通过rolebinding绑定了role，那么该用户就拥有该role对对应资源的操作权限。 下面便是操作流程，通过示例，可以再理解一遍上面的流程。 在本地主机上生成客户端的key和CSR文件 12345678910111213141516171819202122# 生成key文件$ openssl genrsa -out ljchen.key 4096# 创建csr配置文件$ cat csr.cnf[ req ]default_bits = 2048prompt = nodefault_md = sha256distinguished_name = dn[ dn ]CN = ljchenO = dev[ v3_ext ]authorityKeyIdentifier=keyid,issuer:alwaysbasicConstraints=CA:FALSEkeyUsage=keyEncipherment,dataEnciphermentextendedKeyUsage=serverAuth,clientAuth# 生成csr文件$ openssl req -config ./csr.cnf -new -key ljchen.key -nodes -out ljchen.csr 向k8s提交CSR，管理员签发后生成客户端证书 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 创建CSR的k8s yaml$ cat csr.yamlapiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata:name: mycsrspec:groups:- system:authenticatedrequest: $&#123;BASE64_CSR&#125;usages:- digital signature- key encipherment- server auth- client auth# 下发CSR到k8s$ export BASE64_CSR=$(cat ./ljchen.csr | base64 | tr -d '\\n')$ cat csr.yaml | envsubst | kubectl apply -f -# 在k8s上为CSR生成证书$ kubectl get csr$ kubectl certificate approve mycsr$ kubectl get csrNAME AGE REQUESTOR CONDITIONmycsr 9s 28b93...d73801ee46 Approved,Issued# 导出证书$ kubectl get csr mycsr -o jsonpath=’&#123;.status.certificate&#125;’ \\| base64 --decode &gt; ljchen.crt# 查看证书信息$ openssl x509 -in ./ljchen.crt -noout -textCertificate: Data: Version: 3 (0x2) Serial Number: 01:27:cb:76:d1:72:cd:73:a5:be:06:b1:ae:f9:6c:99:11:5c:36:c8 Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Jun 13 02:04:00 2019 GMT Not After : Jun 12 02:04:00 2020 GMT Subject: O=dev, CN=ljchen Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) ... 配置RBAC, 在k8s端使用证书来控制用户认证 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657$ kubectl create ns development# 创建role$ cat role.yamlkind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:namespace: developmentname: devrules:- apiGroups: [\"\"]resources: [\"pods\", \"services\"]verbs: [\"create\", \"get\", \"update\", \"list\", \"delete\"]- apiGroups: [\"apps\"]resources: [\"deployments\"]verbs: [\"create\", \"get\", \"update\", \"list\", \"delete\"]$ kubectl apply -f role.yaml# 创建role-binding, 分两种情况，可以绑定到user或者group，两者二选一# 绑定到user上$ cat role-binding-user.yamlkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:name: devnamespace: developmentsubjects:- kind: Username: ljchenapiGroup: rbac.authorization.k8s.ioroleRef:kind: Rolename: devapiGroup: rbac.authorization.k8s.io# 绑定到group上$ cat role-binding-group.yamlkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:name: devnamespace: developmentsubjects:- kind: Groupname: devapiGroup: rbac.authorization.k8s.ioroleRef:kind: Rolename: devapiGroup: rbac.authorization.k8s.io$ kubectl apply -f role-binding-group.yaml 在客户端，配置kubeconfig文件 123# 1. 先基于生成的ljchen.crt和已存在的ca来配置kubeconfig文件# 2. 将ljchen.key应用到kubeconfig中$ kubectl config set-credentials ljchen --client-key=~/.kube/ljchen.key --embed-certs=true Service Account Token认证service account本来是系统服务使用的账户，当我们创建service account时，它会自带一个secret，这个secret就是token。 前面谈到，rolebinding除了支持绑定user，group之外，还支持绑定到service account。所以，为了方便，我们有时候通过绑定role到某个service account的方式来直接使用sa的sercret来访问系统资源。比如部署dashboard的时候，就经常用这种方法来获取token登录UI。 下面的示例就是创建service account并取得token来登录系统的流程。 服务端获取token 123456789101112131415161718192021222324252627282930#Create a new ServiceAccountkubectl create serviceaccount k8sadmin -n kube-system#Create a ClusterRoleBinding with Cluster Admin Privileges（系统管理员角色！！）kubectl create clusterrolebinding k8sadmin --clusterrole=cluster-admin --serviceaccount=kube-system:k8sadmin#serviceAccount 会自动生成secret，对应的token就是secret的值kubectl get secret -n kube-system | grep k8sadmin | cut -d \" \" -f1 | xargs -n 1 | xargs kubectl get secret -n kube-system -o yamlapiVersion: v1data:ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1EWXhNakV6TWpZd09Gb1hEVEk1TURZd09URXpNall3T0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTktnCnRiSnQxSTZ6QlFVTmE2WHROK1pQN2xLeDdxbVk2aGJiaVByTXVGbmRhanBvd0RYdDhHVlJmNGFhbWVLS2Z0enEKWlJvSko5eHg5T3lnRkVINzRLY2tGRFhCbElUck9oY0t6TjJIWEI4SUlKM3BvN0Qzc0VScHZEYVZ6L0Z5TmRWagpBU0VhQzhlRWRSa2g2akhVeHJUTWlUMlJZemMrZGNaWC83MHcrTW90bjE5Skt4d0Y5N3h3U2EvYmd6NUdXK1V3CllBd0E0TWRwWG1YM1R1TUx5Rm9kSTVZOUt1VHBpVWd1OFp5Sm5ySzBpQ3g5MlpQMGRSVUlITXVRakxKWkhyY2QKOERsSmNnL0ZsOXlwaDh3UTZScDZSbzFlZ05tY2xGVFJBQm12OVhvMmNOdHJ4Mm1BTmp4aDBycjFkU21NWEJqUgpqS1Y0SFdqVWRzcERKbVpXRnJzQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFNZkxia016RzZ6aGpUSGxtaU93SFJqL05BSWEKM0ZOY2lvekhJcUU2VkE5UUlOZ0V4a1pzQnBRREdBWVZvYk1Ec3RSQXQxY2M4cW5qdVltSnoreFlDZTFYY0xLMgpWRlpWdkdOZzJCRmJrTFdtYlRPeXhNNmQ0S2VvZnh6VDQ4cmU3U1pmQWRLRnRrVXN5T3AvUndQWGlOMVFpcy8vCkV6NDA1RlczTFZRWGtpMjh2cG5KUG5WNUhoZy9FY0ZMOXoveXdSUEtldmFRZUE5NGFFVXNuOEJXTGtZcWs3NWgKalM4dlhlNi9KOGVvWDdybVdhOG9QZUdQYmpqZ2E0Q0UyK2N1L0RqMFNFWEduK2xzM2w1V3dGZVNVNXdwd21XUwptdUdPdDZXVTdaa3hJbkNSTERudGovZzlJOFVGai9JZFBvd3B2bU1oUW5DOXFtV2pCZ21RSkhsN29iUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=namespace: a3ViZS1zeXN0ZW0=token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjM0JoWTJVaU9pSnJkV0psTFhONWMzUmxiU0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVmpjbVYwTG01aGJXVWlPaUpyT0hOaFpHMXBiaTEwYjJ0bGJpMXFiamQ0ZGlJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExtNWhiV1VpT2lKck9ITmhaRzFwYmlJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZ5ZG1salpTMWhZMk52ZFc1MExuVnBaQ0k2SW1RMll6TmpPR1V5TFRoa01UVXRNVEZsT1MwNE0yVmtMVEF3TVRZelpURTBPV05tTWlJc0luTjFZaUk2SW5ONWMzUmxiVHB6WlhKMmFXTmxZV05qYjNWdWREcHJkV0psTFhONWMzUmxiVHByT0hOaFpHMXBiaUo5LnBlRGtqREp5UlliRDVfdFJQZDBKaDB5VEpSTDhNM3M1ZlJka1dYUWNDd2RGM0J3TE9mRVBPaWI0bEF3US1OcUptQS1sUlZ2dnQ4ZTVqU3h4UVVJSlRJYWp6S0RxaWYzQUpPVElYVEpmYy1vRGg1VUw2T01WdVJhdlhLNkZkbk1rd281RlhOSzVOYnl4WGx5RGdoaUJqWGc0ZUVXUWRLcXdKc05GYjBtcm15N2czb2NrZUsyQjZONzNDMWNwUGtIajRqTzY3bHl0OVFrQmtRRU1odGlNaURoZlJlTWFzLXF1ZFIzbGh3Z2xZUlNBbkdzelFOSnFKbFZxTzNUSkdEVjNmc0tUNVBlZFVHU1ZjM0JlQm1ZLXhyLUtyZFR5dE1rSk1rRUtoOHNOWEZ2UzN1MFowYjRRaVhwd296cjFkVlBkUnU4NDB2X2hkN1JPUkpOQkZ0UlV1dw==kind: Secretmetadata:annotations: kubernetes.io/service-account.name: k8sadmin kubernetes.io/service-account.uid: d6c3c8e2-8d15-11e9-83ed-00163e149cf2creationTimestamp: \"2019-06-12T13:27:34Z\"name: k8sadmin-token-jn7xvnamespace: kube-systemresourceVersion: \"487\"selfLink: /api/v1/namespaces/kube-system/secrets/k8sadmin-token-jn7xvuid: d6c522cd-8d15-11e9-83ed-00163e149cf2type: kubernetes.io/service-account-token# 获取token值kubectl get secret -n kube-system | grep k8sadmin | cut -d \" \" -f1 | xargs -n 1 | xargs kubectl get secret -o 'jsonpath=&#123;.data.token&#125;' -n kube-system | base64 --decode 客户端使用token 1234kubectl -s https://10.x.x.x:6443 --token eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrOHNhZG1pbi10b2tlbi1iNG1xcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrOHNhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjVjN2RlYTVkLTUzYWMtMTFlOS1iMmNmLTUyNTQwMGZmNzI5YSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprOHNhZG1pbiJ9.bpyv4SuQ1P-LyGIs5udfx74qonM21ickQA60og47HT5MHQDCypPyrsjBwq73mEzVvDmw6A42WKtBq_Tv4V7cT4fB-pnmxtkuESkmDdIr0FH7LCSyd6MIvIkqxkVvawy74AiWB7YbgbA1bJq5_btNPl8kPfFNz9SSPpKSnv3wc6Ln8kfvZRSWM-K_6sE4QQOhlcoshwqzCPSH9hU3HjkcYTg-QAry-IThjSLAoFiq4sgSsl95MG51sxPYhvM5hl-FLVGAehnJtlPlwc29BU5zfeCv64_hhLHIwYT74pLgVNe6O_McuhnreG5W2YcqxngFOUHuHrdogLlwRe_KELRS-Q get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 73d v1.14.0 BootStrap Token认证bootstrap token是kubeadm部署系统的时候，为了方便node加入到集群，从而提供了一个后门。该token对应的user拥有自动签发客户端CSR的权限，这样添加一台新节点的时候，就不用管理员到系统上面手动签发CSR请求了。 bootstrap-token都是以“bootstrap-token-”开头的名称，其中auth-extra-groups指定了其所属的group信息。clusterrolebinding也就是通过绑定clusterrole到该group，从而允许持有该token的kubeadm节点拥有签发证书的权限的。 token中的auth-extra-groups信息 1234567891011121314151617181920212223242526272829303132333435# 查看secret内容$ kcs get secret bootstrap-token-gb4kis -o yamlapiVersion: v1data:auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=description: VGhlIGRlZmF1bHQgYm9vdHN0cmFwIHRva2VuIGdlbmVyYXRlZCBieSAna3ViZWFkbSBpbml0Jy4=expiration: MjAxOS0wNi0xM1QyMToyNjozMyswODowMA==token-id: Z2I0a2lztoken-secret: cW40M3d6ZGJzd2c4ZjdlOA==usage-bootstrap-authentication: dHJ1ZQ==usage-bootstrap-signing: dHJ1ZQ==kind: Secretmetadata:creationTimestamp: \"2019-06-12T13:26:33Z\"name: bootstrap-token-gb4kisnamespace: kube-systemresourceVersion: \"168\"selfLink: /api/v1/namespaces/kube-system/secrets/bootstrap-token-gb4kisuid: b228ddbb-8d15-11e9-83ed-00163e149cf2type: bootstrap.kubernetes.io/token# 解出auth-extra-groups对应的base64编码信息为$ echo \"c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=\" | base64 -dsystem:bootstrappers:kubeadm:default-node-token# 其中system:bootstrappers为group信息#接下来重点关注该group对应的clusterrolebinding$ kcs get clusterrolebinding | grep kubeadmkubeadm:kubelet-bootstrap 14hkubeadm:node-autoapprove-bootstrap 14hkubeadm:node-autoapprove-certificate-rotation 14hkubeadm:node-proxier 14h 绑定的clusterrole clusterrolebinding clusterrole kubeadm:kubelet-bootstrap system:node-bootstrapper kubeadm:node-autoapprove-bootstrap system:certificates.k8s.io:certificatesigningrequests:nodeclient kubeadm:node-autoapprove-certificate-rotation system:certificates.k8s.io:certificatesigningrequests:selfnodeclient kubeadm:node-proxier system:node-proxier 静态Token认证api server参数： –token-auth-file stringIf set, the file that will be used to secure the secure port of the API server via token authentication. api server通过--token-auth-file=SOMEFILE读取文件中的Token。当前，token是无期限持续的，除非重启api server。token文件是一个至少包含3列的csv文件：token, user name, user uid，后跟可选的组名。 如果您有多个组，则列必须是双引号，例如： 1token,user,uid,\"group1,group2,group3\" 用token唯一标识客户端，只要api server存在该token，则认为认证通过，但是如果需要新增Token，则需要重启kube-apiserver组件。当通过客户端使用 bearer token 认证时，API服务器需要一个值为带有Bearer THETOKEN值的Authorization头。bearer token必须是一个字符序列，能够放在HTTP请求头中。 HTTP Basic Authapi server参数： –basic-auth-file stringIf set, the file that will be used to admit requests to the secure port of the API server via http basic authentication. 基础认证模式通过在api server中设置-–basic-auth-file=SOMEFILE来启用。一旦api server服务启动，加载的用户名和密码信息就不会发生改变，任何对源文件的修改必须重启api server才能生效。 静态密码文件是CSV格式的文件，每行对应一个用户的信息: 1password,user,uid,\"group1,group2,group3\" 当Http客户端使用基础认证时，api server需要一个带有Basic BASE64ENCODED(USER:PASSWORD) 值的Authorization头。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"authentication","slug":"authentication","permalink":"http://ljchen.net/tags/authentication/"},{"name":"security","slug":"security","permalink":"http://ljchen.net/tags/security/"}]},{"title":"Etcd原理与运维","slug":"Etcd原理与运维","date":"2019-06-11T13:12:21.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2019/06/11/Etcd原理与运维/","link":"","permalink":"http://ljchen.net/2019/06/11/Etcd原理与运维/","excerpt":"基于K8s之上的声明式API编程可以说Etcd起到了至关重要的作用。一方面，API Server使用了etcd的特性从而提供了订阅，选举等功能。另一方面，k8s集群的所有配置信息也都集中存放于etcd中，etcd完好的情况下，可以随意的变动node节点，k8s最终会保障服务都按照预期来编排和对外提供服务。","text":"基于K8s之上的声明式API编程可以说Etcd起到了至关重要的作用。一方面，API Server使用了etcd的特性从而提供了订阅，选举等功能。另一方面，k8s集群的所有配置信息也都集中存放于etcd中，etcd完好的情况下，可以随意的变动node节点，k8s最终会保障服务都按照预期来编排和对外提供服务。 从etcd自生的实现来讲，其使用了Raft协议来保障数据的一致性，底层使用bbolt k-v存储，同时使用WAL和snapshot等都是分布式系统研究的好素材。 Raft协议说到分布式系统的一致性，都会想到raft协议。那raft协议之所以这么声明大昭，其到底有哪些精妙之处呢？我觉得其实很多协议（包括通讯协议）都是对生活中一些道理的抽象，我后续会举一个例子来讲述raft的选举。 Raft协议主要包含两块： 选举过程主要涉及采用何种流程来确立集群中的江湖地位； 日志复制主要涉及在确立好各个角色江湖地位的情况下，如何保障日志存储的一致性的问题。 选举过程无论是哪一种排别，都要先讲讲江湖地位。这里一种有三种角色，分别为： leader对内，定期发送心跳报文，通告天下，维护自己的江湖地位；对外，负责处理所有客户端的请求，当接收到客户端的写入请求时，会在本地追加一条相应的日志，然后将其封装成消息发送到集群中其他的Follower节点。 candidate由Follower节点转换而来的，当Follower节点长时间没有收到Leader节点发送的心跳消息时，则该节点的选举计时器就会过期，同时会将自身状态转换成Candidate，发起新一轮选举。 follower简单地响应来自Leader或者Candidate的请求; 也不处理Client的请求，而是将请求重定向给集群的Leader节点。 学院派总结起来就是：3/2/1, 即:”三个角色 / 两个定时器 / 一个任期”。 集群初始化时，所有节点都处于Follower状态, 当Follower一段时间(选举计时器超时)收不到Leader的心跳消息，就认为Leader出现故障导致其任期(Term)过期, Follower会转成Candidate； Candidate等选举计时器超时后，会先投自己一票，并向集群中其他节点发起选举请求，并带上自己的Term ID以及当前日志的Max Index; 如果其他节点没有投出Term ID内的一票，就会比较Candidate的Max Index是否比自己小，如果比自己大或者相等，就会投票给Candidate；同时，会将自己的选举计时器重置； Candidate在获取到超过半数节点的选票后，升级为Leader；并按照心跳计时器向集群中其他节点发送心跳报文，并同步log entity等; follower收到心跳报文后，会重置选举计时器。 注意: 在第三步中，除了要确定term任期内未投过票之外，还要确定candidate的log index的原因是保障不会让日志记录缺失的成员成为leader，相当于冒泡排序法，找出log最完整的成员做leader。 另外，为了保障不频繁出现重新选举，对两个定时器的设置需要满足： 广播时间 &lt; 选举超时时间 &lt; 平均故障间隔时间 武林派 话说当年日月圣教由任我行教主执掌，但是出于对武学的痴迷，仍教主天天闭关修炼，不理教内事务。于是教内腐败严重，没有教主的打压，各种小势力乘机崛起，其中，最牛叉的当数东方不败； 东方不败乘任教主闭关期间，怂恿半数的收下归顺自己，同时凭借自己的武功力压群雄，让教内所有人都不得不承认任我行的时期已过去，现在是东方不败的时代了； 作为新的继任者，东方不败一方面整治教内腐败，打压各种小势力；另一方面积极处理对外事务，很快教内又恢复了生机； 有一天任我行闭关归来，发现所有教众都已经诚服与东方不败；同时，功力也不如东方不败，于是也只好认栽！ 这个故事歪曲了《笑傲江湖》的原剧本，谁叫任我行姓任呢，书中他必须要重出江湖呀！但是Raft协议在这里却告诉了我们什么是现实，现实就是你必须低头，你的Term过了，长江后浪推前浪。 日志复制Leader除了向Follower发送心跳消息，还会处理客户端的请求，并将客户端的更新操作以消息(Append Entries消息)的形式发送到集群中所有的Follower。当Follower记录收到的这些消息之后，会向Leader返回相应的响应消息。 Leader在收到半数以上的 Follower的响应消息后，会对客户端的请求进行应答。 Leader会维护nextIndex[]和matchIndex[]，这两个数组中记录的都是日志索引值 nextIndex[]记录了需要发送给每个Follower的下一条日志的索引值; matchIndex[]表示记录了己经复制给每个Follower的最大的日志索引值。 调整过程 当一个新leader被选举出来时，它不知道每一个follower当前log的状态，因此会将每个的nextIndex值都设置为其日志的最大index（然后再逐步调整），同时将其matchIndex设置为0; Leader会向其他节点发送AppendEntries，若节点己经拥有了Leader的全部log（要求index和term都对应），它会返回追加成功的响应并等待后续的日志 ；若节点没有index对应的log，就会返回追加日志失败的响应；收到响应后，Leader节点会将nextIndex前移。 我的理解是，在AppendEntries中只是带了index和term信息，而不带内容；在第二步确定了nextIndex的位置之后，leader才真正将缺失的log内容追加给follower。 Etcd主要介绍一些调试和性能方面可能有影响的知识点，以及运维常用的命令。 知识点可能影响到etcd性能的点。 存储etcd在 BoltDB 中存储的 Key 是 reversion, Value 是 etcd 自定义的键值对组合。 1234rev=&#123;1 0) , key=key1, value=\"value1\"rev=&#123;1 1), key=key2, value=\"value2\" rev=&#123;2 0) , key=key1, value=\"updatel\" rev=&#123;2 1) , key=key2 , value= \"update2\" reversion主要由两部分组成，第一部分是 main reversion，每次事务递增一；第二部分是sub reversion，同一个事务中的每次操作都会递增1，两者结合就可以保证Key唯一且递增。 从backend store保存的数据格式我们可以看出，如果要从BoltDB中查询键值对，必须通过reversion进行查找。但客户端只知道具体的键值对中的Key值，并不清楚每个键值对对应的reversion信息，所以在v3版本存储的内存索引(kvIndex)中保存的就是Key与reversion之前的映射关系。 客户端在查找指定键值对时，会先通过内存中维护的 B树索引(该B树索引中维护了原始Key值到keyIndex的映射关系)查找到对应的keyIndex实例，然后通过keyIndex查找到对应的revision信息(keyIndex内维护了多个版本的revision信息〉，最后通过revision映射到磁盘中的 BoltDB查找并返回真正的键值对数据。 快照随着节点的运行，会处理客户端和集群中其他节点发来的大量请求，相应的WAL日志量会不断增加，会产生大量的WAL日志文件，这就会导致资源浪费 。当节点宕机之后，如果要恢复其状态，则需要从头读取全部的WAL日志文件，这显然是非常耗时的。为了解决这些问题，etcd会定期创建快照并将其保存到本地磁盘中，在恢复节点状态时会先加载快照文件，使用该快照数据将节点恢复到对应的状态，之后从快照数据之后的相应位置开始读取WAL日志文件，最终将节点恢复到正确的状态。 etcd的members目录下可以看到snap和wal两个目录，wal下主要存放Write ahead log的数据；snap下存放的就是从wal做的snap。另外，在snap下还有一个db文件，它就是bbolt的数据库文件。 --snapshot-count参数控制快照的频率，默认是10000，即每10000次变更会触发一次快照操作。如果内存使用率高并且磁盘使用率高，可以尝试调低这个参数。 debug启动参数中添加--debug即可打开debug模式，etcd会在http://x.x.x.x:2379/debug路径下输出debug信息。由于debug信息很多，会导致性能下降。 /debug/pprof为go语言runtime的endpoint，可以用于分析CPU、heap、mutex和goroutine利用率。 调优 将etcd所使用的磁盘与系统盘分开 data目录和wal目录分别挂载不同的磁盘 有条件推荐使用SSD固态硬盘 使用ionice调高etcd进程的IO优先级（这个针对etcd数据目录在系统盘的情况） 1ionice -c2 -n0 -p `pgrep etcd` 运维包含一些运维常用的命令和方法，以备随时拷贝。 添加新节点增加一个新的节点分为两步： 通过 etcdctl或对应的API注册新节点 使用恰当的参数启动新节点 假设我们要新加的节点取名为infra3, peerURLs是http://10.0.1.13:2380 123456$ etcdctl member add infra3 http://10.0.1.13:2380added member 9bf1b35fc7761a23 to clusterETCD_NAME=\"infra3\"ETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"ETCD_INITIAL_CLUSTER_STATE=existing etcdctl 在注册完新节点后，会返回一段提示，包含3个环境变量。然后在第二部启动新节点的时候，带上这3个环境变量即可。 1234$ export ETCD_NAME=\"infra3\"$ export ETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"$ export ETCD_INITIAL_CLUSTER_STATE=existing$ etcd -listen-client-urls http://10.0.1.13:2379 -advertise-client-urls http://10.0.1.13:2379 -listen-peer-urls http://10.0.1.13:2380 -initial-advertise-peer-urls http://10.0.1.13:2380 -data-dir %data_dir% 这样，新节点就会运行起来并且加入到已有的集群中了。 值得注意的是，如果原先的集群只有1个节点，在新节点成功启动之前，新集群并不能正确的形成。因为原先的单节点集群无法完成leader的选举。直到新节点启动完，和原先的节点建立连接以后，新集群才能正确形成。 故障恢复备份数据 12etcdctl backup --data-dir /var/lib/etcd -backup-dir /tmp/etcd_backuptar -zcxf backup.etcd.tar.gz /tmp/etcd_backu 使用--force-new-cluster参数启动Etcd服务。这个参数会重置集群ID和集群的所有成员信息，其中节点的监听地址会被重置为localhost:2379, 表示集群中只有一个节点。 12tar -zxvf backup.etcd.tar.gz -C /var/lib/etcdetcd --data-dir=/var/lib/etcd --force-new-cluster ... 快照恢复节点数据快照 12345678910ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb# exit 0# verify the snapshotETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| fe01cf57 | 10 | 7 | 2.1 MB |+----------+----------+------------+------------+ 需要在每一个节点上都做restore 123456789101112131415$ etcdctl snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380$ etcdctl snapshot restore snapshot.db \\ --name m2 \\ --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host2:2380$ etcdctl snapshot restore snapshot.db \\ --name m3 \\ --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host3:2380 历史记录压缩如果将etcd用作服务发现，每次服务注册和更新都可以看做一条新数据，日积月累，这些数据的量会导致etcd占用内存越来越大，直到etcd到达空间配额限制的时候，etcd的写入将会被静止，影响线上服务，定期删除历史记录就是避免这种情况。 数据压缩并不是清理现有数据，只是对数据的历史版本进行清理，清理后数据的历史版本将不能访问，但不会影响现有最新数据的访问。 –auto-compaction-retentionAuto compaction retention for mvcc key value store in hour. 0 means disable auto compaction.default: 0 –auto-compaction-modeInterpret ‘auto-compaction-retention’ one of: ‘periodic’, ‘revision’. ‘periodic’ for duration based retention, defaulting to hours if no time unit is provided (e.g. ‘5m’). ‘revision’ for revision number based retention.default: periodic v3.3之上的版本有这样一个规则 Periodic compactor keeps recording latest revisions for every compaction period when given period is less than 1-hour, or for every 1-hour when given compaction period is greater than 1-hour (e.g. 1-hour when –auto-compaction-mode=periodic –auto-compaction-retention=24h) 也就是说，如果配置的值小于1小时，那么就严格按照这个时间来执行压缩；如果配置的值大于1小时，会每小时执行压缩，但是采样还是按照保留的版本窗口依然按照用户指定的时间周期来定。 1$ etcd --auto-compaction-retention=1 # 只保留一个小时的历史数据 k8s api-server支持定期执行压缩操作，其参数里面有这样的配置： –etcd-compaction-interval duration Default: 5m0s The interval of compaction requests. If 0, the compaction request from apiserver is disabled. 碎片整理进行compaction操作之后，旧的revision被压缩，会产生内部的碎片，内部碎片是指空闲状态的，能被etcd使用但是仍然消耗存储空间的磁盘空间。去碎片化实际上是将存储空间还给文件系统。 1234567etcdctl defrag# 带参数，整理集群所有节点$ etcdctl defrag --clusterFinished defragmenting etcd member[http://127.0.0.1:2379]Finished defragmenting etcd member[http://127.0.0.1:22379]Finished defragmenting etcd member[http://127.0.0.1:32379] 如果etcd没有运行，可以直接整理目录中db的碎片 1$ etcdctl defrag --data-dir &lt;path-to-etcd-data-dir&gt; Note：碎片整理和压缩都会阻塞对etcd的读写操作。 存取空间限制 Request size limit etcd is designed to handle small key value pairs typical for metadata. Larger requests will work, but may increase the latency of other requests. By default, the maximum size of any request is 1.5 MiB. This limit is configurable through --max-request-bytes flag for etcd server. --max-request-bytes限制请求的大小，默认值是1572864，即1.5M。在某些场景可能会出现请求过大导致无法写入的情况，可以调大到10485760即10M。 Storage size limit The default storage size limit is 2GB, configurable with --quota-backend-bytes flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it. 12345678910111213141516171819# 设置非常小的 16MB 配额$ etcd --quota-backend-bytes=$((16*1024*1024))# 消耗空间$ while [ 1 ]; do dd if=/dev/urandom bs=1024 count=1024 | etcdctl put key || break; done...Error: rpc error: code = 8 desc = etcdserver: mvcc: database space exceeded# 确认配额空间被超过$ etcdctl --write-out=table endpoint status+----------------+------------------+-----------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+----------------+------------------+-----------+---------+-----------+-----------+------------+| 127.0.0.1:2379 | bf9071f4639c75cc | 2.3.0+git | 18 MB | true | 2 | 3332 |+----------------+------------------+-----------+---------+-----------+-----------+------------+# 确认警告已发起$ etcdctl alarm listmemberID:13803658152347727308 alarm:NOSPACE 如果遇到空间不足，可以这样操作： 12345678910# 获取当前版本号$ rev=$(ETCDCTL_API=3 etcdctl --endpoints=:2379 endpoint status --write-out=\"json\" | egrep -o '\"revision\":[0-9]*' | egrep -o '[0-9]*'）# 压缩所有旧版本$ ETCDCTL_API=3 etcdctl compact $rev# 去碎片化$ ETCDCTL_API=3 etcdctl defrag# 取消警报$ ETCDCTL_API=3 etcdctl alarm disarm# 测试通过$ ETCDCTL_API=3 etcdctl put key0 1234","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"Etcd","slug":"Etcd","permalink":"http://ljchen.net/tags/Etcd/"},{"name":"Raft","slug":"Raft","permalink":"http://ljchen.net/tags/Raft/"}]},{"title":"Knative墙内安装","slug":"knative墙内安装","date":"2019-06-03T15:08:42.000Z","updated":"2020-04-06T09:24:01.483Z","comments":true,"path":"2019/06/03/knative墙内安装/","link":"","permalink":"http://ljchen.net/2019/06/03/knative墙内安装/","excerpt":"knative是一个大熔炉，将DevOps构建到服务的自动弹性伸缩，流量管控，事件驱动等都整合到一起。从很早期的版本开始我就一直有在关注，好在其文档还比较完备，可以基于文档一步步部署和尝鲜。","text":"knative是一个大熔炉，将DevOps构建到服务的自动弹性伸缩，流量管控，事件驱动等都整合到一起。从很早期的版本开始我就一直有在关注，好在其文档还比较完备，可以基于文档一步步部署和尝鲜。 东西虽好，但安装却很麻烦！gcr.io一方面被墙，无法拉取镜像；另一方面gcr镜像名称还支持多级子目录，导致没法和docker hub简单做映射。在v0.4.0版本时，我曾一个个镜像在docker hub创建对应的repository。可是这货版本更新又很快，实在无法容忍；于是写了一个脚本，专门用于解决knative墙内安装的问题。 当前v0.7.0已经验证通过！ 基本原理其实原理比较简单，主要分两步： 在墙外执行第一个脚本先基于knative release的yaml文件，从中过滤出gcr中存放的images；将其下载到墙外主机，然后按照特定规则更改每一个镜像的tag为满足docker hub中镜像名称规则的new tag，再推送到docker hub； 在墙内执行第二个脚本按照第一个脚本中镜像old tag到new tag的转换规则，更新knative release的yaml文件中所有的镜像tag。然后使用更新后的yaml文件来部署knative。 安装istio先在k8s上安装istio，具体操作如下（如果你说你不会安装k8s，请移步这里）： 12345678910111213141516171819202122232425262728293031323334353637383940414243# Download and unpack Istioexport ISTIO_VERSION=1.1.7curl -L https://git.io/getLatestIstio | sh -cd istio-$&#123;ISTIO_VERSION&#125;for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; donecat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Namespacemetadata: name: istio-system labels: istio-injection: disabledEOFhelm template --namespace=istio-system \\ --set prometheus.enabled=false \\ --set mixer.enabled=false \\ --set mixer.policy.enabled=false \\ --set mixer.telemetry.enabled=false \\ `# Pilot doesn't need a sidecar.` \\ --set pilot.sidecar=false \\ --set pilot.resources.requests.memory=128Mi \\ `# Disable galley (and things requiring galley).` \\ --set galley.enabled=false \\ --set global.useMCP=false \\ `# Disable security / policy.` \\ --set security.enabled=false \\ --set global.disablePolicyChecks=true \\ `# Disable sidecar injection.` \\ --set sidecarInjectorWebhook.enabled=false \\ --set global.proxy.autoInject=disabled \\ --set global.omitSidecarInjectorConfigMap=true \\ `# Set gateway pods to 1 to sidestep eventual consistency / readiness problems.` \\ --set gateways.istio-ingressgateway.autoscaleMin=1 \\ --set gateways.istio-ingressgateway.autoscaleMax=1 \\ `# Set pilot trace sampling to 100%` \\ --set pilot.traceSampling=100 \\ install/kubernetes/helm/istio \\ &gt; ./istio-lean.yamlkubectl apply -f istio-lean.yaml 生成安装所需yaml这里也就是前面提到的第二个脚本文件，里面的REGISTRY_URL变量是写的我在docker hub的仓库名称。你也可以直接使用该脚本来部署，镜像都是public的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119cat &lt;&lt;EOF &gt; knative-image.shKNATIVE_PATH='knative'KNATIVE_VER='v0.7.0'REGISTRY_URL='ljchen'rm -rf $KNATIVE_PATHmkdir $KNATIVE_PATH# download yamlecho \"download yaml files ...\"cd $KNATIVE_PATHwget -q https://github.com/knative/serving/releases/download/$&#123;KNATIVE_VER&#125;/serving.yaml wget -q https://github.com/knative/build/releases/download/$&#123;KNATIVE_VER&#125;/build.yaml wget -q https://github.com/knative/eventing/releases/download/$&#123;KNATIVE_VER&#125;/release.yaml wget -q https://github.com/knative/eventing-sources/releases/download/$&#123;KNATIVE_VER&#125;/eventing-sources.yaml wget -q https://github.com/knative/serving/releases/download/$&#123;KNATIVE_VER&#125;/monitoring.yaml wget -q https://raw.githubusercontent.com/knative/serving/$&#123;KNATIVE_VER&#125;/third_party/config/build/clusterrole.yamlcd ..# get images listecho \"collect image to tmp file ...\"cd $KNATIVE_PATHrm -rf image.tmpfor line in `grep -RI \" image: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/knative-releases/github.com/knative' ]] then sub_line1=$&#123;line##gcr.io/knative-releases/github.com/knative/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=knative_$&#123;sub_line2//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#image:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=knative_$&#123;sub_line3//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp; fi fidonefor line in `grep -RI \" value: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/knative-releases/github.com/knative' ]] then sub_line1=$&#123;line##gcr.io/knative-releases/github.com/knative/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=knative_$&#123;sub_line2//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#value:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=knative_$&#123;sub_line3//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp; fi fidonecd ..# replace file imagecd $KNATIVE_PATHcounter=0for file in *.yamldo echo \"开始处理文件 \" $file while read line do origin_image=`echo $&#123;line&#125; | awk '&#123;print $1&#125;'` new_image=`echo $&#123;line&#125; | awk '&#123;print $2&#125;'` tmp=$&#123;origin_image//\\//__&#125; origin_image=$&#123;tmp//__/\\\\/&#125; tmp2=$&#123;new_image//\\//__&#125; new_image=$&#123;tmp2//__/\\\\/&#125; sed -i \"s/$&#123;origin_image&#125;/$&#123;new_image&#125;/g\" $&#123;file&#125; #上面这行，如果是MacOS/UNIX请替换为: sed -i \" \" \"s/$&#123;origin_image&#125;/$&#123;new_image&#125;/g\" $&#123;file&#125; done &lt; image.tmp counter=`expr $&#123;counter&#125; + 1`doneecho \"共处理文件数：\" $&#123;counter&#125;rm -rf *.yaml.1cd ..# finishecho \"completed...\"EOFsh ./knative-image.sh 部署knative这里就是用刚才生成的yaml文件来直接部署到knative到k8s了。 12cd knative/kubectl apply -f *.yaml 配套镜像生成脚本这里就是前面说的第一个脚本，如果你想自己再跑一边，请修改REGISTRY_URL的值为你的镜像仓库地址。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101KNATIVE_PATH='knative'KNATIVE_VER='v0.7.0'REGISTRY_URL='ljchen'rm -rf $KNATIVE_PATHmkdir $KNATIVE_PATH# download yamlecho \"download yaml files ...\"cd $KNATIVE_PATHwget -q https://github.com/knative/serving/releases/download/$&#123;KNATIVE_VER&#125;/serving.yaml wget -q https://github.com/knative/build/releases/download/$&#123;KNATIVE_VER&#125;/build.yaml wget -q https://github.com/knative/eventing/releases/download/$&#123;KNATIVE_VER&#125;/release.yaml wget -q https://github.com/knative/eventing-sources/releases/download/$&#123;KNATIVE_VER&#125;/eventing-sources.yaml wget -q https://github.com/knative/serving/releases/download/$&#123;KNATIVE_VER&#125;/monitoring.yaml wget -q https://raw.githubusercontent.com/knative/serving/$&#123;KNATIVE_VER&#125;/third_party/config/build/clusterrole.yamlcd ..# get images listecho \"collect image to tmp file ...\"cd $KNATIVE_PATHrm -rf image.tmpfor line in `grep -RI \" image: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/knative-releases/github.com/knative' ]] then sub_line1=$&#123;line##gcr.io/knative-releases/github.com/knative/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=knative_$&#123;sub_line2//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#image:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=knative_$&#123;sub_line3//\\//_&#125; echo $&#123;line#image:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp; fi fidonefor line in `grep -RI \" value: \" *.yaml | grep gcr.io`do if [[ $&#123;line&#125; =~ 'gcr.io' ]] then if [[ $&#123;line&#125; =~ 'gcr.io/knative-releases/github.com/knative' ]] then sub_line1=$&#123;line##gcr.io/knative-releases/github.com/knative/&#125; sub_line2=$&#123;sub_line1%%@sha*&#125; container_name=knative_$&#123;sub_line2//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp else sub_line1=$&#123;line#value:&#125; sub_line2=$&#123;sub_line1#*/&#125; sub_line3=$&#123;sub_line2%%:*&#125; container_name=knative_$&#123;sub_line3//\\//_&#125; echo $&#123;line#value:&#125; $&#123;REGISTRY_URL&#125;/$&#123;container_name&#125;:$&#123;KNATIVE_VER&#125; &gt;&gt; image.tmp; fi fidonecd ..# download image, tag, pushcd $KNATIVE_PATHwhile read line do origin_image=`echo $&#123;line&#125; | awk '&#123;print $1&#125;'` new_image=`echo $&#123;line&#125; | awk '&#123;print $2&#125;'` echo \"old:\" $&#123;origin_image&#125; echo \"new:\" $&#123;new_image&#125; docker pull $&#123;origin_image&#125; docker tag $&#123;origin_image&#125; $&#123;new_image&#125; docker push $&#123;new_image&#125;done &lt; image.tmpcd ..# doneecho \"completed...\" gcr镜像映射表下面排版不忍直视！尝试过表格等格式，发现更惨不忍睹。hexo确实有点儿二；但是，内容的真实性更要紧，就忍忍吧~~ v0.6.0版本 gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:101f537b53b895b28b84ac3c74ede7d250845e24c51c26516873d8ccb23168ce=&gt; ljchen/knative_build_cmd_creds-init:v0.6.0 gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:ce2c17308e9cb81992be153861c359a0c9e5f69c501a490633c8fe54ec992d53=&gt; ljchen/knative_build_cmd_git-init:v0.6.0 gcr.io/cloud-builders/gcs-fetcher=&gt; ljchen/knative_cloud-builders_gcs-fetcher:v0.6.0 gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:50e2be042298f24800b9840a9aef831a5fe4d89d9a8edea5e0559cdedf32369d=&gt; ljchen/knative_build_cmd_nop:v0.6.0 gcr.io/knative-releases/github.com/knative/build/cmd/controller@sha256:6a762848a46786cb481f5870787133e0d5e15615f8d54a5ba50d86b8315a58eb=&gt; ljchen/knative_build_cmd_controller:v0.6.0 gcr.io/knative-releases/github.com/knative/build/cmd/webhook@sha256:8f0bbc50b63f368c9959acab87838c6986691c28d424847459f3526bf97f8a3e=&gt; ljchen/knative_build_cmd_webhook:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/manager@sha256:99cf1f559f74ae97f271632697ed6e78a3fdd88a155632a57341b0dd6eab6581=&gt; ljchen/knative_eventing-sources_cmd_manager:v0.6.0 k8s.gcr.io/elasticsearch:v5.6.4=&gt; ljchen/knative_elasticsearch:v0.6.0 k8s.gcr.io/fluentd-elasticsearch:v2.0.4=&gt; ljchen/knative_fluentd-elasticsearch:v0.6.0 k8s.gcr.io/addon-resizer:1.7=&gt; ljchen/knative_addon-resizer:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/controller@sha256:85c010633944c06f4c16253108c2338dba271971b2b5f2d877b8247fa19ff5cb=&gt; ljchen/knative_eventing_cmd_controller:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/sources_controller@sha256:aaa48f71a8db1b1dcf86c57d2dd72be1a65ed76d77f23a5abef4b2ad5c01c863=&gt; ljchen/knative_eventing_cmd_sources_controller:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/webhook@sha256:34a7cac96f8c809a7ce8ea0a86445204bbc6ac897525b876f53babb325f50bdc=&gt; ljchen/knative_eventing_cmd_webhook:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/controller@sha256:496c19e81d9e7e40b3887c7c290304934f54f46c8a9186e800e314c014970c26=&gt; ljchen/knative_eventing_cmd_in_memory_controller:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/dispatcher@sha256:897f03ed16e0000944da9ee0fdc971c43c8a494ff771c4e64d0573caf357c013=&gt; ljchen/knative_eventing_cmd_in_memory_dispatcher:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:1e40c99ff5977daa2d69873fff604c6d09651af1f9ff15aadf8849b3ee77ab45=&gt; ljchen/knative_serving_cmd_queue:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/activator@sha256:f553b6cb7599f2f71190ddc93024952e22f2f55e97a3f38519d4d622fc751651=&gt; ljchen/knative_serving_cmd_activator:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:3a466eaf05cd505338163322331ee8634c601204250fa639360ae3524756acc3=&gt; ljchen/knative_serving_cmd_autoscaler:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/controller@sha256:8f402eab0ada038d3de2ad753a40f9f441715d08058d890537146bb0aba11c8e=&gt; ljchen/knative_serving_cmd_controller:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/networking/certmanager@sha256:dc77db09a23103f64a554de4e01cfda7371cbb13bc0954c991bdc4141169257f=&gt; ljchen/knative_serving_cmd_networking_certmanager:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/networking/istio@sha256:55fe9eeacfc20d97d3cd4f80bfc8a9b95cff7b5c50121bda87f754da8f05e57b=&gt; ljchen/knative_serving_cmd_networking_istio:v0.6.0 gcr.io/knative-releases/github.com/knative/serving/cmd/webhook@sha256:f0f98736bd4b55354f447f59183bf26b9be1ab01691b8b4aeee85caeb1166562=&gt; ljchen/knative_serving_cmd_webhook:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/github_receive_adapter@sha256:b5d6e12d16d16c6c42ae3d4325a1ef3a8a129dfc97740aa28000c0867edfc4ff=&gt; ljchen/knative_eventing-sources_cmd_github_receive_adapter:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/ingress@sha256:a0acbe69420a67bef520e86aceaa237bf540c15882701c96245a6c4e06413bf6=&gt; ljchen/knative_eventing_cmd_broker_ingress:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/filter@sha256:b4da7ce7b12aff2355066ed3237aadcf35df3b1c78db83cc538e6cffa564f208=&gt; ljchen/knative_eventing_cmd_broker_filter:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/cronjob_receive_adapter@sha256:6bbb724d5a4dbaaead890ea51d5f84eb9514974a2d06e26c8753db59010987fb=&gt; ljchen/knative_eventing_cmd_cronjob_receive_adapter:v0.6.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/apiserver_receive_adapter@sha256:7349f83eebe85a3eed7cdc4d442a935deab1ba0c42f34294f219f4ef17b59fec=&gt; ljchen/knative_eventing_cmd_apiserver_receive_adapter:v0.6.0 gcr.io/knative-samples/autoscale-go:0.1=&gt; ljchen/knative_knative-samples_autoscale-go:v0.6.0 v0.7.0版本 gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:1a984c032a2606f8491f4a19a85209dcc1ae2cfd494c3dafe8a74269379ad2c8=&gt; ljchen/knative_build_cmd_creds-init:v0.7.0 gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:06505d8c621e9337d0dd1bc13ed4545a33e857fbb6374740cc6337d2ba55796d=&gt; ljchen/knative_build_cmd_git-init:v0.7.0 gcr.io/cloud-builders/gcs-fetcher=&gt; ljchen/knative_cloud-builders_gcs-fetcher:v0.7.0 gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:8aca9c97ede9a550ac3536d00c5d7acaae5e3a4fe514f4329ec261d935eddabb=&gt; ljchen/knative_build_cmd_nop:v0.7.0 gcr.io/knative-releases/github.com/knative/build/cmd/controller@sha256:5adb5ba0647a7b1af1d90848bf72a75fa84efeb89e1d688465a2105c1cce1dc2=&gt; ljchen/knative_build_cmd_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/build/cmd/webhook@sha256:35b1b5f72642e9c1ee71809fec309a019111beebf805f9ddddf154a97ad23975=&gt; ljchen/knative_build_cmd_webhook:v0.7.0 k8s.gcr.io/elasticsearch:v5.6.4=&gt; ljchen/knative_elasticsearch:v0.7.0 k8s.gcr.io/fluentd-elasticsearch:v2.0.4=&gt; ljchen/knative_fluentd-elasticsearch:v0.7.0 k8s.gcr.io/addon-resizer:1.7=&gt; ljchen/knative_addon-resizer:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/controller@sha256:57f273774efb017bbf06729af802514db2f3ab070b51730dba9330903aa34163=&gt; ljchen/knative_eventing_cmd_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/sources_controller@sha256:31ec2b4a1d1d9b81cd1eed6632b8a1c540b510b584e58c14ebc1c000330fe32c=&gt; ljchen/knative_eventing_cmd_sources_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/webhook@sha256:3b5de8074f00469c393910fd0fbac70cec10838a858c94ad755af1b6bd6712fd=&gt; ljchen/knative_eventing_cmd_webhook:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/webhook@sha256:3b5de8074f00469c393910fd0fbac70cec10838a858c94ad755af1b6bd6712fd=&gt; ljchen/knative_eventing_cmd_webhook:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/channel_controller@sha256:292b2dddf074ce355f5793f3d4893ad0863152e0783f32c05e7ae50328b1e2e6=&gt; ljchen/knative_eventing_cmd_in_memory_channel_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/channel_dispatcher@sha256:0c95cafd668283cb045fa4941922ff7365f8e6caef623a9c5a68452be1404b5e=&gt; ljchen/knative_eventing_cmd_in_memory_channel_dispatcher:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/controller@sha256:dfd7b2852c9bc2e391b04193a50a4f635db0e7a4bbd79a20c61199e6880394fe=&gt; ljchen/knative_eventing_cmd_in_memory_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/dispatcher@sha256:345888cb32ce69a45165e02fc87ebfde903fd1d9dfe059892289d66b779e6bee=&gt; ljchen/knative_eventing_cmd_in_memory_dispatcher:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:e007c0a78c541600466f88954deee65c517246a23345bfba45a7f212d09b8f3b=&gt; ljchen/knative_serving_cmd_queue:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/activator@sha256:57fe5f1a8b1d12f29fe9e3a904b00c7219e5ce5825d94f33339db929e92257db=&gt; ljchen/knative_serving_cmd_activator:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:2c2370df2751741348e1cc456f31425cb2455c377ddb45d3f6c17e743fd63d78=&gt; ljchen/knative_serving_cmd_autoscaler:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/controller@sha256:016c95f2d94be89683d1ddb7ea959667fd2d899087a4145a31d26b5d6f0bb38f=&gt; ljchen/knative_serving_cmd_controller:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/networking/certmanager@sha256:c757629165393f778d5c0e8b611c9c4857b24f0c748d985d3a080d0161a85248=&gt; ljchen/knative_serving_cmd_networking_certmanager:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/networking/istio@sha256:bb4407e4714511cd9429e86536c283265629a2c11c80633d91c0f798c494a16f=&gt; ljchen/knative_serving_cmd_networking_istio:v0.7.0 gcr.io/knative-releases/github.com/knative/serving/cmd/webhook@sha256:d9918d40492e0b20b48576ff6182e2ab896e50dfd2313cb471419be98f821b9c=&gt; ljchen/knative_serving_cmd_webhook:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/ingress@sha256:33d41dc38208bf9752b38e5c90149da4d5af74a67f317b7da8cb3c458fbd0fff=&gt; ljchen/knative_eventing_cmd_broker_ingress:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/filter@sha256:5a4eb60a605e189516a36a01c7fd39001d5766b2e6bb80c69744e15515282360=&gt; ljchen/knative_eventing_cmd_broker_filter:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/cronjob_receive_adapter@sha256:634fbf0348f9f10d09c8110c505173aed91ce747bb2b87605e6e1bb10dce270b=&gt; ljchen/knative_eventing_cmd_cronjob_receive_adapter:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing/cmd/apiserver_receive_adapter@sha256:f18cdbc3c3077ece8505a4f4c49055e6c1c577e9fa42446f0f81193e48aa1d60=&gt; ljchen/knative_eventing_cmd_apiserver_receive_adapter:v0.7.0 gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/event_display@sha256:37ace92b63fc516ad4c8331b6b3b2d84e4ab2d8ba898e387c0b6f68f0e3081c4=&gt;ljchen/knative_eventing-sources_cmd_event_display:v0.7.0","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"knative","slug":"knative","permalink":"http://ljchen.net/tags/knative/"}]},{"title":"基于Consul服务发现的istio","slug":"基于consul服务发现的istio","date":"2019-06-02T03:21:21.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2019/06/02/基于consul服务发现的istio/","link":"","permalink":"http://ljchen.net/2019/06/02/基于consul服务发现的istio/","excerpt":"由于业务微服务先前的架构大量使用consul来做服务注册与发现，但是istio主流的方案中，业务还是走k8s基于DNS的服务发现。正好看到istio社区也号称能够基于consul；因此，基于consul做了一些POC，主要情况简单介绍一下。","text":"由于业务微服务先前的架构大量使用consul来做服务注册与发现，但是istio主流的方案中，业务还是走k8s基于DNS的服务发现。正好看到istio社区也号称能够基于consul；因此，基于consul做了一些POC，主要情况简单介绍一下。 现状与需求当前业务方新开发的服务运行到k8s上，另外有一些旧服务运行在VM中（VM网络与办公网络三层可达，存在接口被人随意调用的风险）。希望能够降低服务部署的复杂度，同时当存在异常或瓶颈的时候能够快速感知到异常，定位问题。 因此，其服务治理的目标也比较明确： 网络安全，不允许接口被随意调用； VM+k8s容器通过一套平台，集中治理； 流量管控，需要做服务的蓝绿部署和灰度发布； 可视化，监控。 大体分析一下： 从功能方面来看 基于istio的宣传，乍一看，以上功能都能够cover住。但是这里面仍存在风险点，比如VM+K8S融合部署、集中管控；基于consul的服务注册等。虽然社区号称istio具备这些能力，但是却未在这些点上深度发力（从发布包中可以看到主流方向还是基于纯k8s方向）。 从服务注册方面来看 假设使用consul作为服务发现工具，那么就相当于将vm和k8s pod都拉平来对待，似乎是一个方向。另一方面，现有服务已经在使用spring cloud + consul注册的机制，如果继续使用consul的话，现有代码不需要再做修改。 另外，社区号称istio对consul服务发现的支持还不错，且在其release包中，自带了对应的安装部署的docker-compose文件。 所以，我决定重点POC一把基于consul做服务发现的istio的情况。 系统架构 其架构如上图所示，各个微服务的功能如下： api-server与后端的Etcd负责整个istio的配置持久化; registrator负责监听主机上的docker daemon，一旦有container创建或者销毁就按照其label将其注册到consul，从而供pilot做发现; pilot负责各个envoy的流量管控配置生成与下发; zipkin收集各envoy上报的trace信息，并对外展示。 安装部署涉及istio部署和示例服务bookinfo的部署。 istio部署部署这块，相对来讲比较简单，其发布包里面自带了基于consul部署的docker-compose.yaml。里面的api-server版本都较旧，也说明了社区未在这方面有所更新，这里重点要说下遇到的对应问题以及其更改方法。 docker-compose.yaml文件内容如下: 12345678910111213141516171819202122232425262728293031323334353637version: '2'services: etcd: ...... istio-apiserver: # 更改为当前最新版本的api-server image: gcr.io/google_containers/kube-apiserver-amd64:v1.14.1 networks: istiomesh: ipv4_address: 172.28.0.13 aliases: - apiserver ports: - \"8080:8080\" privileged: true environment: - SERVICE_IGNORE=1 command: [\"kube-apiserver\", \"--etcd-servers\", \"http://etcd:2379\", \"--service-cluster-ip-range\", \"10.99.0.0/16\", \"--insecure-port\", \"8080\", \"-v\", \"2\", \"--insecure-bind-address\", \"0.0.0.0\"] consul: ...... registrator: # 更新版本为master image: gliderlabs/registrator:master networks: istiomesh: volumes: - /var/run/docker.sock:/tmp/docker.sock command: [\"-internal\", \"-retry-attempts=-1\", \"consul://consul:8500\"] pilot: ...... zipkin: ...... 如上文所注释，这里需要修改几个地方： 由于自带的api-server版本太老，部署的时候会出现一些问题，直接使用最新版本就行了； registrator的latest版本无法注册协议到consul的service meta中，需要更改为master版本； 另外，第一次运行由于api-server未能够先ready，pilot会挂掉，只需要重启一次就行了。 应用部署123456[root@vm-1 consul]# pwd/root/istio-1.1.6/samples/bookinfo/platform/consul[root@vm-1 consul]# lsbookinfo.sidecars.yaml destination-rule-all.yaml virtual-service-ratings-test-abort.yaml virtual-service-reviews-test-v2.yamlbookinfo.yaml README.md virtual-service-ratings-test-delay.yaml virtual-service-reviews-v2-v3.yamlcleanup.sh virtual-service-all-v1.yaml virtual-service-reviews-50-v3.yaml virtual-service-reviews-v3.yaml 相信看到这里，大家都明白为啥k8s是主流方向了。这里示例代码中每一个服务的运行实例都需要再手动部署一个sidecar，想想k8s上自动注入的便捷性，突然感觉回到了解放前。 运行起来之后，可以在consul看到下列服务；当然，consul的UI上面未能够展现出所有的tags信息。 通过简单的访问productpage之后，我们可以在zipkin上看到对应的调用链信息视图。 如果这过程中某些配置不正确，可以查看对应envoy的配置信息，基本就是进入到envoy对应的network namespace去执行curl localhost:15000/help 获取各种配置信息（这块在上一篇中有具体的介绍，这里不再详说）。 关于部署就讲这么多，主要是通过这样一个过程让我们对基于consul服务发现的istio有一个更加直观的认识。下面我们就来吐槽！ 待完善的方面 缺少mixer的支持 mixer一方面提供了policy（虽然这个很影响性能，有点鸡肋），另一方面提供了telemetry的能力。虽然policy我们可以先忽略它，但是telemetry影响到后面的流量拓扑、性能的可视化。在这个用户体验为王的时代，没有这块功能相当于直接把自己扼杀了。 按理讲让envoy上报attribute到mixer也不是什么难事，最可怕的就是envoy中mixer filter的内置标签耦合了k8s太多的label信息，导致其对于consul情景来讲直接不可用（这只是猜测，有待考证）。 业务转发逻辑重复 我们来分析一下流量的转发流程：A要发送流量给B服务，假设B有5个实例，基于consul的服务发现，一定是基于业务代码内置的客户端负载均衡逻辑来选择目标服务实例。业务代码通过consul订阅目标服务的实例状态，转发之前通过算法计算一遍目标服务实例IP，总算把报文送出去了，但到了envoy这一层之后，起会直接忽略报文中的目标IP地址，而按照其目标服务的特征和流量控制的规则，从新计算路由。这在高并发下，势必造成大量的资源消耗和浪费。 k8s基于dns的服务发现就没有这样的烦恼，dns信息都是缓存了的，只要能够将报文送达envoy上，至于下一步怎么路由，那是envoy的事情，业务程序不关心转发。 sidecar部署 在VM上，对于少量的几个服务还好，大不了每个服务手动部署一个sidecar，但是噩梦还没完，当删除业务容器的时候，也要一并删除对应的sidecar。当规模上去了之后，怎么办? 这势必需要一套脚本或者自动化的东西来完成这样繁琐单调的工作。 另外，还可以在VM上部署一个集中的sicecar，然后VM上的应用使用非容器化部署(由于pilot-agent使用--serviceCluster参数的值来向pilot标识自己与主业务容器的关联关系，这种方式的可行性有待考证)。 VM与k8s融合部署 假设按照我们前面的需求，VM要与k8s一同被istio集中管控怎么办？首先服务发现怎么解决，假设使用consul就意味着k8s上的服务也需要注册到consul上来，当然这块业务代码已经实现。但是注册时可能并未按照istio所需要的label信息来注册到consul，这可能导致consul上的服务无法被pilot标识。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"consul","slug":"consul","permalink":"http://ljchen.net/tags/consul/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"istio","slug":"istio","permalink":"http://ljchen.net/tags/istio/"}]},{"title":"Istio与vm融合","slug":"istio与vm融合","date":"2019-05-29T09:45:33.000Z","updated":"2020-04-06T09:24:01.481Z","comments":true,"path":"2019/05/29/istio与vm融合/","link":"","permalink":"http://ljchen.net/2019/05/29/istio与vm融合/","excerpt":"最近在搞ISTIO与VM集成的方案，官网上有一些文档，按照流程部署下去发现问题不少。下面对在遇到验证中遇到的一些问题和调试经验都整理了一下，供大家参考。","text":"最近在搞ISTIO与VM集成的方案，官网上有一些文档，按照流程部署下去发现问题不少。下面对在遇到验证中遇到的一些问题和调试经验都整理了一下，供大家参考。 部署接下来的实验的拓扑图如下所示： 基础网络准备为了使VM与已经部署了ISTIO的K8S集群连通，我们需要先把VM加入到k8s集群，这样会自动部署flannel网络，保障POD网络可通。这块如果使用kubeadm来部署的k8s的话，比较简单，直接kubeadm join就可以加入一个新的集群了。 为了让VM的应用可以直接使用服务名称来通信，需要修改vm的域名解析服务的配置文件 (从k8s的任意一个pod上拷贝/etc/resolv.conf) 123456789101112cat /etc/resolv.conf# Your system has been configured with 'manage-resolv-conf' set to true.# As a result, cloud-init has written this file with configuration data# that it has been provided. Cloud-init, by default, will write this file# a single time (PER_ONCE).##nameserver 183.60.83.19#nameserver 183.60.82.98nameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 预安装123kubectl create namespace istio-systemhelm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f - 安装istio主要是需要指定 meshExpansion 扩展： 1234helm install --set global.meshExpansion.enabled=true install/kubernetes/helm/istio --name istio --namespace istio-system --values install/kubernetes/helm/istio/values-istio-demo.yaml# 或者安装istio（带meshExpansion-认证）helm install --set global.meshExpansion.enabled=true install/kubernetes/helm/istio --name istio --namespace istio-system --values install/kubernetes/helm/istio/values-istio-demo-auth.yaml cluster.env12345678cat &lt;&lt; EOF &gt; cluster.envISTIO_CP_AUTH=MUTUAL_TLS # 需要使用这个值，很重要ISTIO_SERVICE_CIDR=10.244.0.0/16 # Pod子网CIDRISTIO_INBOUND_PORTS=3306,9080 # VM上对外提供服务的端口号POD_NAME=details # 似乎作用不大，可以随意EOFsudo cp cluster.env /var/lib/istio/envoy 准备证书123456789kubectl -n istio-system get secret istio.default -o jsonpath='&#123;.data.root-cert\\.pem&#125;' |base64 --decode &gt; root-cert.pemkubectl -n istio-system get secret istio.default -o jsonpath='&#123;.data.key\\.pem&#125;' |base64 --decode &gt; key.pemkubectl -n istio-system get secret istio.default -o jsonpath='&#123;.data.cert-chain\\.pem&#125;' |base64 --decode &gt; cert-chain.pemsudo mkdir -p /etc/certssudo cp &#123;root-cert.pem,cert-chain.pem,key.pem&#125; /etc/certs 安装sidecarDebian包针对Ubuntu，可以直接从官方下载安装包： 123curl -L https://storage.googleapis.com/istio-release/releases/1.1.6/deb/istio-sidecar.deb &gt; istio-sidecar.debsudo dpkg -i istio-sidecar.deb RPM包针对CentOS系统，需要自己编译安装包： 编译 在本地的GOPATH目录下克隆istio，然后在istio的根目录执行 123cd /go/src/github.com/istio/istiomake rpm/builder-imagemake rpm/proxy 安装 然后在/go/src/out/目录下可以找到对应的rpm包。 在安装rpm包的时候，会发现没有GLIBC_2.18,解决方法如下： 下载：wget http://mirrors.ustc.edu.cn/gnu/libc/glibc-2.18.tar.gz 解压：tar -zxvf glibc-2.18.tar.gz 进入解压文件夹，创建文件夹build, 运行configure配置： 123456mkdir buildcd build../configure --prefix=/usrmake -j4sudo make install 准备用户 12sudo useradd istio-proxysudo chown -R istio-proxy /etc/certs /var/lib/istio/envoy 启动服务12sudo systemctl start istio-auth-node-agentsudo systemctl start istio sidecar.env内容如下 12345## cat sidecar.env | grep -v '#' ISTIO_SERVICE=detailsISTIO_SVC_IP=10.244.3.0ENVOY_USER=istio-proxyISTIO_AGENT_FLAGS=\"--zipkinAddress zipkin.istio-system:9411 --proxyLogLevel debug\" 重启ISTIO服务1systemctl restart istio 部署业务首先使用bookinfo中的示例来部署整个调用链中的服务和workload。 1kubectl apply -f istio-1.1.6/samples/bookinfo/platform/kube/bookinfo.yaml 等k8s中所有服务都起来之后，删除details这个deployment以及其对应的services。然后在github上找到details的ruby代码文件，拷贝到VM上保存为ruby_server.rb。 在VM上需要直接使用ubuntu用户来运行该服务： 1ruby ./ruby_server.rb 9080 在k8s集群里面，重新创建details服务: 12istioctl -n default register details 10.244.3.0 http:9080# 这里的 10.244.3.0 是VM的IP地址，http为通信协议，9080为details服务对外暴露的端口号 部署istio转发规则 12kubectl apply -f istio-1.1.6/samples/bookinfo/networking/destination-rule-all.yamlkubectl apply -f istio-1.1.6/samples/bookinfo/networking/virtual-service-all-v1.yaml 效果展示调用链 Granfana 待解决问题VM网络转发到当前，bookinfo的流量转发应该能够正常，但是还存在一个问题： 如果直接在vm上通过service名称来访问k8s中的其他服务，将无法经过sidecar。其主要原因是iptables，通过查看配置可以看到： 12345# iptables -t nat -nvL OUTPUTChain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 744K 82M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 2285 137K ISTIO_OUTPUT tcp -- * * 0.0.0.0/0 0.0.0.0/0 由于我们在VM上部署了k8s，kube-proxy的KUBE-SERVICES链要优先于ISTIO_OUTPUT导致上面的问题。 Prometheus监控vm-envoy由于prometheus对所有sidecar的envoy监控是通过k8s来做服务发现的，而VM的envoy并没有在k8s中体现出来。这块需要我们手动配置prometheus的target。 具体通过修改istio-system命令空间下prometheus configmap的配置, 添加vm-envoy-stats任务。 12345678910111213global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 1mscrape_configs:- job_name: vm-envoy-stats scrape_interval: 15s scrape_timeout: 10s metrics_path: /stats/prometheus scheme: http static_configs: - targets: - 10.244.3.0:15090 kiali视图当前还没有办法在kiali上展现出vm上流量的视图，虽然可以展示出details的service信息，但是一直报该service缺少了sidecar。我认为这应该和kialai对pod的映射有关，后续考虑看一下它的实现逻辑，应该有解决的方法。 调试经验查看envoy的配置信息对于pod的sidecar，直接通过nsenter进入到pid的net namespace下（对于VM直接在VM上）执行以下命令： 1234567891011121314151617181920212223# curl localhost:15000/helpadmin commands are: /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /contention: dump current Envoy mutex contention stats (if enabled) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /heapprofiler: enable/disable the heap profiler /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /memory: print current allocation/heap usage /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 当然, 对于pod如果是指查看xds的配置，可以直接执行 1234567891011121314151617181920# istioctl psNAME CDS LDS EDS RDS PILOT VERSIONdetails.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3istio-egressgateway-7dbbb87698-ds9j2.istio-system SYNCED SYNCED SYNCED (100%) NOT SENT istio-pilot-59954cff67-wxgnr 1.1.3istio-ingressgateway-565b894b5f-rn2l2.istio-system SYNCED SYNCED SYNCED (100%) NOT SENT istio-pilot-59954cff67-wxgnr 1.1.3productpage-v1-79458795bc-jdvg6.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3ratings-v1-5b7cd6c58f-559nb.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3ratings-v2-mysql-vm-74978d9b97-mlv8r.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3reviews-v1-54c7c79486-wrw6f.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3reviews-v2-7dc5785684-zccst.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3reviews-v3-6c464d7bf4-5px2m.default SYNCED SYNCED SYNCED (50%) SYNCED istio-pilot-59954cff67-wxgnr 1.1.3# istioctl pc listeners productpage-v1-79458795bc-jdvg6.defaultADDRESS PORT TYPE10.244.1.130 9080 HTTP10.103.241.209 31400 TCP10.103.241.209 8060 TCP10.107.214.136 16686 TCP....# 通过指定-o json可以查看到更详细的配置信息 查看envoy是否GRPC上报telemetry到mixer需要先打开envoy的日志模块，具体如下： 1234curl -X POST localhost:15000/logging?level=infocurl -X POST localhost:15000/logging?grpc=tracetailf /var/log/istio/istio.err.log 可以看到如下信息： 123456789101112131415161718192021 strings &#123; key: 3 value: -8 &#125; strings &#123; key: 17 value: -11 &#125; ...&#125;default_words: \"kubernetes://details.default\"default_words: \"details.default.svc.cluster.local\"default_words: \"istio-envoy\"default_words: \"Wed, 29 May 2019 07:33:10 GMT\"default_words: \"178\"default_words: \"origin.ip\"default_words: \"default\"default_words: \"kubernetes://productpage-v1-79458795bc-jdvg6.default\"default_words: \"istio://default/services/details\"...global_word_count: 221 这里就是envoy的mixer filter上报到mixer的attribute信息。每一个key都对应着mixer中内置的attribute关键字的index，每一个value对应default_workds索引。具体参考这篇文章。 之前遇到一个问题，发现envoy有通过grpc Report到mixer；同时在mixer的accessLog也能够看到logentry生成到stdio的访问日志。但是prometheus始终无法从istio-telemetry采集到istio_requests_total的指标。后来调查发现是手动通过istioctl register注册服务的时候，没有指定其协议类型导致的。 查看envoy的业务流量访问日志istio默认是不看业务流量访问日志的，因为日志量太大。如果需要调试，需要手动修改istio-system中的configmap的配置。 123456789kubectl -n istio-system edit cm istio#apiVersion: v1data: mesh: \"# Set the following variable to true to disable policy checks by the Mixer.\\n# Note that metrics will still be reported to the Mixer.\\ndisablePolicyChecks: true\\n\\n# Set enableTracing to false to disable request tracing.\\nenableTracing: true\\n\\n# Set accessLogFile to empty string to disable access log.\\naccessLogFile: \\\"/dev/stdout\\\"\\n\\n# 修改最后一行为/dev/stdout，这样通过查看业务pod的istio-proxy container的日志就可以看到流量的access-log了。当然，对于VM可以直接修改这个值为一个目录，然后在VM上重启istio服务就可以在日志文件中看到流量信息。 123456# kc logs --tail=5 -f productpage-v1-79458795bc-jdvg6 -c istio-proxy[2019-05-29T07:57:21.444Z] \"GET /productpage?u=normal HTTP/1.1\" 200 - \"-\" 0 4083 5034 5034 \"-\" \"curl/7.47.0\" \"9d5d739a-9fbd-9659-ab58-2619a6792936\" \"productpage.default.svc.cluster.local:9080\" \"127.0.0.1:9080\" inbound|9080|http|productpage.default.svc.cluster.local - 10.244.1.130:9080 10.244.3.0:51704 -[2019-05-29T07:57:29.495Z] \"GET /details/0 HTTP/1.1\" 200 - \"-\" 0 178 2 2 \"-\" \"curl/7.47.0\" \"366fc77a-d9d5-9971-9a3b-60576e1ca230\" \"details:9080\" \"10.244.3.0:9080\" outbound|9080||details.default.svc.cluster.local - 10.102.237.154:9080 10.244.1.130:35582 -[2019-05-29T07:57:29.500Z] \"GET /reviews/0 HTTP/1.1\" 500 - \"-\" 0 3926 2510 2509 \"-\" \"curl/7.47.0\" \"366fc77a-d9d5-9971-9a3b-60576e1ca230\" \"reviews:9080\" \"10.244.1.155:9080\" outbound|9080|v3|reviews.default.svc.cluster.local - 10.109.123.127:9080 10.244.1.130:35202 -[2019-05-29T07:57:32.012Z] \"GET /reviews/0 HTTP/1.1\" 500 - \"-\" 0 3926 2510 2509 \"-\" \"curl/7.47.0\" \"366fc77a-d9d5-9971-9a3b-60576e1ca230\" \"reviews:9080\" \"10.244.1.155:9080\" outbound|9080|v3|reviews.default.svc.cluster.local - 10.109.123.127:9080 10.244.1.130:35258 - 查看流表转发流程istio中流量的转发流程可以参考jimmy的这张图, 下面要说的是具体chain中的规则。 inbound流量 12345# iptables -t nat -nvL PREROUTINGChain PREROUTING (policy ACCEPT 15 packets, 900 bytes) pkts bytes target prot opt in out source destination 1969K 174M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 162K 9707K ISTIO_INBOUND tcp -- * * 0.0.0.0/0 0.0.0.0/0 首先查看PREROUTING chain，可以看到，流量被送到ISTIO_INBOUND。 12345# iptables -t nat -nvL ISTIO_INBOUNDChain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 2536 152K ISTIO_IN_REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 343 20580 ISTIO_IN_REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:9080 在ISTIO_INBOUND中，VM提供对外服务的端口3306和9080的流量被转发到 ISTIO_IN_REDIRECT。这里如果VM上面有新添加服务端口，需要修改了cluster.env之后重启istio服务。 1234# iptables -t nat -nvL ISTIO_IN_REDIRECTChain ISTIO_IN_REDIRECT (2 references) pkts bytes target prot opt in out source destination 2962 178K REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15001 在 ISTIO_IN_REDIRECT中，流量被送到了envoy处理。 outbound流量 同理，这里只给出对应的链表规则： 12345# iptables -t nat -nvL OUTPUTChain OUTPUT (policy ACCEPT 24 packets, 2927 bytes) pkts bytes target prot opt in out source destination 782K 86M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 5033 302K ISTIO_OUTPUT tcp -- * * 0.0.0.0/0 0.0.0.0/0 1234567891011 # iptables -t nat -nvL ISTIO_OUTPUTChain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 ISTIO_REDIRECT all -- * lo 0.0.0.0/0 !127.0.0.1 790 47400 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner UID match 1000 4387 263K RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner UID match 0 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner GID match 1000 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner GID match 0 0 0 RETURN all -- * * 0.0.0.0/0 127.0.0.1 0 0 ISTIO_REDIRECT all -- * * 0.0.0.0/0 10.244.0.0/16 2 120 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 这里的uid 1000其实就是envoy进程的uid，而0是root。之所以这样设置，是因为如果将root的流量都干掉了，将导致ssh无法连接主机。继续查看ISTIO_REDIRECT链的规则： 1234# iptables -t nat -nvL ISTIO_REDIRECTChain ISTIO_REDIRECT (2 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15001 这样就基本明白了inbound和outbound流量被送进envoy之前的转发过程了。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"istio","slug":"istio","permalink":"http://ljchen.net/tags/istio/"}]},{"title":"Apache Flink原理","slug":"Apache-Flink原理","date":"2019-05-12T01:22:52.000Z","updated":"2020-04-06T09:24:01.464Z","comments":true,"path":"2019/05/12/Apache-Flink原理/","link":"","permalink":"http://ljchen.net/2019/05/12/Apache-Flink原理/","excerpt":"Flink是Apache下的一款分布式的计算引擎，它的亮点在于处理实时数据流（无界数据流），实时地产生数据的结果；当然，通过划分窗口（时间窗口等）同样适用于批处理（有界数据流）。想想Spark streaming也可以处理实时数据呀，那为什么会诞生flink呢？flink与spark相比有哪些特色？下文将逐个介绍这些内容。","text":"Flink是Apache下的一款分布式的计算引擎，它的亮点在于处理实时数据流（无界数据流），实时地产生数据的结果；当然，通过划分窗口（时间窗口等）同样适用于批处理（有界数据流）。想想Spark streaming也可以处理实时数据呀，那为什么会诞生flink呢？flink与spark相比有哪些特色？下文将逐个介绍这些内容。 flink Vs. spark更多比较见美图数据团队的文章Spark Streaming VS Flink 架构[TODO] 任务执行过程[TODO] 实践 使用brew安装 1brew install apache-flink 使用docker-compose部署 123456789101112131415161718192021222324version: \"2.1\"services:jobmanager: image: $&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125; expose: - \"6123\" ports: - \"8081:8081\" command: jobmanager environment: - JOB_MANAGER_RPC_ADDRESS=jobmanagertaskmanager: image: $&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125; expose: - \"6121\" - \"6122\" depends_on: - jobmanager command: taskmanager links: - \"jobmanager:jobmanager\" environment: - JOB_MANAGER_RPC_ADDRESS=jobmanager","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://ljchen.net/tags/Flink/"},{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"}]},{"title":"Spark介绍","slug":"spark介绍","date":"2019-05-11T13:40:07.000Z","updated":"2021-01-25T15:51:19.972Z","comments":true,"path":"2019/05/11/spark介绍/","link":"","permalink":"http://ljchen.net/2019/05/11/spark介绍/","excerpt":"Spark是基于内存计算的大数据并行计算框架.Spark基于内存计算，提高了在大数据环境下数据处理的实时性。它是MapReduce的替代方案，而且兼容HDFS、Hive等分布式存储层，可融入Hadoop的生态系统，以弥补缺失MapReduce的不足。","text":"Spark是基于内存计算的大数据并行计算框架.Spark基于内存计算，提高了在大数据环境下数据处理的实时性。它是MapReduce的替代方案，而且兼容HDFS、Hive等分布式存储层，可融入Hadoop的生态系统，以弥补缺失MapReduce的不足。通过下面这张图，可以了解到Spark的主要组件及其功能。 可以看到，为了兼容Apache大数据生态，Spark在底层支持三种资源调度框架： Standalone: Spark原生的资源管理，由Master负责资源的分配。 Apache Mesos: 与 Hadoop MapReduce兼容性良好的一种资源调度框架。 Hadoop Yarn: 主要是指的Yarn中的ResourceManager。 架构Spark内部主要涉及两个核心组件，下面是其关系图。 预习知识: 用户编写的Spark应用程序，其中包含了Driver功能的代码和分布在集群中多个节点上运行的Executor代码。 Driver：在一个Spark集群中，有一个节点负责中央协调，调度各个分布式工作节点；这个中央协调节点被称为驱动器节点。Driver运行Application的main()函数并创建SparkContext（负责与ClusterManager通信，进行资源的申请、任务的分配和监控等）。 把用户程序（Application）转为任务 为执行器调度任务 Executor：与之对应的工作节点。 负责运行组成Spark应用的任务，并将结果返回给Driver; 通过自身的块管理器(Block Manager)为用户程序中要求缓存的RDD提供内存式存储。 任务执行过程通用流程 用户通过 spark-submit 脚本提交应用； spark-submit 脚本启动Driver，调用用户定义的 main() 方法； Driver与集群管理器通信，申请资源以启动Executor； 集群管理器为Driver启动Executor； Driver执行用户应用中的操作（根据程序中所定义的对RDD的转化操作和行动操作，Drvier把工作以任务的形式发送到Executor）； 任务在Executor中进行计算并保存结果； 如果Driver的 main() 方法退出，或者调用了SparkContext.stop()，Driver会终止Executor，并且通过集群管理器释放资源。 Yarn-Cluster流程 Spark Yarn Client向Yarn中提交应用程序； ResourceManager收到请求后，在集群中选择一个NodeManager，并为该应用程序分配一个Container，在这个 Container 中启动应用程序的ApplicationMaster，ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，并在获得的Container中启动 CoarseGrainedExecutorBackend，启动后会向ApplicationMaster中的SparkContext注册并申请Task； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 有没有发现这和之前讲的Yarn调度MapReduce的流程很像，具体见Hadoop原理-任务创建流程 RDD与操作 actions：这类算子会触发 SparkContext 提交 Job 作业，并将数据输出 Spark系统。 transformations：转换, 从现有RDD创建一个新的RDD。这种变换并不触发提交作业，只是完成作业中间过程处理。Transformation 是延迟计算的，也就是说从一个 RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。 实践 本地docker启动spark(自带hadoop集群，使用yarn来启动spark) 1docker run -it -p 8088:8088 -p 8042:8042 -h sandbox sequenceiq/spark:1.6.0 bash 也可通过brew来安装 12345[~] brew install apache-spark[/usr/local/Cellar/apache-spark/2.4.2/bin] lsdocker-image-tool.sh load-spark-env.sh run-example spark-class spark-sql sparkRfind-spark-home pyspark spark-beeline spark-shell spark-submit 安装包 123456wget http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgztar -xzvf spark-3.0.1-bin-hadoop3.2.tgzmv spark-3.0.1-bin-hadoop3.2 spark-3.0.1pip install pyspark 配置 spark-env.sh 1234567891011cd $SPARK_HOME/confcp spark-env.sh.template spark-env.sh# 配置spark-env.shexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_MASTER_IP=localhostexport SPARK_LOCAL_DIRS=$SPARK_HOMEexport SPARK_DRIVER_MEMORY=1g# spark-history-server, 需要在hdfs上创建/spark-logs 目录export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://localhost:9000/spark-logs\" slaves 12cd $SPARK_HOME/confcp slaves.template slaves spark-defaults.conf (用于启用history-server) 12345678cp spark-defaults.conf.template spark-defaults.conf# 配置spark-defaults.confspark.master spark://localhost:7077spark.eventLog.enabled truespark.eventLog.dir hdfs://localhost:9000/spark-logsspark.serializer org.apache.spark.serializer.KryoSerializerspark.driver.memory 1g 启动 启动服务 123456789101112$SPARK_HOME/sbin/start-all.sh# 查看多了Master和Worker进程[root@aliyun-vm spark-3.0.1]# jps 9633 Worker 2005 SecondaryNameNode 1654 NameNode 9576 Master 9720 Jps 1790 DataNode 2286 ResourceManager 2415 NodeManager 启动history-server （可选） 12345678910111213$SPARK_HOME/sbin/start-history-server.sh# 查看多了HistoryServer进程[root@aliyun-vm spark-3.0.1]# jps 9633 Worker 9762 HistoryServer 2005 SecondaryNameNode 1654 NameNode 9576 Master 9804 Jps 1790 DataNode 2286 ResourceManager 2415 NodeManager 访问web界面spark-master: http://localhost:8080History-server: http://localhost:18080 连接到spark 其中--deploy-mode参数的意思是：是否发布驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*如果设置client模式, 驱动直接在spark-submit进程中启动,输入输出都可以显示在控制台.。所以这种模式特别适合REPL(读取-求值-输出循环)， 比如Spark shell。 python方式 1234567891011121314# 在yarn上调度spark任务，完成后，可以在yarn上看到任务信息。#提前使用hdfs dfs命令上传data目录下的users.txt到hdfs的根目录，然后执行以下操作。$SPARK_HOME/bin/pyspark \\--master yarn \\--deploy-mode client&gt;&gt;&gt; textFile = spark.read.text(\"/users.txt\")&gt;&gt;&gt; textFile.count()7&gt;&gt;&gt; textFile.first()Row(value=u'1,BarackObama,Barack Obama')&gt;&gt;&gt; linesWithSpark = textFile.filter(textFile.value.contains(\"Obama\"))&gt;&gt;&gt; textFile.filter(textFile.value.contains(\"Obama\")).count()1 提交任务 12345678910111213# 在spark-master中启动应用，完成后，可以在spark web ui中看到任务信息。$SPARK_HOME/bin/spark-submit \\ --master spark://localhost:7077 \\ --deploy-mode client \\ examples/src/main/python/pi.py \\ 10 # 在yarn中启动应用，完成后，可以在spark history web ui中看到任务信息。$SPARK_HOME/bin/spark-submit \\ --master yarn \\ --deploy-mode cluster \\ hdfs://localhost:9000/pi.py \\ 10 java方式 1$SPARK_HOME/bin/spark-shell Spark-SQL 将Hive元数据的配置文件放到spark/conf下： 123cd $SPARK_HOMEcd conf cp $&#123;HIVE_HOME&#125;/conf/hive-site.xml . #hive-site.xml中配置了元数据的连接 在Spark中加入mysql的driver驱动包（因为Hive元数据是存储在MySQL上）cp包进$SPARK_HOME/jars注意 Hive底层是HDFS，一定要保证HDFS进程是开启的。 mysql的服务需要是开启的。 想要使用Spark整合Hive，在编译Spark的时候需要在编译指令中加入-Phive -Phive-thriftserver。 因为Spark配置中加入了hive-site.xml，所以默认的文件系统就是HDFS。否则就是本地文件系统，create table的时候就会在本地创建/user/hive/warehouse目录 。","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"},{"name":"Spark","slug":"Spark","permalink":"http://ljchen.net/tags/Spark/"}]},{"title":"Sqoop学习","slug":"Sqoop学习","date":"2019-05-11T06:56:12.000Z","updated":"2020-04-06T09:24:01.469Z","comments":true,"path":"2019/05/11/Sqoop学习/","link":"","permalink":"http://ljchen.net/2019/05/11/Sqoop学习/","excerpt":"Sqoop用来在关系型数据库与HDFS（Hive、HBase）之间导入和导出数据，其既可以作为ETL抽数据到DW的工具，又可以作为对DM处理好的数据导出到业务数据库的工具，其地位如下图所示：","text":"Sqoop用来在关系型数据库与HDFS（Hive、HBase）之间导入和导出数据，其既可以作为ETL抽数据到DW的工具，又可以作为对DM处理好的数据导出到业务数据库的工具，其地位如下图所示： Sqoop主要通过JDBC和关系数据库进行交互。理论上支持JDBC的Database都可以使用Sqoop和HDFS进行数据交互。 原理Sqoop有两个版本，他们之间的差异见下表： 我们这里重点将下Sqoop2的架构 导入到HDFS 在导入前，Sqoop使用JDBC来检查将要导入的数据表; Sqoop检索出表中所有的列以及列的SQL数据类型; 把这些SQL类型的映射到java数据类型，例如（VARCHAR、INTEGER）-&gt;（String，Integer）; 在MapReduce应用中将使用这些对应的java类型来保存字段的值; Sqoop的代码生成器使用这些信息来创建对应表的类，用于保存从表中抽取的记录。 导出到JDBC 在导出前，sqoop会根据数据库连接字符串来选择一个导出方法(对于大部分系统来说，sqoop会选择JDBC); Sqoop会根据目标表的定义生成一个java类; 这个生成的类能够从文本中解析出记录，并能够向表中插入类型合适的值（除了能够读取ResultSet中的列）; 然后启动一个MapReduce作业，从HDFS中读取源数据文件; 使用生成的类解析出记录，并且执行选定的导出方法。 实践时间将 MySQL数据库的表dept导入HDFS1sqoop import --connect jdbc:mysql://10.10.75.100:3306/sqoopdemo -- username root --password root123 - table dept -m 1 -target-dir /user/dept 将数据从HDFS导出到MySQL数据库的表dept12345678910sqoop export --connect jdbc :mysql://10.10.75.100:3306/sqoopdemo --username root --password root123 --table dept -m 1 --export-dir /user/dept # Mysql-&gt;hivesqoop import --connect jdbc:mysql://10.10.75.100:3306/sqoopdemo --username root --password root123 --table dept -m 1 --hive-import# Hive-&gt;mysqlsqoop export --connect jdbc:mysql://10.10.75.100:3306/sqoopdemo --username root --password root123 -table dept -m 1 --export-dir /user/hive/warehouse/dept --input-fields-terminated-by '\\0001'# Mysql-&gt;hbasesqoop import --connect jdbc:mysql://10.10.75.100:3306/sqoopdemo --username root --password rootl23 --table dept 一-hbase-create-table --hbase- table hbase dept --column-family colfamily --hbase-row-key id 更多操作方面的内容可以参考这篇Sqoop架构以及应用介绍。","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"},{"name":"Sqoop","slug":"Sqoop","permalink":"http://ljchen.net/tags/Sqoop/"}]},{"title":"Hive原理","slug":"Hive原理","date":"2019-05-10T13:53:38.000Z","updated":"2021-01-24T12:16:10.883Z","comments":true,"path":"2019/05/10/Hive原理/","link":"","permalink":"http://ljchen.net/2019/05/10/Hive原理/","excerpt":"相对于hadoop复杂的job（MapReduce）编写过程，Hive提供了HQL（类似于SQL的）语言来高效的生成查询job，方便数据工程师在不会编程的情况下，进行并专注于数据分析。从本质上讲，Hive同Pig一样，都是一种简化批处理任务编程复杂度的工具，下面我们就来总体认识一下。","text":"相对于hadoop复杂的job（MapReduce）编写过程，Hive提供了HQL（类似于SQL的）语言来高效的生成查询job，方便数据工程师在不会编程的情况下，进行并专注于数据分析。从本质上讲，Hive同Pig一样，都是一种简化批处理任务编程复杂度的工具，下面我们就来总体认识一下。 服务端组件 Driver组件：该组件包括：Compiler、Optimizer、Executor,它可以将Hive的HQL进行编译、解析、优化后，最终转化为MapReduce任务提交给Hadoop 1中的JobTracker，或者Hadoop 2中的ResourceManager来执行实际的任务。 MetaStore组件：存储着Hive的元数据信息，Hive将自己的元数据存储到了关系型数据库当中，支持的数据库主要有：Mysql、Derby、支持把metastore独立出来放在远程的集群上面，使得hive更加健壮。元数据主要包括了表的名称、表的列、分区和属性、表的属性（是不是外部表等等）、表的数据所在的目录。 用户接口：CLI(常用的接口：命令行模式）、Client:Hive的客户端用户连接至Hive Server, 在启动Client的时候，需要指定Hive Server所在的节点，并且在该节点上启动Hive Server、Web UI:通过浏览器的方式访问Hive。 任务执行流程 用户提交查询等任务给Driver； Compiler获得该用户的任务Plan； Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息； Compiler得到元数据信息，对任务进行编译 先将HiveQL转换为抽象语法树， 然后将抽象语法树转换成查询块， 将查询块转化为逻辑的查询计划， 重写逻辑查询计划，将逻辑计划转化为物理的计划 最后选择最佳的策略； 将最终的计划提交给Driver； Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者ResourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作； 获取执行的结果； 取得并返回执行结果。 数据模型 Table类似于传统数据库中的Table，每一个Table在Hive中都有一个相应的目录来存储数据。例如：一个表t，它在HDFS中的路径为：/user/hive/warehouse/t。内部表做删除表，就删除了目录及数据。 External Table指向已存在HDFS中的数据，可创建Partition。数据是存储在Location后面指定的HDFS路径中的，并不会移动到数据仓库中。删除外部表，只是删除了元数据的信息，该外部表所指向的数据是不会被删除。 Partition 在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition数据都存储在对应的目录中。例如：t表中包含ds和city两个Partition，则对应于ds=2014，city=beijing的HDFS子目录为：/user/hive/warehouse/t/ds=2014/city=Beijing；需要注意的是，分区列是表的伪列，表数据文件中并不存在这个分区列的数据。一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。分区避免 Hive Select查询中扫描整个表内容，防止消耗很多时间做没必要的工作（相当于只查询了子表&lt;子目录&gt;）。 Buckets 对指定列计算的hash，根据hash值切分数据，目的是为了便于并行，每一个Buckets对应一个文件。将user列分至32个Bucket上，首先对user列的值计算hash，比如，对应hash=0的HDFS目录为：/user/hive/warehouse/t/ds=2014/city=Beijing/part-00000；对应hash=20的目录为：/user/hive/warehouse/t/ds=2014/city=Beijing/part-00020。 buckets是对partition的一种补充，具体见以下例子： 假设我们有一张日志表，我们需要按照日期和用户id来分区，目的是为了加快查询谁哪天干了什么。但是这里面用user_id去切分的话，就会产生很多很多的分区了，这些分区可大可小，这个数量是文件系统所不能承受的。在这种情况下，我们既想加快查询速度，又避免出现如此多的小分区，篮子（bucket）就出现了。 123CREATE TABLE weblog (user_id INT, url STRING, source_ip STRING) &gt; PARTITIONED BY (dt STRING) &gt; CLUSTERED BY (user_id) INTO 96 BUCKETS; 首先按照日期分区，分区结束之后再按照user_id把日志放在96个篮子，这样同一个用户的所有日志都会在同一个篮子里面，并且一个篮子里面有好多用户的日志。 操作实践安装包1234wget https://mirrors.sonic.net/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gztar -xzvf apache-hive-3.1.2-bin.tar.gzmv apache-hive-3.1.2-bin hive-3.1.2 配置 hive-default.xml 123456789101112131415cd $HIVE_HOME/confcp hive-default.xml.template hive-default.xml #注意修改thrift账号密码#写入xml&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.user&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Username to use against thrift client&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.password&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Password to use against thrift client&lt;/description&gt;&lt;/property&gt; hive-env.sh 12345cd $HIVE_HOME/confcp hive-env.sh.template hive-env.sh#追加内容HADOOP_HOME=/root/bigData/hadoop-3.3.0 hive-site.xml 需要在$HIVE_HOME/conf/下创建hive-site.xml，并写入以下值： 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt; &lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:derby:$&#123;HIVE_HOME&#125;/metastore_db;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 初始化与启动 初始化metastore 1234567891011121314151617181920212223 $HIVE_HOME/bin/schematool -initSchema -dbType derby # 遇到以下报错 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/root/bigData/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/root/bigData/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Exception in thread \"main\" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)at org.apache.hadoop.conf.Configuration.set(Configuration.java:1380)at org.apache.hadoop.conf.Configuration.set(Configuration.java:1361)at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:448)at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141)at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104)at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96)at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at org.apache.hadoop.util.RunJar.run(RunJar.java:323)at org.apache.hadoop.util.RunJar.main(RunJar.java:236) 解决方案为：比较hadoop安装目录下share/hadoop/common/lib内guava.jar版本，与hive安装目录下lib内guava.jar的版本 如果两者不一致，删除版本低的，并拷贝高版本的。 hive命令行启动 1234567891011$HIVE_HOME/bin/hive# 启动后，会发现多了一个RunJar进程[root@aliyun-vm ~]# jps 6882 NodeManager 6754 ResourceManager 4164 SecondaryNameNode 3814 NameNode 3946 DataNode 8236 RunJar 8430 Jps metastore启动 1$HIVE_HOME/bin/hive --service metastore hiveserver2启动 123456$HIVE_HOME/bin/hiveserver2 --hiveconf hive.root.logger=DEBUG,console# 使用客户端连接$HIVE_HOME/bin/beelinebeeline&gt; !connect jdbc:hive2://127.0.0.1:10000/default #账号密码为前面设置的hive/hivebeeline&gt; !close 访问HiveServer2 web-ui http://localhost:10002/ HSQL配置 创建一个库表，表名为 cwqsolo， 含有2个字段 id， name，并且记录中各个字段的分割符为 ‘,’，在ascii中表示为 ‘\\054’; 1CREATE TABLE cwqsolo(id INT, name STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\054’; 修改字段名为ID的字段，修改后名称为 myid， string类型 1ALTER TABLE cwqsolo CHANGE id myid String; 12345678910111213141516CREATE TABLE `employees`( `employeenumber` int, `lastname` string, `firstname` string, `extension` string, `email` string, `officecode` string, `jobtitle` string) COMMENT 'import from mysql' PARTITIONED BY (reportsto int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS textfile; 增加一个字段，字段名为 sex，性别 1ALTER TABLE cwqsolo ADD COLUMNS (sex STRING COMMENT 'sex type'); 加载文件 1234[root@archive studydata]# vi test1.txt 1001,cwq,male 1101,lxj,female hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/hive/studydata/test1.txt' INTO TABLE cwqsolo; 导入employees数据到表 1LOAD DATA LOCAL INPATH './classicmodels_employees.csv' OVERWRITE into table employees PARTITION (reportsto='1002'); 插入条目 12insert into table cwqsolo values ( '1005', 'ddd','male' ); # 追加insert overwrite table test_insert select * from test_table; # 覆盖 执行SQL文本 1hive -f /home/hadoop/hive/sql/creat_table_biss.sql 导出文本到本地 123insert overwrite local directory '/home/data/cwq/' select * from zc_bicc_isup_bdr where calling= '13003097698'; insert overwrite local directory '/home/data/cwq/output1’ row format delimited fields terminated by ',' select * from zc_bicc_isup_bdr where calling= '13003097698’; # 指定分隔符","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"},{"name":"Hive","slug":"Hive","permalink":"http://ljchen.net/tags/Hive/"}]},{"title":"HBase原理","slug":"HBase原理","date":"2019-04-27T01:16:35.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2019/04/27/HBase原理/","link":"","permalink":"http://ljchen.net/2019/04/27/HBase原理/","excerpt":"HDFS提供了分布式底层文件系统，解决了基础设施层面文件存储的问题，但是对于文件内容的查询等操作需要依赖于MapReduce计算框架，且效率低下。HBase支持在HDFS之上存储大文件，并提供比HDFS更高效检索数据的框架，下面是学习的一写资料整理。","text":"HDFS提供了分布式底层文件系统，解决了基础设施层面文件存储的问题，但是对于文件内容的查询等操作需要依赖于MapReduce计算框架，且效率低下。HBase支持在HDFS之上存储大文件，并提供比HDFS更高效检索数据的框架，下面是学习的一写资料整理。 总体架构下图是网上找到的，虽然版本比较旧，涉及的一些内容在新版本上已有更新，但是基本表述清楚了HBase的整体架构和核心原理。 数据流程要了解数据流程，除了知道前面大图中涉及的HBase基本组件之外，还需要了解以下知识点： HRegion由多个Store组成（每个Store对应一个clumne family），Store内部包含memstore和多个HFile（多个HFile的原因是Minor Compact会生成很多的storeFile，其实storeFile就是HFile）； region server上的所有region都共享同一个HLog。 读取流程 client访问zookeeper获取hbase:meta表的地址； 通过查询hbase:meta表，client获取到目标对象region所在的region server；（在老版本中是三层结构，新版本为两层） client直接去region server访问数据; client会将region server与region的映射缓存到本地，只有在发生读取错误的情况下才会重新走一遍前面的查询流程。 写入流程 client请求到达region server，region server在写完HLog以后，数据写入的下一个目标就是region的memstore； 写入到memstore后，该次写入请求就可以被返回，HBase即认为该次数据写入成功（支持三种刷盘方式）； 通过全局内存控制，触发memstore刷盘操作 手动触发memstore刷盘操作 memstore上限触发数据刷盘 每次memstore的刷盘都会相应生成一个存储文件storeFile（即HFile在HBase层的轻量级封装）； region server通过compact把大量小的HFile进行文件合并，生成大的HFile文件（支持两种压缩类型）； Minor Compact Major Compact（对整个region下相同CF的所有HFile进行compact，清理过期或者被删除的数据） Region分裂HBase同样提供了region的 split方案来解决大的HFile造成数据查询时间过长问题。一个较大的region(指其内部的所有sotre总和达到阀值)通过split操作，会生成两个小的region，称之为Daughter。 流程： region先更改ZK中该region的状态为SPLITING； Master检测到region状态改变； region会在存储目录下新建.split文件夹用于保存split后的daughter region信息； Parent region关闭数据写入并触发flush操作，保证所有写入Parent region的数据都能持久化； 在.split文件夹下新建两个region，称之为daughter A、daughter B； Daughter A、Daughter B拷贝到HBase根目录下，形成两个新的region； Parent region通知修改.META.表后下线，不再提供服务； Daughter A、Daughter B上线，开始向外提供服务； 如果开启了balance_switch服务，split后的region将会被重新分布。 过滤器TODO","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"},{"name":"HBase","slug":"HBase","permalink":"http://ljchen.net/tags/HBase/"}]},{"title":"Hadoop原理","slug":"hadoop原理","date":"2019-04-27T00:25:00.000Z","updated":"2021-01-24T09:38:58.384Z","comments":true,"path":"2019/04/27/hadoop原理/","link":"","permalink":"http://ljchen.net/2019/04/27/hadoop原理/","excerpt":"大数据平台底层都是采用Hadoop作为框架，Hadoop包含底层的分布式文件系统和资源调度框架Yarn；当然，算上计算框架还有MapReduce。这里重点总结一下HDFS和其资源调度框架Yarn的原理。","text":"大数据平台底层都是采用Hadoop作为框架，Hadoop包含底层的分布式文件系统和资源调度框架Yarn；当然，算上计算框架还有MapReduce。这里重点总结一下HDFS和其资源调度框架Yarn的原理。 HDFS系统架构这块就不用怎么介绍了，主要是NameNode和DataNode，一个用于存储数据块和文件映射、文件属性信息，另一个用来存放真正的业务数据; 重点要说下NameNode。 NameNode EditLog事务日志文件记录每一个对文件系统元数据的改变，如在HDFS中创建一个新的文件，名称节点将会在EditLog中插入一条记录来记录这个改变。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等等都存放在Fslmage文件中。 NameNode HA NameNode用于存储FsImage与EditLog，其高可用支持两种解决方案，其本质都是实现存储的高可用： 运行多个NameNode，并使用类似于NFS之类的共享文件存储； 运行多个NameNode，同时使用journalNode用来存储数据。 一个NameNode处于Active状态，另一个处于Standby状态。Active NameNode对外提供服务，Standby NameNode仅通过JournalNode同步Active NameNode的状态，以便能够在它失败时快速进行切换（使用ZooKeeper）。 每个Journal节点暴露RPC接口，允许NameNode读取和写入数据，数据存放在Journal节点的本地磁盘。当Active NameNode写入 EditLog时，它向集群的所有JournalNode发送写入请求，当多数节点回复确认成功写入之后， EditLog就认为是成功写入。 SecondaryNameNode 由于每次nameNode启动时，都需要先加载FsImage，再加载EditLog，在合并后写入到FsImage中。如果当文件较大时，该操作较耗时。因此，增加了secondaryNameNode角色，用于按照EditLog文件大小阈值或周期性时间触发的方式来执行FsImage与EditLog的merge操作。 数据流基本概念 block文件上传前需要分块，这个块就是block，一般为128MB，可以调整。 块太小：寻址时间占比过高；块太大：Map任务数太少，作业执行速度变慢。 packetpacket是第二大的单位，它是client端向DataNode，或DataNode的PipLine之间传数据的基本单位，默认64KB。 chunkchunk是最小的单位，它是client向DataNode，或DataNode的PipLine之间进行数据校验的基本单位，默认512Byte。 写入流程 客户端请求NameNode上传数据，NameNode检查是否已存在文件、检查权限；通过后为其分配资源，并写入EditLog返回stream； HDFSStream会有一个chunk buff，客户端每写满一个chunk后，会计算校验和； 带有校验和的chunk被写入packet，当客户端一个packet满后被发送到服务端，服务端packet会进入dataQueue队列； 其他机架的DataNode从服务端dataQueue获取client端上传的packet，并存储； 一个DataNode成功存储一个packet后之后就会返回一个ack packet，放入ack Queue中； client写完数据后，关闭stream，发送完成信号给NameNode将元数据刷到FsImage。 读取流程 client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，并返回输入流对象； client基于返回的列表，就近挑选一台DataNode，请求建立输入流； DataNode向输入流中写数据，以packet为单位来校验； client关闭输入流。 其他如果文件小于块大小（128M），会占用一个块资源，但是在硬盘上大小依然是文件的大小。块的概念是为了切分大文件，同时便于对map按照块来操作数据。 Yarn架构 包含以下组件： Resource Manager Application Manager Yarn Scheduler Node Manager Application Master Container 任务创建流程 用户向YARN中提交应用，其中包括Application Master程序、启动Application Master的命令、用户程序等； Resource Manager内部包含了Application Manager和\bYarn Scheduler，RM为应用分配第一个Container，并要求Node Manager在Container中启动应用程序的Application Master； Application Master向Resource Manager注册自己，这样用户就可以通过Resource Manager感知应用的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7； Application Master基于应用中任务内容，向Resource Manager申请资源； 申请到资源后，AM与分配到的Node Manager通信，要求它启动任务； Node Manager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务； 各任务向Application Master汇报状态和进度，在任务失败时重新启动任务； 应用程序运行完成后，Application Master向Resource Manager注销并关闭自己。 操作实践安装包123wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gzhadoop-3.3.0.tar.gztar -xzvf hadoop-3.3.0.tar.gz 配置配置文件 etc/hadoop/core-site.xml 把下面的{root}换成代理账户，比如你mac的登录账户名。 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.&#123;root&#125;.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.&#123;root&#125;.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置远程登录mac上需要打开 共享 中的 远程登录 功能，然后执行以下配置，保障 ssh localhost 不再要求输入密码。 123ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 格式化HDFS文件 1$HADOOP_HOME/bin/hdfs namenode -format 配置变量在start-dfs.sh中： 1234HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root 在start-yarn.sh中 123YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root hadoop-env.sh 1export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk 启动脚本 使用命令启动hdfs和yarn： 12345$HADOOP_HOME/sbin/start-dfs.sh$HADOOP_HOME/sbin/start-yarn.sh$HADOOP_HOME/bin/hdfs dfs -mkdir /user$HADOOP_HOME/bin/hdfs dfs -mkdir /user/ljchen 查看各角色运行状态： 1234567[root@aliyun-vm ~]# jps6882 NodeManager6754 ResourceManager4164 SecondaryNameNode3814 NameNode3946 DataNode7021 Jps 访问节点 节点类型 访问地址 HDFS NameNode http://localhost:9870 Yarn http://localhost:8088","categories":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/categories/big-data/"}],"tags":[{"name":"big-data","slug":"big-data","permalink":"http://ljchen.net/tags/big-data/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://ljchen.net/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://ljchen.net/tags/HDFS/"},{"name":"Yarn","slug":"Yarn","permalink":"http://ljchen.net/tags/Yarn/"}]},{"title":"Elasticsearch学习笔记","slug":"elastic-stack学习笔记","date":"2019-04-13T01:09:18.000Z","updated":"2020-04-06T09:24:01.481Z","comments":true,"path":"2019/04/13/elastic-stack学习笔记/","link":"","permalink":"http://ljchen.net/2019/04/13/elastic-stack学习笔记/","excerpt":"在容器云的日志收集和分析中，没有采用CNCF的fluentd方案，而是直接采用了elastic-stack的一整套解决方案。 通过服务编排的时候写日志收集的CRD到K8S，在主机上通过daemonset的方式来运行fileBeat收集容器日志；fileBeat的output是kafka，通过消息中间件来做汇聚和压力削峰，接下来再是logstach和elasticsearch。这只是日志收集的流程，其实里面还涉及具体如何做系统的负载均衡，日志流的告警等，这里就不一一分析。 本文重点聊下elasticsearch相关的一些知识点。","text":"在容器云的日志收集和分析中，没有采用CNCF的fluentd方案，而是直接采用了elastic-stack的一整套解决方案。 通过服务编排的时候写日志收集的CRD到K8S，在主机上通过daemonset的方式来运行fileBeat收集容器日志；fileBeat的output是kafka，通过消息中间件来做汇聚和压力削峰，接下来再是logstach和elasticsearch。这只是日志收集的流程，其实里面还涉及具体如何做系统的负载均衡，日志流的告警等，这里就不一一分析。 本文重点聊下elasticsearch相关的一些知识点。 ES安装部署 1docker run --name=elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS=-Xms256m -Xmx256m\" -d elasticsearch:7.3.0 插件命令 12345678910111213141516171819root@2c03a6312e66:/usr/share/elasticsearch# elasticsearch-plugin --helpA tool for managing installed elasticsearch pluginsCommands--------list - Lists installed elasticsearch pluginsinstall - Install a pluginremove - removes a plugin from ElasticsearchNon-option arguments:commandOption Description------ ------------h, --help show help-s, --silent show minimal output-v, --verbose show verbose output# 安装插件 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.3.0/elasticsearch-analysis-ik-7.3.0.zip Kibana安装 1docker run -d --link elasticsearch:elasticsearch --name kibana -p 5601:5601 kibana:7.3.0 ES核心知识主要涉及对ES整体框架认识理解的几个方面。 集群管理节点角色 Master节点 Master节点控制Elasticsearch集群，并负责在集群范围内创建/删除索引，跟踪哪些节点是集群的一部分，并将分片分配给这些节点。主节点一次处理一个集群状态，并将状态广播到所有其他节点，这些节点需要响应并确认主节点的信息。 在elasticsearch.yml中，将nodes.master属性设置为true（默认），可以将节点配置为有资格成为主节点的节点。对于大型生产集群，建议拥有一个专用主​​节点来控制集群，并且不服务任何用户请求。 Data节点 数据节点用来保存数据和倒排索引。默认情况下，每个节点都配置为一个data节点，并且在elasticsearch.yml中将属性node.data设置为true。如果您想要一个专用的master节点，那么将node.data属性更改为false。 Client节点 如果将node.master和node.data设置为false，则将节点配置为客户端节点，并充当负载平衡器，将传入的请求路由到集群中的不同节点。若你连接的是作为客户端的节点，该节点称为协调节点(coordinating node)。协调节点将客户机请求路由到集群中对应分片所在的节点。对于读取请求，协调节点每次选择不同的分片来提供请求以平衡负载。 跨集群搜索相当于在多个ES集群之外，配置一个ES作为代理，这个代理节点不会加入到存储数据的ES集群中；它只是作为查询代理节点。 配置的方式可以是直接修改yaml文件，或者通过HTTP的方式下发配置，指定remote ES cluster。 123456789101112131415161718192021222324PUT _cluster/settings&#123; \"persistent\": &#123; \"cluster\": &#123; \"remote\": &#123; \"cluster_one\": &#123; \"seeds\": [ \"127.0.0.1:9300\" ] &#125;, \"cluster_two\": &#123; \"seeds\": [ \"127.0.0.1:9301\" ] &#125;, \"cluster_three\": &#123; \"seeds\": [ \"127.0.0.1:9302\" ] &#125; &#125; &#125; &#125;&#125; 具体参考这里 故障检测zen discovery模块 Ping：执行该过程的节点，用来发现对方。 Unicast：该模块包含一个主机名列表，用于控制要ping哪个节点。 故障检测的过程是这样的: 主节点ping所有的节点来检查这些节点是否存活，而所有的节点回ping主节点来汇报他们是存活的。 discovery.zen.minimum_master_nodes推荐的值为: (num_master_node / 2) + 1 假设我们有一个集群，有3个master节点，当网络发生故障的时候，有可能其中一个节点不能和其他节点进行通信了。这个时候，当discovery.zen.minimum_master_nodes设置为1，就会分成两个小的独立集群，当网络好的时候，就会出现数据错误或者丢失数据的情况。当discovery.zen.minimum_master_nodes设置为2的时候，一个网络中有两个主资格节点，可以继续工作，另一部分，由于只有一个master节点，则不会形成一个独立的集群，这个时候当网络恢复的时候，节点又会从新加入集群。 shard (分片)每个索引包含多个shard，默认是5个，Shard是一个最小的Lucene索引单元。 分片，ES是分布式搜索引擎，每个索引有一个或多个分片，索引的数据被分配到各个分片上，相当于一桶水用了N个杯子装。 分片有助于横向扩展，N个分片会被尽可能平均地（rebalance）分配在不同的节点上（例如你有2个节点，4个主分片，那么每个节点会分到2个分片，后来你增加了2个节点，那么你这4个节点上都会有1个分片，这个过程叫relocation，ES感知后自动完成)。 分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request。 每个分片都是一个Lucene Index，所以一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519 个docs。 当写入一个document的时候，ES通过对docid进行hash来确定将其放在哪个shard上面，然后在shard上面进行索引存储(可以通过指定routing字段来固定hash值，从而将document固定到某个分片上)。 replicas (副本)replicas就是备份，ES采用的是Push Replication模式，当你往master主分片上面索引一个文档，该分片会复制该文档到分片副本中，这些分片也会索引这个文档。默认情况下一个索引创建5个分片一个备份，即 5primary + 5replica = 10个分片 主分片和备分片不会出现在同一个节点上（防止单点故障）。 如果只有一个节点，那么5个replica都无法分配（unassigned），此时cluster status会变成Yellow。 primary分片丢失，replica分片就会被顶上去成为新的主分片，同时根据这个新的主分片创建新的replica，集群数据安然无恙。 索引写请求只能发生在主分片上，replica不能执行索引写请求（可以执行读请求）。 对于一个索引，除非重建索引（reindex，需先close索引）否则不能调整分片的数目（主分片数, number_of_shards），但可以随时调整replica数(number_of_replicas)。 写数据主要涉及ES为何能够做到实时查询的相关原理。 flush/refresh决定一个document在哪个shard上，最重要的一个值就是routing值，默认是_id，也可以手动指定。routing也可以在发送请求的时候，手动指定一个routing value，比如说：put /index/type/id?routing=user_id 当向elasticsearch发送创建document索引请求的时候，document数据会先进入到index buffer之后，与此同时会将操作记录在translog之中，当发生 refresh 时（数据从index buffer中进入filesystem cache的过程，时间很短，默认是1s一次，写入cache的数据其实是按照多个index segment文件的形势组织的）translog中的操作记录并不会被清除，而是当数据从filesystem cache中被写入磁盘之后才会将translog中清空。而从filesystem cache写入磁盘的过程就是 flush，该过程会写一个commit point，指明这次flush包含了哪些index segment。 translog translog的功能 保证在filesystem cache中的数据不会因为elasticsearch重启或是发生意外故障的时候丢失。 当系统重启时会从translog中恢复之前记录的操作。 当对elasticsearch进行CRUD操作的时候，会先到translog之中进行查找，因为tranlog之中保存的是最新的数据。 translog的清除时间到期时进行flush操作之后（将数据从filesystem cache刷入disk之中）。 flush操作的时候，会先保存buffer的数据到filesystem cache，然后将cache的数据refresh到disk，再清除translog的数据。 flush操作的时间点 es的各个shard会每个30分钟进行一次flush操作。 当translog的数据达到某个上限的时候会进行一次flush操作。 [Note]:但是由于translog本身cache的缘故，如果此时关机，可能导致5秒钟的数据丢失。 translog和flush的一些配置项 index.translog.flush_threshold_ops:当发生多少次操作时进行一次flush。默认是 unlimited。 index.translog.flush_threshold_size:当translog的大小达到此值时会进行一次flush操作。默认是512mb。 index.translog.flush_threshold_period:在指定的时间间隔内如果没有进行flush操作，会进行一次强制flush操作。默认是30m。 index.translog.interval:多少时间间隔内会检查一次translog，来进行一次flush操作。es会随机的在这个值到这个值的2倍大小之间进行一次操作，默认是5s。 删除、更新数据如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 index segment file（在OS cache中），所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多。 ES会定期执行 merge（也就是translog flush时候的commit point），每次 merge 的时候，会将多个 segment file 合并成一个。同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘。这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。（该流程类似于etcd的compact和Hbase的region merge） reindex可以实现将远端数据index到本地，也可以将本地的index做修改字段等操作。 123456789101112131415161718192021222324POST _reindex&#123; \"source\": &#123; \"remote\": &#123; \"host\": \"http://otherhost:9200\", // 远程es的ip和port列表 \"socket_timeout\": \"1m\", \"connect_timeout\": \"10s\" // 超时时间设置 &#125;, \"index\": \"my_index_name\", // 源索引名称 \"query\": &#123; // 满足条件的数据 \"match\": &#123; \"test\": \"data\" &#125; &#125; &#125;, \"dest\": &#123; \"index\": \"dest_index_name\" // 目标索引名称 &#125;, \"script\": &#123; // 修改信息 \"source\": \"if (ctx._source.foo == 'bar') &#123;ctx._version++; ctx._source.remove('foo')&#125;\", \"lang\": \"painless\" &#125;&#125; 查询当进行查询时： 如果提供了查询的DocID，ES通过hash就知道Doc存在哪个shard上面，然后再基于随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡。 如果不提供DocID（比如搜索）, 请求先发到协调节点，协调节点会在该Index（indics）的shard所在的所有primary或replicas上执行搜索；每个shard将搜索结果（其实就是一些doc id）返回给协调节点。协调节点会对结果进行合并、排序、分页等操作，生成结果。然后根据结果中的doc id去各个node上拉取文档数据，然后返回给客户端。 查询方式各类查询 match all 123456GET /_search&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;&#125; match 12345678GET /_search&#123; \"query\": &#123; \"match\": &#123; \"title\": \"my elasticsearch article\" &#125; &#125;&#125; multi match 123456789GET /test_index/test_type/_search&#123; \"query\": &#123; \"multi_match\": &#123; \"query\": \"test\", \"fields\": [\"test_field\", \"test_field1\"] &#125; &#125;&#125; range query 12345678910GET /company/employee/_search &#123; \"query\": &#123; \"range\": &#123; \"age\": &#123; \"gte\": 30 &#125; &#125; &#125;&#125; term query 12345678GET /test_index/test_type/_search &#123; \"query\": &#123; \"term\": &#123; \"test_field\": \"test hello\" &#125; &#125;&#125; terms query 12345678GET /_search&#123; \"query\": &#123; \"terms\": &#123; \"tag\": [ \"search\", \"full_text\", \"nosql\" ] &#125; &#125;&#125; 条件组合query 12345678910111213141516171819202122232425262728GET /website/article/_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"match\": &#123; \"title\": \"elasticsearch\" &#125; &#125; ], \"should\": [ &#123; \"match\": &#123; \"content\": \"elasticsearch\" &#125; &#125; ], \"must_not\": [ &#123; \"match\": &#123; \"author_id\": 111 &#125; &#125; ] &#125; &#125;&#125; scroll query 使用scoll滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来 scroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的 采用基于_doc进行排序的方式，性能较高 每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了 12345678910111213141516GET /test_index/test_type/_search?scroll=1m&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;, \"sort\": [ \"_doc\" ], \"size\": 3&#125;# 当scroll返回ID之后，第二次使用该ID查询；GET /_search/scroll&#123; \"scroll\": \"1m\", \"scroll_id\" : \"DnF1ZXJ5VGhlbkZldGNoBQAAAAAAACxeFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYBY0b25zVFlWWlRqR3ZJajlfc3BXejJ3AAAAAAAALF8WNG9uc1RZVlpUakd2SWo5X3NwV3oydwAAAAAAACxhFjRvbnNUWVZaVGpHdklqOV9zcFd6MncAAAAAAAAsYhY0b25zVFlWWlRqR3ZJajlfc3BXejJ3\"&#125; 排序 123456789101112131415161718192021GET /company/employee/_search &#123; \"query\": &#123; \"constant_score\": &#123; \"filter\": &#123; \"range\": &#123; \"age\": &#123; \"gte\": 30 &#125; &#125; &#125; &#125; &#125;, \"sort\": [ &#123; \"join_date\": &#123; \"order\": \"asc\" &#125; &#125; ]&#125; filter/query区别 filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响 query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序 一般来说，如果是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter。 123456789101112131415161718192021GET /company/employee/_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"match\": &#123; \"join_date\": \"2019-01-01\" &#125; &#125; ], \"filter\": &#123; \"range\": &#123; \"age\": &#123; \"gte\": 30 &#125; &#125; &#125; &#125; &#125;&#125; multi-index/multi-type搜索 /_search：所有索引，所有type下的所有数据都搜索出来 /index1/_search：指定一个index，搜索其下所有type的数据 /index1,index2/_search：同时搜索两个index下的数据 /1,2/_search：按照通配符去匹配多个索引 /index1/type1/_search：搜索一个index下指定的type的数据 /index1/type1,type2/_search：可以搜索一个index下多个type的数据 /index1,index2/type1,type2/_search：搜索多个index下的多个type的数据 /_all/type1,type2/_search：_all，可以代表搜索所有index下的指定type的数据 mappingmapping，就是index的type的元数据，每个type都有一个自己的mapping，决定了数据类型，建立倒排索引的行为，还有进行搜索的行为。 ES默认是动态创建索引和索引类型的mapping的。这就相当于无需定义Schema，无需指定各个字段的索引规则就可以索引文件，很方便。但有时方便就代表着不灵活。比如，ES默认一个字段是要做分词的，但我们有时要搜索匹配整个字段却不行。如有统计工作要记录每个城市出现的次数。对于NAME字段，若记录“new york”文本，ES可能会把它拆分成“new”和“york”这两个词，分别计算这个两个单词的次数，而不是我们期望的“new york”。 这时，就需要我们在创建索引时定义mapping。此外，es支持多字段结构，例如：我们希望两个字段中有相同的值，一个用于搜索，一个用户排序；或者一个用于分词器分析，一个用于空白字符。例如：编写mapping文件如下： 12345678910111213141516171819202122&#123; \"index_type\":&#123; \"properties\":&#123; \"ID\":&#123; \"type\":\"string\", \"index\":\"not_analyzed\" &#125;, \"NAME\":&#123; \"type\":\"string\", \"fields\":&#123; \"NAME\":&#123; \"type\":\"string\" &#125;, \"raw\":&#123; \"type\":\"string\", \"index\":\"not_analyzed\" &#125; &#125; &#125; &#125; &#125; &#125; 就是说我们对于index_type这个索引类型，定义了它的mapping。重点是将NAME这个字段映射为两个，一个是需要做索引分析的NAME，另一个是不分析的raw，即不会拆分new york这种词组。这样我们在做搜索的时候，就可以对NAME.raw这个字段做term aggregation，获得所有城市出现的次数了。 核心的数据类型 string byte，short，integer，long float，double boolean date dynamic mapping true or false –&gt; boolean 123 –&gt; long 123.45 –&gt; double 2017-01-01 –&gt; date “hello world” –&gt; string/text 查看mapping 1GET /index/_mapping/type 总结 往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping mapping中就自动定义了每个field的数据类型 不同的数据类型（比如说text和date），可能有的是exact value，有的是full text exact value，在建立倒排索引的时候，分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full text，会经历各种各样的处理，分词，normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中 同时呢，exact value和full text类型的field就决定了，在一个搜索过来的时候，对exact value field或者是full text field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text query string，也会进行分词和normalization再去倒排索引中去搜索 可以用es的dynamic mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为，包括分词器，等等 phrase首先说，wildcard/regexp/prefix这三个的性能都不好！wildcard和regexp查询的工作方式和prefix查询完全一样。它们也需要遍历倒排索引中的词条列表来找到所有的匹配词条，然后逐个词条地收集对应的文档ID。它们和prefix查询的唯一区别在于它们能够支持更加复杂的模式。 prefix，wildcard以及regexp查询基于词条进行操作。如果你在一个analyzed字段上使用了它们，它们会检查字段中的每个词条，而不是整个字段。比如，假设我们的title字段中含有”Quick brown fox”，它会产生词条quick，brown和fox。这个查询能够匹配：1&#123; \"regexp\": &#123; \"title\": \"br.*\" &#125;&#125; 而不会匹配：1&#123; \"regexp\": &#123; \"title\": \"quick br*\" &#125;&#125; 如果需要匹配字符串”quick brown fox”，可以采用phrase。phrase在ES中叫邻近匹配, slop值来用指定查询项之间可以分隔多远的距离。 123456789101112POST /bookdb_index/book/_search&#123; \"query\": &#123; \"multi_match\" : &#123; \"query\": \"quick brown fox\", \"fields\": [\"title\", \"summary\"], \"type\": \"phrase\", \"slop\": 1 &#125; &#125;, \"_source\": [ \"title\", \"summary\", \"publish_date\" ]&#125; 前面的查询的数据必须满足下面3个条件： quick, brown, fox 三个词必须同时出现 brown 的位置必须比quick大1 fox 的位置必须比quick大2 有关ES查询相关的方法，可以参考这篇文档, 总结的比较好！ bulk apibulk的json数据结构如下：1234&#123;\"action\": &#123;\"meta\"&#125;&#125;\\n&#123;\"data\"&#125;\\n&#123;\"action\": &#123;\"meta\"&#125;&#125;\\n&#123;\"data\"&#125;\\n bulk api的实例如下：12345678POST /_bulk&#123; \"delete\": &#123; \"_index\": \"test_index\", \"_type\": \"test_type\", \"_id\": \"3\" &#125;&#125; &#123; \"create\": &#123; \"_index\": \"test_index\", \"_type\": \"test_type\", \"_id\": \"12\" &#125;&#125;&#123; \"test_field\": \"test12\" &#125;&#123; \"index\": &#123; \"_index\": \"test_index\", \"_type\": \"test_type\", \"_id\": \"2\" &#125;&#125;&#123; \"test_field\": \"replaced test2\" &#125;&#123; \"update\": &#123; \"_index\": \"test_index\", \"_type\": \"test_type\", \"_id\": \"1\", \"_retry_on_conflict\" : 3&#125; &#125;&#123; \"doc\" : &#123;\"test_field2\" : \"bulk test1\"&#125; &#125; 一个bulk api中可以带多种操作，如果采用数组的方式，需要java做json数组到数据结构的解析，会消耗掉大量的内存。使用上面的结构后，有以下好处。 不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json 对每两个一组的json，读取meta，进行document路由 直接将对应的json发送到node上去 Logstashpipeline 可以运行多个pipeline，这个需要指定一个配置文件pipelines.yml，在这个文件中指定每一个pipeline所使用的具体的input，filter，ouput的配置文件； 一般情况下，一个logstash只运行一个pipeline，可以指定pipeline的worker数为cpu cores数，但是这样容易出现对多个input的相互阻塞的问题； 多个pipeline的分别拥有隔离的Persistent queues 和 dead letter queues，但是需要小心设置其workers，因为会出现资源竞争的问题； 高可用场景 直接使用beats对接到logstash，这样logstash只需要扩展节点，因为beats可以做客户端负载均衡； 使用kafka，然后将多个logstash节点配置为consumer去消费同一个topic的group，这样kafka可做负载均衡； 对于udp、tcp之类的力量，需要在logstash nodes前添加负载均衡设备 PluginsLogstash有按照条件发送邮件的output plugins，按照这个的逻辑，可以实现一个告警的output plugins，在被匹配的规则满足条件后，调用webhook或者是直接发送邮件，从而实现日志告警功能。","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://ljchen.net/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"http://ljchen.net/tags/logstash/"}]},{"title":"Istio的sidecar注入","slug":"istio的sidecar注入","date":"2019-04-04T14:53:40.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2019/04/04/istio的sidecar注入/","link":"","permalink":"http://ljchen.net/2019/04/04/istio的sidecar注入/","excerpt":"之前那篇文章太长，直接导致网站的search模块废掉，现单独将sidecar的注入这块内容拉出来讲，也方面后面再更新更多内容。","text":"之前那篇文章太长，直接导致网站的search模块废掉，现单独将sidecar的注入这块内容拉出来讲，也方面后面再更新更多内容。 K8S的MutatingAdmissionWebhook支持两种类型:validate和mutate对应ValidatingAdmissionWebhook和MutatingAdmissionWebhook，其中validate用来配置验证，mutate用来修改对象，这里我们使用mutate来注入。 MutatingWebhookConfiguration1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master ~]# kc get mutatingwebhookconfigurationNAME CREATED ATistio-sidecar-injector 2019-03-31T13:55:50Z[root@k8s-master ~]# kc get mutatingwebhookconfiguration -o yamlapiVersion: v1items:- apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration metadata: ... labels: app: istio-sidecar-injector chart: sidecarInjectorWebhook-1.0.4 heritage: Tiller release: istio name: istio-sidecar-injector ... webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: ... service: name: istio-sidecar-injector namespace: istio-system path: /inject failurePolicy: Fail name: sidecar-injector.istio.io namespaceSelector: matchLabels: istio-injection: enabled rules: - apiGroups: - \"\" apiVersions: - v1 operations: - CREATE resources: - pods scope: '*' sideEffects: Unknown timeoutSeconds: 30kind: Listmetadata: resourceVersion: \"\" selfLink: \"\" 当webhook中配置了namespaceSelector，该namespace的对象被调用的时候，会去clientConfig中定义的服务去请求webhook（也就是在这个时候注入的）。在service中可以看到对应的服务，基于此，我们可以查询到对应的pod。 1234567[root@k8s-master ~]# kci get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-sidecar-injector ClusterIP 10.110.47.206 &lt;none&gt; 443/TCP 8d [root@k8s-master ~]# kci get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESistio-sidecar-injector-8477498cbf-mgjjb 1/1 Running 1 8d 10.244.0.132 k8s-master &lt;none&gt; &lt;none&gt; 配置文件至于具体注入了些什么内容，需要来查看配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master ~]# kci describe deploy istio-sidecar-injectorName: istio-sidecar-injectorNamespace: istio-systemCreationTimestamp: Sun, 31 Mar 2019 21:55:50 +0800Labels: app=sidecarInjectorWebhook chart=sidecarInjectorWebhook-1.0.4 heritage=Tiller istio=sidecar-injector release=istio...Pod Template: ... Containers: sidecar-injector-webhook: Image: docker.io/istio/sidecar_injector:1.0.4 Port: &lt;none&gt; Host Port: &lt;none&gt; Args: --caCertFile=/etc/istio/certs/root-cert.pem --tlsCertFile=/etc/istio/certs/cert-chain.pem --tlsKeyFile=/etc/istio/certs/key.pem --injectConfig=/etc/istio/inject/config --meshConfig=/etc/istio/config/mesh --healthCheckInterval=2s --healthCheckFile=/health ... Mounts: /etc/istio/certs from certs (ro) /etc/istio/config from config-volume (ro) /etc/istio/inject from inject-config (ro) Volumes: config-volume: Type: ConfigMap (a volume populated by a ConfigMap) Name: istio Optional: false certs: Type: Secret (a volume populated by a Secret) SecretName: istio.istio-sidecar-injector-service-account Optional: false inject-config: Type: ConfigMap (a volume populated by a ConfigMap) Name: istio-sidecar-injector Optional: false ... 这里使用了几个配置文件，都是从configmap中获取到的，分别来看看。 /etc/istio/config12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576[root@k8s-master ~]# kci get cm istio -o=go-template='&#123;&#123; .data.mesh&#125;&#125;' # Set the following variable to true to disable policy checks by the Mixer.# Note that metrics will still be reported to the Mixer.disablePolicyChecks: false# Set enableTracing to false to disable request tracing.enableTracing: true# Set accessLogFile to empty string to disable access log.accessLogFile: \"/dev/stdout\"## Deprecated: mixer is using EDSmixerCheckServer: istio-policy.istio-system.svc.cluster.local:9091mixerReportServer: istio-telemetry.istio-system.svc.cluster.local:9091# policyCheckFailOpen allows traffic in cases when the mixer policy service cannot be reached.# Default is false which means the traffic is denied when the client is unable to connect to Mixer.policyCheckFailOpen: false# Unix Domain Socket through which envoy communicates with NodeAgent SDS to get# key/cert for mTLS. Use secret-mount files instead of SDS if set to empty. sdsUdsPath: \"\"# How frequently should Envoy fetch key/cert from NodeAgent.sdsRefreshDelay: 15s#defaultConfig: # # TCP connection timeout between Envoy &amp; the application, and between Envoys. connectTimeout: 10s # ### ADVANCED SETTINGS ############# # Where should envoy's configuration be stored in the istio-proxy container configPath: \"/etc/istio/proxy\" binaryPath: \"/usr/local/bin/envoy\" # The pseudo service name used for Envoy. serviceCluster: istio-proxy # These settings that determine how long an old Envoy # process should be kept alive after an occasional reload. drainDuration: 45s parentShutdownDuration: 1m0s # # The mode used to redirect inbound connections to Envoy. This setting # has no effect on outbound traffic: iptables REDIRECT is always used for # outbound connections. # If \"REDIRECT\", use iptables REDIRECT to NAT and redirect to Envoy. # The \"REDIRECT\" mode loses source addresses during redirection. # If \"TPROXY\", use iptables TPROXY to redirect to Envoy. # The \"TPROXY\" mode preserves both the source and destination IP # addresses and ports, so that they can be used for advanced filtering # and manipulation. # The \"TPROXY\" mode also configures the sidecar to run with the # CAP_NET_ADMIN capability, which is required to use TPROXY. #interceptionMode: REDIRECT # # Port where Envoy listens (on local host) for admin commands # You can exec into the istio-proxy container in a pod and # curl the admin port (curl http://localhost:15000/) to obtain # diagnostic information from Envoy. See # https://lyft.github.io/envoy/docs/operations/admin.html # for more details proxyAdminPort: 15000 # # Set concurrency to a specific number to control the number of Proxy worker threads. # If set to 0 (default), then start worker thread for each CPU thread/core. concurrency: 0 # # Zipkin trace collector zipkinAddress: zipkin.istio-system:9411 # # Mutual TLS authentication between sidecars and istio control plane. controlPlaneAuthPolicy: NONE # # Address where istio Pilot service is running discoveryAddress: istio-pilot.istio-system:15007 /etc/istio/inject123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154[root@k8s-master ~]# kci get cm istio-sidecar-injector -o=go-template=&#123;&#123;.data.config&#125;&#125;policy: enabledtemplate: |- initContainers: - name: istio-init image: \"docker.io/istio/proxy_init:1.0.4\" args: - \"-p\" - [[ .MeshConfig.ProxyListenPort ]] - \"-u\" - 1337 - \"-m\" - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]] - \"-i\" - \"[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` \"*\" ]]\" - \"-x\" - \"[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` \"\" ]]\" - \"-b\" - \"[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]\" - \"-d\" - \"[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` \"\" ) ]]\" imagePullPolicy: IfNotPresent securityContext: capabilities: add: - NET_ADMIN privileged: true restartPolicy: Always containers: - name: istio-proxy image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage` \"docker.io/istio/proxyv2:1.0.4\" ]] ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom args: - proxy - sidecar - --configPath - [[ .ProxyConfig.ConfigPath ]] - --binaryPath - [[ .ProxyConfig.BinaryPath ]] - --serviceCluster [[ if ne \"\" (index .ObjectMeta.Labels \"app\") -]] - [[ index .ObjectMeta.Labels \"app\" ]] [[ else -]] - \"istio-proxy\" [[ end -]] - --drainDuration - [[ formatDuration .ProxyConfig.DrainDuration ]] - --parentShutdownDuration - [[ formatDuration .ProxyConfig.ParentShutdownDuration ]] - --discoveryAddress - [[ annotation .ObjectMeta `sidecar.istio.io/discoveryAddress` .ProxyConfig.DiscoveryAddress ]] - --discoveryRefreshDelay - [[ formatDuration .ProxyConfig.DiscoveryRefreshDelay ]] - --zipkinAddress - [[ .ProxyConfig.ZipkinAddress ]] - --connectTimeout - [[ formatDuration .ProxyConfig.ConnectTimeout ]] - --proxyAdminPort - [[ .ProxyConfig.ProxyAdminPort ]] [[ if gt .ProxyConfig.Concurrency 0 -]] - --concurrency - [[ .ProxyConfig.Concurrency ]] [[ end -]] - --controlPlaneAuthPolicy - [[ annotation .ObjectMeta `sidecar.istio.io/controlPlaneAuthPolicy` .ProxyConfig.ControlPlaneAuthPolicy ]] [[- if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \"0\") ]] - --statusPort - [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] - --applicationPorts - \"[[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/applicationPorts` (applicationPorts .Spec.Containers) ]]\" [[- end ]] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: [[ or (index .ObjectMeta.Annotations \"sidecar.istio.io/interceptionMode\") .ProxyConfig.InterceptionMode.String ]] [[ if .ObjectMeta.Annotations ]] - name: ISTIO_METAJSON_ANNOTATIONS value: | [[ toJson .ObjectMeta.Annotations ]] [[ end ]] [[ if .ObjectMeta.Labels ]] - name: ISTIO_METAJSON_LABELS value: | [[ toJson .ObjectMeta.Labels ]] [[ end ]] imagePullPolicy: IfNotPresent [[ if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \"0\") ]] readinessProbe: httpGet: path: /healthz/ready port: [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] initialDelaySeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/initialDelaySeconds` 1 ]] periodSeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/periodSeconds` 2 ]] failureThreshold: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/failureThreshold` 30 ]] [[ end -]]securityContext: readOnlyRootFilesystem: true [[ if eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) \"TPROXY\" -]] capabilities: add: - NET_ADMIN runAsGroup: 1337 [[ else -]] runAsUser: 1337 [[ end -]] restartPolicy: Always resources: [[ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) -]] requests: cpu: \"[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU` ]]\" memory: \"[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory` ]]\" [[ else -]] requests: cpu: 10m [[ end -]] volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true [[ if eq .Spec.ServiceAccountName \"\" -]] secretName: istio.default [[ else -]] secretName: [[ printf \"istio.%s\" .Spec.ServiceAccountName ]] [[ end -]]","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"istio","slug":"istio","permalink":"http://ljchen.net/tags/istio/"}]},{"title":"Istio实践","slug":"istio实践","date":"2019-04-03T15:53:40.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2019/04/03/istio实践/","link":"","permalink":"http://ljchen.net/2019/04/03/istio实践/","excerpt":"本文主要涉及Istio学习过程中使用过的一些yaml文件的整理，便于后续作为模板快速拷贝^_^ 组件调试在Istio使用中，遇到被悲催的事莫过于：”配置下发了，但是系统中流量规则始终不生效”（这个直接影响到业务，所以真心要在生产环境中慎用istio）。此时，需要对istio中各组件的工作原理以及其debug方式有一定的了解，下面是一些简单的信息收集方式。","text":"本文主要涉及Istio学习过程中使用过的一些yaml文件的整理，便于后续作为模板快速拷贝^_^ 组件调试在Istio使用中，遇到被悲催的事莫过于：”配置下发了，但是系统中流量规则始终不生效”（这个直接影响到业务，所以真心要在生产环境中慎用istio）。此时，需要对istio中各组件的工作原理以及其debug方式有一定的了解，下面是一些简单的信息收集方式。 Pilot-DiscoveryPilot启动以后，监听端口 15010(gRPC)和 8080(HTTP)。当应用的Sidecar（Envoy，Istio-Proxy）启动以后，它将会连接 pilot.istio-system:15010，获取初始配置，并保持长连接。 调试方式为： 1234567891011PILOT=istio-pilot.istio-system:9093# What is sent to envoy# Listeners and routescurl $PILOT/debug/adsz# Endpointscurl $PILOT/debug/edsz# Clusterscurl $PILOT/debug/cdsz 这里是代码中对应可以用于调试的URL信息： EnvoyEnvoy作为客户端，是通过静态配置一个对应的xds server来通过gRPC与server建立长连接并主动获取配置的。对于XDS server这块，有java和golang的control-plane项目，可以实现配置的集中管理。Envoy启动之后，监听 15000 来作为本地的admin端口， 使用15001作为本地的业务端口。 调试的时候，可以通过以下命令来在istio-proxy中获取到动态配置的内容： 1curl http://127.0.0.1:15000/config_dump &gt; config_dump network.istio.io主要涉及virtualService，destinationRule，gateway以及serviceEntry这几个概念的理解。 VirtualServices按权重比例路由需要结合destinationRule先指定subsets。 1234567891011121314151617apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local http: - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 weight: 70 - destination: host: flaskapp.default.svc.cluster.local subset: v2 weight: 30 按HTTP header匹配http报文header带lab:canary的走v2, 其他走v1 1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local http: - match: - headers: lab: exact: canary route: - destination: host: flaskapp.default.svc.cluster.local subset: v2 - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 按labels路由基于source service的labels走v2, 其他走v1 1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local http: - match: - sourceLabels: app: sleep version: v1 route: - destination: host: flaskapp.default.svc.cluster.local subset: v2 - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 URI重定向基于URI做重定向，将”/env/HOSTNAME”替换为对”/env/version”的访问 1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local http: - match: - sourceLabels: app: sleep version: v1 uri: exact: \"/env/HOSTNAME\" redirect: uri: /env/version - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 URI重写基于uri做rewrite, 和redirect不同的是：必须包含route，且不能够和redirect共存 12345678910111213141516171819apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local http: - match: - uri: exact: \"/get\" rewrite: uri: /post route: - destination: host: flaskapp.default.svc.cluster.local - route: - destination: host: flaskapp.default.svc.cluster.local 超时123456789101112apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin.default.svc.cluster.local http: - timeout: 3s route: - destination: host: httpbin.default.svc.cluster.local 重试123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin.default.svc.cluster.local http: - route: - destination: host: httpbin.default.svc.cluster.local retries: attempts: 3 perTryTimeout: 1s timeout: 7s 故障注入123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin.default.svc.cluster.local http: - route: - destination: host: httpbin.default.svc.cluster.local fault: delay: fixDelay: 3s percent: 100 中断注入123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin.default.svc.cluster.local http: - route: - destination: host: httpbin.default.svc.cluster.local fault: abort: httpStaus: 500 percent: 100 mirror将流量路由到v1，同时，mirror一份到v2。 123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin.default.svc.cluster.local http: - route: - destination: host: httpbin.default.svc.cluster.local subsets: v1 mirror: host: httpbin.default.svc.cluster.local subsets: v2 DestinationRules设置subsets12345678910111213apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: flaskappspec: host: flaskapp subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 熔断设置123456789101112131415161718apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: httpbinspec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: httplMaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: consecutiveErrors: 1 interval: 1s baseEjectionTime: 3m maxEjectionPercent: 100 Gateway网关例子只会接受请求，但不知道路由到哪里，需要配合virtualService来设置。 123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: example-gatewayspec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*.microservice.rocks\" - \"*.microservece.xyz\" 指定路由virtualService配合gateway路由流量到后端服务。 12345678910111213141516apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local - flaskapp.microservice.rocks gateways: - mesh - example-gateway http: - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 证书支持 创建secret文件 1kubectl create -n istio-system secret tls istio-ingressgateway-certs --key rocks/key.pem --cert rocks/cert.pem 将secret挂载到/etc/istio/ingressgateway-ca-certs目录 1234567891011121314151617181920212223242526apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: example-gatewayspec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*.microservice.rocks\" - \"*.microservece.xyz\" - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE serviceCertificate: /etc/istio/ingressgateway-ca-certs/tls.crt privateKey: /etc/istio/ingressgateway-ca-certs/tls.key hosts: - \"flask.microservice.rocks\" - \"flask.microservece.xyz\" virtualService match网关1234567891011121314151617181920212223apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: flaskappspec: hosts: - flaskapp.default.svc.cluster.local - flaskapp.microservice.rocks gateways: - mesh - example-gateway http: - match: - gateways: - example-gateway route: - destination: host: flaskapp.default.svc.cluster.local subset: v2 - route: - destination: host: flaskapp.default.svc.cluster.local subset: v1 serviceEntry创建条目通过hosts来匹配路由，也需要通过virtualService配合。 123456789101112apiVersion: networking.istio.io/v1alpha3kind: ServiceEntrymetadata: name: httpbin-extspec: hosts: - httpbin.org ports: - number : 80 name: http protocol: HTTP resolution: DNS 匹配并设置超时123456789101112apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbin-servicespec: hosts: - httpbin.org http: - timeout: 3s route: - destination: host: httpbin.org config.istio.ioMixer Instance: 声明一个模板, 用模板将传给 Mixer 的数据转换为适合特定适配器的输出格式 Handler: 声明一个适配器的配置 Rule: 将Instance和Handler连接起来，确认处理关系 handler列表 名称 作用 Denier 根据自定义条件判断是否拒绝服务 Fluentd 向Fluentd服务提交日志 List 用于执行白名单或者黑名单检查 MemQuota 以内存为存储后端，提供简易的配额控制功能 Prometheus 为Prometheus提供Istio的监控指标 RedisQuota 基于Redis存储后端，提供配额管理功能 StatsD 向StatsD发送监控指标 Stdio 用于在本地输出日志和指标 denier handler 12345678apiVersion: \"config.istio.io/v1alpha2\" kind: deniermetadata: name: code-7 spec : status: code: 7 message: Not allowed instance 12345apiVersion: \"config.istio.io/v1alpha2\" kind: checknothingmetadata: name: palce-holderspec: rule 123456789apiVersion: \"config.istio.io/v1alpha2\" kind: rulemetadata: name: deny-sleep-v1-to-httpbin spec: match: destination.labels[\"app\"] == \"httpbin\" &amp;&amp; source.labels[\"app\"] == \"sleep\" &amp;&amp; source.labels[\"version\"] == \"v1\" actions: - handler: code-7.denier instances: [place-holder.checknothing] listchecker handler 1234567apiVersion: config.istio.io/v1alpha2 kind: listcheckermetadata: name: chaosspec: overrides: [\"v1\",\"v3\"] blacklist: true instance 123456apiVersion: config.istio.io/v1alpha2 kind: listentrymetadata: name: version spec: value: source.labels[\"version\"] rule 12345678910apiVersion: \"config.istio.io/v1alpha2\" kind: rulemetadata: name: checkversionspec: match: destination.labels[\"app\"]==\"httpbin\" actions: - handler: chaos.listchecker instances: - version.listentry prometheus instance 12345678[root@k8s istio]# kci get metricsNAME AGErequestcount 3d1hrequestduration 3d1hrequestsize 3d1hresponsesize 3d1htcpbytereceived 3d1htcpbytesent 3d1h handler 123[root@k8s istio]# kci get prometheusNAME AGEhandler 3d1h rules 1234[root@k8s istio]# kci get rulesNAME AGEpromhttp 3d1hpromtcp 3d1h log(stdio)满足条件的访问日志会被记录到mixer的stdout上面，通过kubectl logs查看。 1234[root@k8s istio]# kci get logentry NAME AGEaccesslog 3d1htcpaccesslog 3d1h 123[root@k8s istio]# kci get stdioNAME AGEhandler 3d1h 123456789101112131415161718192021222324[root@k8s istio]# kci get rulesNAME AGEstdio 3d1h[root@k8s istio]# kci get rule stdio -o yamlapiVersion: config.istio.io/v1alpha2kind: rulemetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;\"apiVersion\":\"config.istio.io/v1alpha2\",\"kind\":\"rule\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"name\":\"stdio\",\"namespace\":\"istio-system\"&#125;,\"spec\":&#123;\"actions\":[&#123;\"handler\":\"handler.stdio\",\"instances\":[\"accesslog.logentry\"]&#125;],\"match\":\"context.protocol == \\\"http\\\" || context.protocol == \\\"grpc\\\"\"&#125;&#125; creationTimestamp: \"2019-03-31T13:55:50Z\" generation: 1 name: stdio namespace: istio-system resourceVersion: \"16200\" selfLink: /apis/config.istio.io/v1alpha2/namespaces/istio-system/rules/stdio uid: b1935286-53bc-11e9-ad8b-525400ff729aspec: actions: - handler: handler.stdio instances: - accesslog.logentry match: context.protocol == \"http\" || context.protocol == \"grpc\" log(fluentd)需要先在k8s上部署fluentd，并暴露service为fluentd-listener:24224 handler 123456apiVersion: config.istio.io/v1alpha2kind: fluentdmetadata: name: handler spec: address: \"fluentd-listener:24224\" instance 定义logentry(sleep-log)并应用； rule 12345678910apiVersion: config.istio.io/v1alpha2 kind: rulemetadata: name: fluentdspec: actions: - handler: handler.fluentd instances: - sleep-log.logentry match: context.protocol == \"http\" &amp;&amp; source.labels[\"app\"] == \"sleep\"","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"},{"name":"service-mesh","slug":"service-mesh","permalink":"http://ljchen.net/tags/service-mesh/"},{"name":"istio","slug":"istio","permalink":"http://ljchen.net/tags/istio/"}]},{"title":"Cloud Native架构设计","slug":"Cloud-Native架构设计","date":"2019-03-30T12:56:53.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2019/03/30/Cloud-Native架构设计/","link":"","permalink":"http://ljchen.net/2019/03/30/Cloud-Native架构设计/","excerpt":"之前有在文章中介绍过云原生的十二要素，今天来介绍下云原生架构以及其架构设计中需要考虑的关键因素（后续逐步完善）。 概要还是先通过一张图简单介绍云原生架构的组成，具体见下图：","text":"之前有在文章中介绍过云原生的十二要素，今天来介绍下云原生架构以及其架构设计中需要考虑的关键因素（后续逐步完善）。 概要还是先通过一张图简单介绍云原生架构的组成，具体见下图： 敏捷基础设施 基础设施及代码（公有云、私有云） 公共基础服务 中间件服务 监控、告警反馈 微服务架构 Spring Cloud Dubbo DevOps 持续交付流水线 自动化工具链 团队文化 康威定律 工程师文化 敏捷迭代 架构设计 高可用、高可靠 可扩展性 性能 一致性 接下来重点讲解其架构设计中的四个要素。 架构设计主要包括：高可用、可扩展性、性能、一致性这四个方面。 可用性设计可能影响系统可用性的因素主要有三方面：发布、故障和压力。下面就分析基于这三方面的应对方案。 发布 影子测试 除了使用log或者tcpcopy来复制影子环境以外，如果系统使用了MQ，可以直接为MQ添加消费者，将流量复制到影子系统； 蓝绿发布 蓝绿部署的问题是: 需要一套独立的闲置资源； 灰度发布 过程: 内部员工 &gt; 1%的外部友好用户 &gt; 5%的友好用户 &gt; 10%的友好用户 &gt; 全网发布 故障（容错设计） 消除单点 飞行员总是要将飞机上升到执行两次失误的高度才开始做动作，也启发我们为什么负载均衡后的节点都推荐是n+2 而不是n+1； 特征开关 代码分支管理，如果是基于master开发，需要提供特征开关，从而避免相互影响的时候能够快速切换； 服务分级 在高流量期，可以考虑牺牲部分功能，通过降级服务来达到整体系统的高可用； 超时重试策略 熔断器 故障隔离 压力（流控设计） 限流算法 窗口、漏桶、令牌桶 限流策略 GW入口、微服务入口、中间件 容量预估 全链路压测 故障演练 各种monkey 扩展性设计 AFK扩展立方X: 横向扩展（数据库、磁盘压力）Y: 微服务拆分（数据库压力）Z: 分库分表 数据库扩展 数据库的三坐标X: master+slave，扩展多个slave用于读；Y: 分库（微服务拆分）；Z: SATA盘，单表十个字段，数据量达到1千万会出现瓶颈，需要分片 数据库分片方法 区间法（按照ID范围）, 容易导致数据热点; 轮转法（hash（key）mod node_number ），不容易排序，拓扑变化需要重新分配 扩容 停服扩容； 0中断扩容（先将新加节点设为slave，然后提升为master，再修改数据库中间件配置，注意：需要支持去重） 应该让单次请求在一个数据中心处理完 两地三中心、同城多活、异地多活 性能设计性能指标 响应时间 吞吐量单位时间的响应次数 负载敏感度用户增加、系统响应时间的衰减度（系统数据量、数据增长速度） 可伸缩性使吞吐量增加一倍，需要扩展的节点数（资源限制） 优化方法 通信优化序列化消息（grpc protobuf 等） 通过中间件提高吞吐量 MQ解决写的性能瓶颈 缓存解决读的性能瓶颈（cache aside， cache as sor） 一致性设计由于微服务拆分后，各个微服务使用独立数据库，可能导致数据不一致。 CAP 一致性（Consistence）分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 可用性（Availability）集群出现故障节点后，是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（Partition tolerance）实际中通信产生延时。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C点和A点之间做出选择。 BASE 基本可用（Basically Available）基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。 软状态（ Soft State）软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。 最终一致性（ Eventual Consistency）最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 Quorum机制（NWR原理）思想：一致性不取决于写时进行全同步，只要保障读的时候能够读到最新的数据就解决问题了。只需要满足 R + W &gt; N， 其中N为副本数, R和W分别为读写的节点数。 实现强一致性 两阶段提交的问题 可能第二阶段出异常（某个参与者网断了），无法处理； 协调者的可用性无法保障，可能导致第一阶段后资源被锁死； 两个阶段之间需要锁定资源，可能导致阻塞 三阶段提交 流程和思想 将两阶段的第一阶段分为两个阶段，先请求，得到同意字后再锁定资源； 另外引入超时机制； 问题 可能第三阶段出异常（某个参与者网断了），无法处理； 如何实现最终一致性 重试 记录操作状态（对失败的操作做重试） 悲观锁与乐观锁分布式锁 zookeeper Redis基于setnx问题在于Redis是异步复制的，如何保障主节点挂了之后，不影响分布式锁？ 幂等性 幂等令牌（redis）用token作为key，value-1代表正在处理，value-2代表处理成功；","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"}]},{"title":"分布式定时任务","slug":"分布式定时任务","date":"2019-03-30T00:12:05.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2019/03/30/分布式定时任务/","link":"","permalink":"http://ljchen.net/2019/03/30/分布式定时任务/","excerpt":"在实现定时弹性伸缩的时候，遇到需要配置定时任务的场景，按照云原生应用架构，应该将这类状态放到中间件中。苦于TbSchedule和Elastic-Job这类的任务管理服务都无法满足需求： 语言特性方面，都是Java一派的，不支持golang； 弹性伸缩规则需要做到能够动态添加删除、也就是任务的cron expression不是固定配置死的； 于是，决定自己实现一套分布式的定时任务管理服务，定位为中间件。","text":"在实现定时弹性伸缩的时候，遇到需要配置定时任务的场景，按照云原生应用架构，应该将这类状态放到中间件中。苦于TbSchedule和Elastic-Job这类的任务管理服务都无法满足需求： 语言特性方面，都是Java一派的，不支持golang； 弹性伸缩规则需要做到能够动态添加删除、也就是任务的cron expression不是固定配置死的； 于是，决定自己实现一套分布式的定时任务管理服务，定位为中间件。 需求与设计主要从下面几个方面来考虑系统的设计： 高可用既然叫做”分布式定时任务”，就应该保障不会因为单台节点的宕机而影响整体的可用性。 高可靠既然定位为中间件，就需要担负起相应的重担，在保障高可用的前提下，能承载住业务层下放的大并发定时任务，不丢任务数据，不丢失回调消息。作为和黑系统，有定时任务必触发回调；有回调必保障可达。 可扩展在负荷变重时能够横向扩展，从而保障稳定可靠。 性能需要从以下指标来评判系统性能：任务下发响应时间、定时任务承载量（吞吐量）、定时任务回调触发时延、拓扑变动任务再均衡时间。 一致性定时任务存储的一致性、节点拓扑信息的一致性、任务调度的一致性等。 架构设计总体架构如下图所示，接下来逐个分解各模块的作用和详细设计。 作用 接口层（API layer） 外部访问的Rest接口实现，主要提供对定时任务的下发、删除、更改以及查询等操作； 另外，基于任务调度逻辑实现对已请求的代理，将对具体任务的操作代理到任务所在节点执行。 任务调度（job management） 基于底层当前节点拓扑，完成任务的调度； 当拓扑变更时，实现任务的重新分配和资源的均衡。 拓扑管理（topology management） 集群中节点的状态管理，变更后上报等操作。 数据访问层（abstract storage） 基于用于配置初始化底层物理存储； 对上层提供抽象的存储封装，并将操作转化为对实际存储驱动的调用； 底层存储支持MySQL和Raft-Log两种。 定时任务驱动（cronjob driver） 实现将当前节点定时任务下放到操作系统的集中管理。 回调逻辑（CallBack Logic） 定时任务触发后，执行响应回调逻辑 支持两种回调方式：HTTP/gRPC 当对接服务注册中心Consul，支持客户端负载均衡 回调支持重试队列，按照配置规则重试 核心模块设计拓扑管理（gossip协议）基于gossip协议，当集群中节点有不健康时，其他节点一旦发现能够快速在集群中洪泛该消息，从而让所有节点能够迅速感知到新拓扑，达到拓扑状态的一致性。当拓扑变更时，会执行回调函数，从而保障对拓扑敏感的模块能够快速响应变化。具体可以访问之前介绍过的Serf。 任务调度（一致性哈希）任务调度分两块： 基于一致性哈希结果来分配定时任务到不同的节点在节点拓扑结构稳定的情况下，哈希环均衡的将任务分配到各物理节点；如果任务被分配到当前节点，还会负责调用对应的驱动代码将任务配置到操作系统层。该层也向接口层提供对接的访问接口；当更改任务的请求到达某个节点的接口层，哈希算法基于任务ID计算出该任务是否被分配到本节点；如果不在该节点上，就会触发对应的代理逻辑，将操作请求代理到对应节点。 当节点拓扑变化时，重新均衡定时任务到各节点当拓扑变化时，节点从持久化存储中读取任务列表，并基于新的哈希环调度任务。 数据持久化（抽象存储层、mysql、raft）数据持久化层包含抽象层和具体的存储介质实现，抽象层主要为业务层提供统一的封装，让底层的存储对上层透明。对于底层存储接口，MySql是比较稳妥的方案，也是借鉴了apollo的思路。重点是raft的支持比较繁琐，当前是基于etcd-raft来实现的。 Raft-Log 该模块将raft协议封装到了抽象的node里面，对外部暴露propose和configChange的channel用于下发日志和配置变更； 该模块启动的时候需要指定集群中所有的node，以及当前node的id信息，显得过于繁琐； 向raft propose的数据，是否有被commit需要在业务逻辑中实现，这块也比较麻烦； 就当前所知，raft是不支持向key提交delete和覆盖操作的，这块仍在研究中。 任务回调（回调队列、多种模式）其实是想实现一个类似于prometheus的alertManager一样的功能，当任务触发的时候，可以支持各种类型的回调操作，当前暂时支持了HTTP和gRpc。 由于需要对接外部系统，而且外部系统很多没有做对应的服务端负载均衡，因此设计基于consul来监听target endpoints列表的客户端负载均衡。相对来讲HTTP是比较好实现的，对于gRpc就相对更加繁琐一点，这块刚好之前有分析过gRpc，就将内容更新之前的文章中了。具体见浅析gRPC 接口层当更改删除任务的时候，基于任务ID来调用任务调度的逻辑，哈希得到任务所在的节点，并代理请求到对应节点做处理；当创建任务的时候，可能此时请求中并没有带任务ID，系统会尝试为其生成一个uuid，如果该uuid哈希之后的结果不在该节点上，那就重新生成uuid，知道最终为其分配的任务ID保障任务落在该节点上。这是为了避免引入集中是的ID生成服务，提高性能而刻意设计的，这样设计的前提是认为分布式定时任务外层已经有对应的LB或者客户端已经做了负载均衡。 源码当前系统已基于golang实现，并运行良好，源码地址, 请联系我开放github权限！","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"dTimer","slug":"dTimer","permalink":"http://ljchen.net/tags/dTimer/"},{"name":"cron-job","slug":"cron-job","permalink":"http://ljchen.net/tags/cron-job/"}]},{"title":"NATS Streaming实践","slug":"NATS-Streaming实践","date":"2019-03-17T02:35:13.000Z","updated":"2020-04-06T09:24:01.468Z","comments":true,"path":"2019/03/17/NATS-Streaming实践/","link":"","permalink":"http://ljchen.net/2019/03/17/NATS-Streaming实践/","excerpt":"Rocket MQ在业务开发中用的比较多，无论是其功能还是易用性方面表现很不错。先前云平台代码中也是用RMQ作为消息中间件，但是在上生产的前夜，QA对微服务做高可用测试时发现一些连接相关的问题。连夜分析，深入追踪当时使用的一个RMQ非官方golang库，发现其代码比较乱，最终评估下来决定在放弃对RMQ的使用。","text":"Rocket MQ在业务开发中用的比较多，无论是其功能还是易用性方面表现很不错。先前云平台代码中也是用RMQ作为消息中间件，但是在上生产的前夜，QA对微服务做高可用测试时发现一些连接相关的问题。连夜分析，深入追踪当时使用的一个RMQ非官方golang库，发现其代码比较乱，最终评估下来决定在放弃对RMQ的使用。于是CNCF的NATS就进入了我们的视野，由于其本来就是基于golang实现，对于云平台来讲适配会更加友好；但是NATS缺乏消息持久化的支持，必须通过NATS Streaming才能够弥补这块的缺陷。 本文重点不是介绍NATS，而是NAT Streaming；接下来，就来讲下我在NAT Streaming使用中的一些实践。 NATS Streaming介绍官方有这样一副图，大致意思是NATS Streaming Module都一定是需要attach到NATS上，将消息内容持久化到存储中，且并不单独对外提供访问接口的。 主要功能包括 保障可达 (At-Least-Once delivery) 消息、事件的持久化 (persistence) 消费者可按需重复消费消息 (replay by subject) 生产者、消费者端均支持限速 (rate limit) 提供持久化的订阅 (Durable subscriptions） HA环境在这部分，我们先搭建一个由三个节点组成的集群，然后再简单了解一些常用的监控和调试方式。 NATS Streaming自带内嵌NATS Server，但通过分析参数列表，我们发现其貌似无法使用内嵌的NATS Server建立NAST集群。为了实现NATS Streaming的高可用，我们需要建立两层集群。 NATS Server集群无法使用NAT Streaming内嵌的NATS，必须维护一套独立的NATS集群。 NATS Streaming集群通过指定external NATS的形势来连接NATS，NATS Streaming集群只负责数据持久化，只要有一个外部NATS Server可用，就不会受到影响。 集群启动将下列粘贴到一个文件(nats-boot.sh)中，然后接下来介绍几种方法来替换变量，然后分别在三台主机上运行生成的命令。 1234567891011121314151617### node-1 命令###############docker run --restart=always -d --net=host --name=nats-node-1 nats -m 8222 -p 4222 -cluster nats://0.0.0.0:6222 -routes nats://$&#123;NODE-2-IP&#125;:6222,nats://$&#123;NODE-3-IP&#125;:6222docker run --restart=always --name=nats-streaming-node-1 -p 9222:9222 -d nats-streaming -m 9222 --stan_trace=true --stan_debug=true --debug=true --trace=true -store file -dir datastore -clustered -cluster_id $&#123;NATS-CLUSTER-ID&#125; -cluster_node_id node-1 -cluster_peers node-1,node-2,node-3 -ns nats://$&#123;NODE-1-IP&#125;:6222,nats://$&#123;NODE-2-IP&#125;:6222,nats://$&#123;NODE-3-IP&#125;:6222### node-2 命令###############docker run --restart=always -d --net=host --name=nats-node-2 nats -m 8222 -p 4222 -cluster nats://0.0.0.0:6222 -routes nats://$&#123;NODE-1-IP&#125;:6222,nats://$&#123;NODE-3-IP&#125;:6222docker run --restart=always --name=nats-streaming-node-2 -p 9222:9222 -d nats-streaming -m 9222 --stan_trace=true --stan_debug=true --debug=true --trace=true -store file -dir datastore -clustered -cluster_id $&#123;NATS-CLUSTER-ID&#125; -cluster_node_id node-2 -cluster_peers node-1,node-2,node-3 -ns nats://$&#123;NODE-1-IP&#125;:6222,nats://$&#123;NODE-2-IP&#125;:6222,nats://$&#123;NODE-3-IP&#125;:6222### node-3 命令###############docker run --restart=always -d --net=host --name=nats-node-3 nats -m 8222 -p 4222 -cluster nats://0.0.0.0:6222 -routes nats://$&#123;NODE-1-IP&#125;:6222,nats://$&#123;NODE-2-IP&#125;:6222docker run --restart=always --name=nats-streaming-node-3 -p 9222:9222 -d nats-streaming -m 9222 --stan_trace=true --stan_debug=true --debug=true --trace=true -store file -dir datastore -clustered -cluster_id $&#123;NATS-CLUSTER-ID&#125; -cluster_node_id node-3 -cluster_peers node-1,node-2,node-3 -ns nats://$&#123;NODE-1-IP&#125;:6222,nats://$&#123;NODE-2-IP&#125;:6222,nats://$&#123;NODE-3-IP&#125;:6222 通过环境变量设置 环境变量设置模板如下： 12345NATS-CLUSTER-ID=cluster-1NODE-1-IP=1.1.1.1NODE-2-IP=2.2.2.2NODE-3-IP=3.3.3.3 直接在vim中替换变量 123456vim ./nats-boot.sh:%s/NATS-CLUSTER-ID/cluster-1/g:%s/NODE-1-IP/1.1.1.1/g:%s/NODE-2-IP/2.2.2.2/g:%s/NODE-3-IP/3.3.3.3/g 命令行替换变量 1234sed -i &apos;s/NATS-CLUSTER-ID/cluster-1/g&apos; nats-boot.shsed -i &apos;s/NODE-1-IP/1.1.1.1/g&apos; nats-boot.shsed -i &apos;s/NODE-2-IP/2.2.2.2/g&apos; nats-boot.shsed -i &apos;s/NODE-3-IP/3.3.3.3/g&apos; nats-boot.sh 状态监控nats streaming实现了监控的对应接口，当然还不够完善；不过可以对接外部exporter来将监控信息收集到prometheus等。 集群状态 12345678910111213141516curl localhost:9222/streaming/serverz&#123; \"cluster_id\": \"$&#123;NATS-CLUSTER-ID&#125;\", \"server_id\": \"2DTn1aiWKHXcrTWScNMYbB\", \"version\": \"0.11.2\", \"go\": \"go1.11.1\", \"state\": \"CLUSTERED\", \"now\": \"2019-03-13T08:05:32.976885437Z\", \"start_time\": \"2019-03-13T07:54:17.961989986Z\", \"uptime\": \"11m15s\", \"clients\": 0, \"subscriptions\": 0, \"channels\": 0, \"total_msgs\": 0, \"total_bytes\": 0&#125; 存储使用情况 1234567891011121314151617curl localhost:9222/streaming/storez&#123; \"cluster_id\": \"$&#123;NATS-CLUSTER-ID&#125;\", \"server_id\": \"2DTn1aiWKHXcrTWScNMYbB\", \"now\": \"2019-03-13T08:08:49.634481176Z\", \"type\": \"RAFT_FILE\", \"limits\": &#123; \"max_channels\": 100, \"max_msgs\": 1000000, \"max_bytes\": 1024000000, \"max_age\": 0, \"max_subscriptions\": 1000, \"MaxInactivity\": 0 &#125;, \"total_msgs\": 0, \"total_bytes\": 0&#125; client列表 1234567891011curl localhost:9222/streaming/clientsz&#123; \"cluster_id\": \"$&#123;NATS-CLUSTER-ID&#125;\", \"server_id\": \"2DTn1aiWKHXcrTWScNMYbB\", \"now\": \"2019-03-13T08:10:38.048664063Z\", \"offset\": 0, \"limit\": 1024, \"count\": 0, \"total\": 0, \"clients\": []&#125; channel信息查询 12345678910curl localhost:9222/streaming/channelsz&#123; \"cluster_id\": \"$&#123;NATS-CLUSTER-ID&#125;\", \"server_id\": \"2DTn1aiWKHXcrTWScNMYbB\", \"now\": \"2019-03-13T08:11:22.338425862Z\", \"offset\": 0, \"limit\": 1024, \"count\": 0, \"total\": 0&#125; NATS Streming客户端官方文档见nats-straming client golang version Subscribe subscribe类似于广播，如果不使用用Queue Group，只要订阅了该topic的消费者均能够收取消息； durable subscription通过client-id和durable name来唯一表示一个client。当该client网络中断后，nats会将错过的消息都记录下来，等待该client-id对应的client重新起来之后再发送消息过去继续消费。 Queue Group VS. Durable Queue Group 名称 功能 生命周期 效果 Queue Group 用来将一堆消费者归类，在这个组内的消费者，只有一个会收到消息，而非广播的。 当queue group中最后一个消费者离开的时候（调用unscribe或者close），queue group会被销毁。 假设最后一个member调用close退出queue group，queue group将被销毁。而在该member（相同client-id）重新上线之前，queue group所订阅topic（channel）的消息将被丢失。 Durable Queue Group 除了queue group的功能外，可以持久化消息，直到有新的消费者加入后继续消费。 可以当queueu group中所有的消费者都离开的时候，依然存在；当最后一个消费者调用unscribe的时候销毁。 假设最后一个member调用close退出queue group，queue group会被保留。而在该member（相同client-id）重新上线之前，queue group所订阅topic（channel）的消息将被在member起来之后，发送到member重新消费。 Client Id维护client-id相同的两个member无法同时连接到nats streaming，而如果member挂掉了，新上线的member可以继续使用之前的client-id（IP地址可以不一致）可以继续从durable queue group中消费宕机时漏掉的消息。这里具体分析一下nats streaming行为。 已知使用durable queue group的last member未来得及执行unsubscribe和close操作就异常终止了；然后新的member（使用相同client-id）在另外一台主机上（IP不一致）启动，日志如下。 1234[1] 2019/03/17 10:15:14.464874 [DBG] STREAM: [Client:consumer-5] Suspended durable queue subscription, subject=nautilus-test, inbox=_INBOX.1ENluyqy0UX4IeohElmiGm, queue=mydurableName:oceanus, subid=9[1] 2019/03/17 10:15:14.464912 [DBG] STREAM: [Client:consumer-5] Closed (Inbox=_INBOX.1ENluyqy0UX4IeohElmhyO)[1] 2019/03/17 10:15:14.464925 [DBG] STREAM: [Client:consumer-5] Replaced old client (Inbox=_INBOX.Ah2vp3eh5BKTfcSVfq1Sv8)[1] 2019/03/17 10:15:14.679385 [DBG] STREAM: [Client:consumer-5] Resumed durable queue subscription, subject=nautilus-test, inbox=_INBOX.Ah2vp3eh5BKTfcSVfq1T4o, queue=mydurableName:oceanus, subid=9 先前的member异常终止，server无法感知，因此等待新的member起来之后，才去check老的member的状态。具体操作流程为：向老的member对应client-id的Inbox发送消息，并等待反馈（因为nats-streaming不直接与客户端通信），如果无法得到响应就开始执行： Suspended durable queue subscription Close旧member的connection 由于两个member都使用相同的client-id，于是改变client-id映射的Inbox，即替换client-id对应的member Resumed durable queue subscription","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"NATS","slug":"NATS","permalink":"http://ljchen.net/tags/NATS/"},{"name":"NATS-Streaming","slug":"NATS-Streaming","permalink":"http://ljchen.net/tags/NATS-Streaming/"}]},{"title":"配置中心Apollo入门","slug":"配置中心apollo入门","date":"2019-03-02T02:37:52.000Z","updated":"2020-04-06T09:24:01.503Z","comments":true,"path":"2019/03/02/配置中心apollo入门/","link":"","permalink":"http://ljchen.net/2019/03/02/配置中心apollo入门/","excerpt":"Apollo是携程开源的配置中心，社区比较火，当前在生产中使用的用户量也非常大。用户侧的功能和使用，本文就不一一介绍了，这方面的文档比较多。这里结合最近在公司环境中的大规模部署，简要做一些概要总结，重点涉及一些分布式部署中可能遇到的问题。","text":"Apollo是携程开源的配置中心，社区比较火，当前在生产中使用的用户量也非常大。用户侧的功能和使用，本文就不一一介绍了，这方面的文档比较多。这里结合最近在公司环境中的大规模部署，简要做一些概要总结，重点涉及一些分布式部署中可能遇到的问题。 软件架构 我们先来对架构有一个总体的认识，上图是apollo的模块架构，接下来我们将对图上各模块做逐一功能职责的介绍。 各模块功能 ConfigService 提供配置获取接口 提供配置推送接口 服务于Apollo客户端 MetaServer Portal通过域名访问MetaServer获取AdminService的地址列表 Client通过域名访问MetaServer获取ConfigService的地址列表 相当于一个Eureka Proxy (提供rest-api), 和ConfigService在一起部署（同一个jvm） Eureka 用于服务发现和注册 Config/AdminService注册实例并定期报心跳 和ConfigService住在一起部署 AdminService 提供配置管理接口 提供配置修改发布接口 服务于管理界面Portal 吐槽: 这货与ConfigService共享DB, 除了使用DB做消息通道外（原谅）、还做配置中心（靠） Portal 配置管理界面 通过MetaServer获取AdminService的服务列表 使用客户端软负载SLB方式调用AdminService LB 和域名系统配合，协助Portal访问MetaServer获取AdminService地址列表（也可以配置为客户端负载均衡） 和域名系统配合，协助Client访问MetaServer获取ConfigService地址列表 和域名系统配合，协助用户访问Portal进行配置管理 Client 为应用获取配置，支持实时更新 通过MetaServer获取ConfigService的服务列表 使用客户端软负载SLB方式调用ConfigService 配置下发流程为了突出关键内容，下面流程忽略掉了服务发现的步骤；结合架构图，请自行脑补各模块之间怎么基于DB中的内容来找到目标模块。 用户在Portal上修改配置，并发布； 基于配置所在的目标环境，Portal调用对应AdminService的接口来操作发布； AdminService发布配置后，发送ReleaseMessage到其关联的数据库； ConfigService每秒扫描数据库，一旦发现存在新的消息记录，就通知到消息监听器，由消息监听器通知Client； Client与ConfigService之间是通过HTTP long-polling来实现长连接的； 作为一种补偿机制，Client还定期从ConfigService拉取最新配置（默认每5分钟拉取一次，可以自定义）； 服务端口 服务 端口 config server 8080 admim server 8090 portal server 8070 部署准备工作在github上下载apollo对应的release包，主要包含三部分： apollo-adminserver apollo-configservice (包含ConfigServer、MetaServer和Eureka) apollo-portal 除了这些jar包之外，还需要mysql数据库和负载均衡，用来向客户端和用户暴露统一的访问地址。 部署架构下面是部署两套环境DEV、UAT（每套环境都有两个IDC）的一张架构图，具体如下： 数据流 在前面的章节我们大概讲了配置发布的流程，这里重点结合负载均衡来讲一下（对理解配置有好处）。 portal-server配置 首先，用户访问UI的请求通过LB-1进来被送到portal server上处理。因此，LB-1应该暴露8070端口，其upstream为多个portal-server的8070端口。 portal-server接收发布配置请求之后，需要找到配置所属环境的admin-server来将配置写入到数据库中，并向admin-server发送releaseMessage。为了便于找到各个环境对应的admin-server，portal-server需要去查找对应环境的meta-server，再通过meta-server来查询当前可用的admin-server。各环境与其对应meta-server地址的映射关系，需要我们手动写入到portal-server的配置文件中，具体见架构图。 现在问题来了，假设我们在同一个环境中有多个idc，每个idc为了client不跨地域访问，都只将config-server和admin-server注册到本地域的meta-server上，那么，portal-server应该如何路由需要发送到admin-server的请求呢？答案是：随意！因为对于portal-server来讲，admin-server属于哪一个idc都没有关系，只要它能够正确的访问到该环境的数据库，portal-server就敢大胆的将活儿承包给这个老弟做。 假设我们在lb-2上代理idc-1的metadata-server多个实例的地址，这样会有什么问题吗？举一个例子，如果meta-server-1是健康的，所以lb-2会将请求路由过来处理。但是meta-server-1上注册的所有admin-server都挂掉了，此时这个请求就必然面临超时的异常。但是，portal-server的作者为我们想到了这一点，进入源码查看，会发现portal-server每次都是直接找到可用的admin-server，而不是找到对应的meta-server就终止。 config-server配置 如前面提到的，当我们在某一个环境有跨数据中心部署的需求时，各个数据中心理论上都有自己的中间件，因此对于应用程序来说，配置都会有差异。因此，两个区域的应用程序理论上都没有跨区域去获取配置的需求。 对于这种情况，config-server、admin-server支持按照区域（cluster）来注册到不同的meta-server。当然，meta-server通config-server一起部署到一个jvm中，因此也是位于不同的cluster中。比如，图中DEV环境的idc-1中，多个meta-server之间相互同步所注册的服务，但是与DEV环境的idc-2中的meta-server之间不会有同步操作。对于客户端来讲，其所位于哪一个idc，就需要访问该idc中对应的meta-server，一般该meta-server都做了多实例部署，通过暴露负载均衡地址的方式来供客户端访问。 为了让idc中的config-server、admin-server只注册到本idc的meta-server，需要在ServerConfig表中添加对应idc的条目，重点是对应的eureka.server.url。同时，需要修改对应的配置文件/opt/settings/server.properties，具体见图上。 使用与抽象 项目或者应用（application）代表一个使用配置中心的应用，每个应用会有一个appID，需要在代码中使用到； 环境（environment）这里的环境是固定的几个值，即：DEV、UAT、PRO。环境信息决定了portal要将配置信息发送到哪些admin-server中，是由外入里的第一层。 集群（cluster）是一种抽象，可以是机房的概念，也可以是一个逻辑集合。不同集群会使用不同的config-server，一个config-server就对应一个集群。 命名空间（namespace）对应同一个应用的多个配置文件，一个命名空间对应一个配置文件。命名空间有分类，有的可以继承配置信息，就不需要重复配置数据。应用默认会自带application这个namespace，一般对应JAVA的application.yaml。namespace支持的格式包括：properties、xml、yml、yaml、json。namespace支持两种权限： private（能被任何应用获取） 和 public(只能被所属的应用获取到)namespace支持三种权属关系：私有类型、公共类型、关联类型（继承类型）","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"apollo","slug":"apollo","permalink":"http://ljchen.net/tags/apollo/"}]},{"title":"Gitbook必备技能","slug":"gitbook必备技能","date":"2019-01-04T15:22:20.000Z","updated":"2020-04-06T09:24:01.481Z","comments":true,"path":"2019/01/04/gitbook必备技能/","link":"","permalink":"http://ljchen.net/2019/01/04/gitbook必备技能/","excerpt":"GitBook是一个基于Node.js的命令行工具，可使用Github/Git和Markdown来制作电子书（其实主要是写用户手册，API使用文档之类的啦）。总之，程序员必备技能，这里就不多强调重要性了！markdown语法请自行google，本文只简单介绍一些gitbook的目录结构和简单用法。 安装gitbook-cli1npm install gitbook-cli -g","text":"GitBook是一个基于Node.js的命令行工具，可使用Github/Git和Markdown来制作电子书（其实主要是写用户手册，API使用文档之类的啦）。总之，程序员必备技能，这里就不多强调重要性了！markdown语法请自行google，本文只简单介绍一些gitbook的目录结构和简单用法。 安装gitbook-cli1npm install gitbook-cli -g 安装好之后，可以大概了解下其功能和命令123456789101112131415161718192021222324252627282930313233gitbook help build [book] [output] build a book --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) --[no-]timing Print timing debug information (Default is false) serve [book] [output] serve the book as a website for testing --port Port for server to listen on (Default is 4000) --lrport Port for livereload server to listen on (Default is 35729) --[no-]watch Enable file watcher and live reloading (Default is true) --[no-]live Enable live reloading (Default is true) --[no-]open Enable opening book in browser (Default is false) --browser Specify browser for opening book (Default is ) --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) install [book] install all plugins dependencies --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) parse [book] parse and print debug information about a book --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) init [book] setup and create files for chapters --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) pdf [book] [output] build a book into an ebook file --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) epub [book] [output] build a book into an ebook file --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) mobi [book] [output] build a book into an ebook file --log Minimum log level to display (Default is info; Values are debug, info, warn, error, disabled) 制作电纸书初始化首先创建一个电子书的目录，比如叫做book。然后进入到book目录下，执行以下操作。 1234$ gitbook init$ lsREADME.md SUMMARY.md 执行以上命令会自动创建电纸书必须的README.md和SUMMARY.md文件。 README.md文件是你作品的介绍 SUMMARY.md是你作品的目录结构 添加内容初始化之后，我们就可以往book目录下添加子目录和文件(需要按照markdown语法来写内容)了，下面是添加了二级目录和内容后的结构。 123456789101112131415161718192021$ tree.├── LICENSE├── README.md├── SUMMARY.md├── book│ ├── README.md│ ├── file.md│ └── prjinit.md├── howtouse│ ├── Nodejsinstall.md│ ├── README.md│ ├── gitbookcli.md│ └── gitbookinstall.md├── output│ ├── README.md│ ├── outfile.md│ └── pdfandebook.md└── publish ├── README.md └── gitpages.md 对应的，我们需要修改SUMMARY.md, 从而建立电子书的目录层次和到具体文章的链接（请注意这里是使用的相对地址，而不是绝对地址）。 12345678910111213141516$ cat SUMMARY.md* [Introduction](README.md)* [基本安装](howtouse/README.md) * [Node.js安装](howtouse/nodejsinstall.md) * [Gitbook安装](howtouse/gitbookinstall.md) * [Gitbook命令行速览](howtouse/gitbookcli.md)* [图书项目结构](book/README.md) * [README.md 与 SUMMARY编写](book/file.md) * [目录初始化](book/prjinit.md)* [图书输出](output/README.md) * [输出为静态网站](output/outfile.md) * [输出PDF](output/pdfandebook.md)* [发布](publish/README.md) * [发布到Github Pages](publish/gitpages.md)* [结束](end/README.md) 运行服务12# 默认启动，使用4000端口gitbook serve 运行起来后，可以在浏览器中通过 http://localhost:4000 来访问启动后的服务。[注意]: 默认启动使用的是4000端口，当然我们也可以通过参数 -p 来指定使用的接口。 12# 通过-p指定端口gitbook serve -p 8080 发布撰写完成后，我们可以生成静态网站用来发布。1gitbook build","categories":[{"name":"others","slug":"others","permalink":"http://ljchen.net/categories/others/"}],"tags":[{"name":"gitbook","slug":"gitbook","permalink":"http://ljchen.net/tags/gitbook/"},{"name":"user-manuals","slug":"user-manuals","permalink":"http://ljchen.net/tags/user-manuals/"}]},{"title":"Consul原理解析","slug":"consul原理解析","date":"2019-01-04T13:03:41.000Z","updated":"2020-04-06T09:24:01.469Z","comments":true,"path":"2019/01/04/consul原理解析/","link":"","permalink":"http://ljchen.net/2019/01/04/consul原理解析/","excerpt":"从2016年开始接触微服务的时候就使用consul，当初只知道其特别方便，是一款不错的服务注册与发现工具。至于其部署架构，实现原理都没有深入去了解过，就如同年少读书不求甚解。最近，在着手搞微服务治理，服务治理与发现这块正好选型consul，这才详细的琢磨了下其代码，也对其原理有了一定的认识。下面就听我就徐徐道来……","text":"从2016年开始接触微服务的时候就使用consul，当初只知道其特别方便，是一款不错的服务注册与发现工具。至于其部署架构，实现原理都没有深入去了解过，就如同年少读书不求甚解。最近，在着手搞微服务治理，服务治理与发现这块正好选型consul，这才详细的琢磨了下其代码，也对其原理有了一定的认识。下面就听我就徐徐道来…… 架构介绍下面是consul官方给出的一张架构图，我们先来理解一下。 首先，从架构上，图片被两个datacenter分成了上下两部分；但这两部分又并不是完全隔离的，他们之间通过WAN GOSSIP在Internet上交互报文。因此，我们了解到consul是可以支持多个数据中心之间基于WAN来做同步的。 再看单个datacenter内部，节点被划分为两种颜色，其中红色为server，紫色为client。它们之间通过GRPC通信（主要用于业务数据）。除此之外，server和client之间，还有一条LAN GOSSIP通信，这是用于当LAN内部发生了拓扑变化时，存活的节点们能够及时感知，比如server节点down掉后，client就会触发将对应server节点从可用列表中剥离出去。 当然，server与server之间，client与client之间，client与server之间，在同一个datacenter中的所有consul agent会组成一个LAN网络（当然它们之间也可以按照区域划分segment），当LAN网中有任何角色变动，或者有用户自定义的event产生的时候，其他节点就会感知到，并触发对应的预置操作。 所有的server节点共同组成了一个集群，他们之间运行raft协议，通过共识仲裁选举出leader。所有的业务数据都通过leader写入到集群中做持久化，当有半数以上的节点存储了该数据后，server集群才会返回ACK，从而保障了数据的强一致性。当然，server数量大了之后，也会影响写数据的效率。所有的follower会跟随leader的脚步，保障其有最新的数据副本。 同一个consul agent程序，通过启动的时候指定不同的参数来运行server或client模式。这两种模式下，各自所负责的事务具体如下。 Server节点 参与共识仲裁(raft) 存储群集状态(日志存储) 处理查询 维护与周边(LAN/WAN)各节点关系 Agent节点 负责通过该节点注册到consul的微服务的健康检查 将客户端注册请求以及查询转化为对server的RPC请求 维护与周边(LAN/WAN)各节点关系 服务端口 端口 作用 8300 RPC exchanges 8301 LAN GOSSIP 8302 WAN GOSSIP 8400 RPC exchanges by the CLI 8500 Used for HTTP API and web interface 8600 Used for DNS server 实现原理纵观consul的实现，其核心在于两点： 集群内节点间信息的高效同步机制，其保障了拓扑变动以及控制信号的及时传递； server集群内日志存储的强一致性。 它们主要基于以下两个协议来实现： 使用gossip协议在集群内传播信息 使用raft协议来保障日志的一致性 Serfserf是hashicorp基于GOSSIP协议来实现的一个用于分布式集群成员管理，失败检测以及编排的工具，当前最新版本为v0.8.1。有兴趣的朋友可以到这个链接具体了解hashicorp serf，下面我来简单介绍一下其功能。 集群管理这台机器上有两个IP地址，一个是172.20.20.10，另一个为172.20.20.10。我准备启动两个serf agent进程，分别绑定到不同的两个IP地址上，各自叫做agent-one和agent-two。 由于它们启动之后，相互之间是不知道彼此的，我通过执行serf join来把它们组成一个LAN serf。这样它们就可以彼此检测到彼此，通过查看serf members可以看到所有的节点以及其健康状况。 123456789$ serf agent -node=agent-one -bind=172.20.20.10$ serf agent -node=agent-two -bind=172.20.20.11$ serf join 172.20.20.11$ serf membersagent-one 172.20.20.10:7946 aliveagent-two 172.20.20.11:7946 alive 事件响应在前面的步骤中，我们将两个serf进程加入到了同一个LAN中，接下来我们将进行一些更加激动人心的实践。接下来，我们创建了一个脚本(handler.sh)，大致内容为:当脚本被调用的时候，会打印出一些具体的信息。然后，我们在启动serf agent的时候，通过参数将该脚本传递给serf agent。这样当收该serf节点收到event时，就会调用用户指定的handler（即执行脚本）。 12345678910$ cat handler.sh#!/bin/bashechoecho \"New event: $&#123;SERF_EVENT&#125;. Data follows...\"while read line; do printf \"$&#123;line&#125;\\n\"done$ serf agent -log-level=debug -event-handler=handler.sh 发送自定义event 1$ serf event hello-there Event类型serf指定了下面这些类型的event，各自的作用如下所示： 1234567member-join One or more members have joined the cluster.member-leave One or more members have gracefully left the cluster.member-failed One or more members have failed, meaning that they did not properly respond to ping requests.member-update One or more members have updated, likely to update the associated tagsmember-reap Serf has removed one or more members from its list of members. This means a failed node exceeded the reconnect_timeout, or a left node reached the tombstone_timeout.user A custom user event, covered later in this guide.query A query event, covered later in this guide Raft由于介绍raft协议的文章已经比较多，我这里就不在详述。这里重点分析一下在consul中，raft协议运作的一些实践和日志。 节点状态变更 在节点数达到bootstrap-expect的数时，开始启用raft选举 在节点数超过bootstrap-expect数时，其他节点为follower 在leader被干掉后，raft如果判断到节点数依然大于等于bootstrap-expect时，重新选举 逐一干掉节点，当节点数少于bootstrap-expect时，raft协议不再选举，将维持先前的状态。 Raft选举日志分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# 选举日志信息 （bootstrap）==&gt; Starting Consul agent...bootstrap_expect &gt; 0: expecting 3 servers==&gt; Consul agent running! Version: 'v1.4.0' Node ID: 'f217ca95-e83c-9a1f-9e87-3b5c1f5a82a3' Node name: '42ddc7aa3bb6' Datacenter: 'dc1' (Segment: '&lt;all&gt;') Server: true (Bootstrap: false) Client Addr: [127.0.0.1] (HTTP: 8500, HTTPS: -1, gRPC: -1, DNS: 8600) Cluster Addr: 172.17.0.2 (LAN: 8301, WAN: 8302) Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false==&gt; Log data will now stream in as it occurs: 2018/12/03 07:21:34 [INFO] raft: Initial configuration (index=0): [] 2018/12/03 07:21:34 [INFO] serf: EventMemberJoin: 42ddc7aa3bb6.dc1 172.17.0.2 2018/12/03 07:21:34 [INFO] serf: EventMemberJoin: 42ddc7aa3bb6 172.17.0.2 2018/12/03 07:21:34 [INFO] raft: Node at 172.17.0.2:8300 [Follower] entering Follower state (Leader: \"\") 2018/12/03 07:21:34 [INFO] consul: Adding LAN server 42ddc7aa3bb6 (Addr: tcp/172.17.0.2:8300) (DC: dc1) 2018/12/03 07:21:34 [INFO] consul: Handled member-join event for server \"42ddc7aa3bb6.dc1\" in area \"wan\" 2018/12/03 07:21:34 [INFO] agent: Started DNS server 127.0.0.1:8600 (tcp) 2018/12/03 07:21:34 [INFO] agent: Started DNS server 127.0.0.1:8600 (udp) 2018/12/03 07:21:34 [INFO] agent: Started HTTP server on 127.0.0.1:8500 (tcp) 2018/12/03 07:21:34 [INFO] agent: started state syncer 2018/12/03 07:21:34 [INFO] agent: Retry join LAN is supported for: aliyun aws azure digitalocean gce k8s os packet scaleway softlayer triton vsphere 2018/12/03 07:21:34 [INFO] agent: Joining LAN cluster... 2018/12/03 07:21:34 [INFO] agent: (LAN) joining: [172.17.0.2] 2018/12/03 07:21:34 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt; 2018/12/03 07:21:34 [INFO] agent: Join LAN completed. Synced with 1 initial agents # node数量没有达到，无法开始选举 2018/12/03 07:21:41 [ERR] agent: failed to sync remote state: No cluster leader 2018/12/03 07:21:43 [WARN] raft: no known peers, aborting election 2018/12/03 07:21:54 [INFO] serf: EventMemberJoin: 4eb2b75f454a 172.17.0.3 2018/12/03 07:21:54 [INFO] consul: Adding LAN server 4eb2b75f454a (Addr: tcp/172.17.0.3:8300) (DC: dc1) 2018/12/03 07:21:54 [INFO] serf: EventMemberJoin: 4eb2b75f454a.dc1 172.17.0.3 2018/12/03 07:21:54 [INFO] consul: Handled member-join event for server \"4eb2b75f454a.dc1\" in area \"wan\" 2018/12/03 07:21:58 [INFO] serf: EventMemberJoin: b603f61d1449 172.17.0.4 2018/12/03 07:21:58 [INFO] consul: Adding LAN server b603f61d1449 (Addr: tcp/172.17.0.4:8300) (DC: dc1) # node数量达到，开始选举 2018/12/03 07:21:58 [INFO] consul: Found expected number of peers, attempting bootstrap: 172.17.0.2:8300,172.17.0.3:8300,172.17.0.4:8300 2018/12/03 07:21:58 [INFO] serf: EventMemberJoin: b603f61d1449.dc1 172.17.0.4 2018/12/03 07:21:58 [INFO] consul: Handled member-join event for server \"b603f61d1449.dc1\" in area \"wan\" 2018/12/03 07:22:03 [WARN] raft: Heartbeat timeout from \"\" reached, starting election # 状态迁移 2018/12/03 07:22:03 [INFO] raft: Node at 172.17.0.2:8300 [Candidate] entering Candidate state in term 2 # 获胜 2018/12/03 07:22:03 [INFO] raft: Election won. Tally: 2 2018/12/03 07:22:03 [INFO] raft: Node at 172.17.0.2:8300 [Leader] entering Leader state 2018/12/03 07:22:03 [INFO] raft: Added peer 5b0b26fb-5e62-c390-0ced-b80e0f3293ef, starting replication 2018/12/03 07:22:03 [INFO] raft: Added peer 3844affd-9b4e-ad3d-84f3-25fb77806e7c, starting replication 2018/12/03 07:22:03 [INFO] consul: cluster leadership acquired 2018/12/03 07:22:03 [INFO] consul: New leader elected: 42ddc7aa3bb6 2018/12/03 07:22:03 [WARN] raft: AppendEntries to &#123;Voter 3844affd-9b4e-ad3d-84f3-25fb77806e7c 172.17.0.4:8300&#125; rejected, sending older logs (next: 1) 2018/12/03 07:22:03 [WARN] raft: AppendEntries to &#123;Voter 5b0b26fb-5e62-c390-0ced-b80e0f3293ef 172.17.0.3:8300&#125; rejected, sending older logs (next: 1) 2018/12/03 07:22:03 [INFO] raft: pipelining replication to peer &#123;Voter 3844affd-9b4e-ad3d-84f3-25fb77806e7c 172.17.0.4:8300&#125; 2018/12/03 07:22:03 [INFO] raft: pipelining replication to peer &#123;Voter 5b0b26fb-5e62-c390-0ced-b80e0f3293ef 172.17.0.3:8300&#125; 2018/12/03 07:22:03 [INFO] consul: member '42ddc7aa3bb6' joined, marking health alive 2018/12/03 07:22:03 [INFO] consul: member '4eb2b75f454a' joined, marking health alive 2018/12/03 07:22:03 [INFO] consul: member 'b603f61d1449' joined, marking health alive==&gt; Failed to check for updates: Get https://checkpoint-api.hashicorp.com/v1/check/consul?arch=amd64&amp;os=linux&amp;signature=2ba01aad-86ad-32b1-2cff-dc77537fa0dd&amp;version=1.4.0: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 2018/12/03 07:22:05 [INFO] agent: Synced node info 2018/12/03 07:27:25 [INFO] serf: EventMemberJoin: 212149acfdcd 172.17.0.5 2018/12/03 07:27:25 [INFO] consul: Adding LAN server 212149acfdcd (Addr: tcp/172.17.0.5:8300) (DC: dc1) # 添加的新节点为novoter角色，无法参与选举 2018/12/03 07:27:25 [INFO] raft: Updating configuration with AddNonvoter (45c059ba-bd5c-5a00-f7d6-e490324e7b52, 172.17.0.5:8300) to [&#123;Suffrage:Voter ID:f217ca95-e83c-9a1f-9e87-3b5c1f5a82a3 Address:172.17.0.2:8300&#125; &#123;Suffrage:Voter ID:5b0b26fb-5e62-c390-0ced-b80e0f3293ef Address:172.17.0.3:8300&#125; &#123;Suffrage:Voter ID:3844affd-9b4e-ad3d-84f3-25fb77806e7c Address:172.17.0.4:8300&#125; &#123;Suffrage:Nonvoter ID:45c059ba-bd5c-5a00-f7d6-e490324e7b52 Address:172.17.0.5:8300&#125;] 2018/12/03 07:27:25 [INFO] serf: EventMemberJoin: 212149acfdcd.dc1 172.17.0.5 2018/12/03 07:27:25 [INFO] consul: Handled member-join event for server \"212149acfdcd.dc1\" in area \"wan\" 2018/12/03 07:27:25 [INFO] raft: Added peer 45c059ba-bd5c-5a00-f7d6-e490324e7b52, starting replication 2018/12/03 07:27:25 [WARN] raft: AppendEntries to &#123;Nonvoter 45c059ba-bd5c-5a00-f7d6-e490324e7b52 172.17.0.5:8300&#125; rejected, sending older logs (next: 1) 2018/12/03 07:27:25 [INFO] consul: member '212149acfdcd' joined, marking health alive 2018/12/03 07:27:25 [INFO] raft: pipelining replication to peer &#123;Nonvoter 45c059ba-bd5c-5a00-f7d6-e490324e7b52 172.17.0.5:8300&#125; 2018/12/03 07:27:28 [INFO] serf: EventMemberJoin: edb64d232050 172.17.0.6 2018/12/03 07:27:28 [INFO] consul: Adding LAN server edb64d232050 (Addr: tcp/172.17.0.6:8300) (DC: dc1) # 添加的新节点为novoter角色，无法参与选举 2018/12/03 07:27:28 [INFO] raft: Updating configuration with AddNonvoter (46ebd85c-5e96-f9bd-81e4-0a82d3b405c7, 172.17.0.6:8300) to [&#123;Suffrage:Voter ID:f217ca95-e83c-9a1f-9e87-3b5c1f5a82a3 Address:172.17.0.2:8300&#125; &#123;Suffrage:Voter ID:5b0b26fb-5e62-c390-0ced-b80e0f3293ef Address:172.17.0.3:8300&#125; &#123;Suffrage:Voter ID:3844affd-9b4e-ad3d-84f3-25fb77806e7c Address:172.17.0.4:8300&#125; &#123;Suffrage:Nonvoter ID:45c059ba-bd5c-5a00-f7d6-e490324e7b52 Address:172.17.0.5:8300&#125; &#123;Suffrage:Nonvoter ID:46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 Address:172.17.0.6:8300&#125;] 2018/12/03 07:27:28 [INFO] serf: EventMemberJoin: edb64d232050.dc1 172.17.0.6 2018/12/03 07:27:28 [INFO] consul: Handled member-join event for server \"edb64d232050.dc1\" in area \"wan\" 2018/12/03 07:27:28 [INFO] raft: Added peer 46ebd85c-5e96-f9bd-81e4-0a82d3b405c7, starting replication 2018/12/03 07:27:28 [INFO] consul: member 'edb64d232050' joined, marking health alive 2018/12/03 07:27:28 [WARN] raft: AppendEntries to &#123;Nonvoter 46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 172.17.0.6:8300&#125; rejected, sending older logs (next: 1) 2018/12/03 07:27:28 [INFO] raft: pipelining replication to peer &#123;Nonvoter 46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 172.17.0.6:8300&#125; 2018/12/03 07:27:43 [INFO] autopilot: Promoting Server (ID: \"45c059ba-bd5c-5a00-f7d6-e490324e7b52\" Address: \"172.17.0.5:8300\") to voter 2018/12/03 07:27:43 [INFO] raft: Updating configuration with AddStaging (45c059ba-bd5c-5a00-f7d6-e490324e7b52, 172.17.0.5:8300) to [&#123;Suffrage:Voter ID:f217ca95-e83c-9a1f-9e87-3b5c1f5a82a3 Address:172.17.0.2:8300&#125; &#123;Suffrage:Voter ID:5b0b26fb-5e62-c390-0ced-b80e0f3293ef Address:172.17.0.3:8300&#125; &#123;Suffrage:Voter ID:3844affd-9b4e-ad3d-84f3-25fb77806e7c Address:172.17.0.4:8300&#125; &#123;Suffrage:Voter ID:45c059ba-bd5c-5a00-f7d6-e490324e7b52 Address:172.17.0.5:8300&#125; &#123;Suffrage:Nonvoter ID:46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 Address:172.17.0.6:8300&#125;] 2018/12/03 07:27:43 [INFO] autopilot: Promoting Server (ID: \"46ebd85c-5e96-f9bd-81e4-0a82d3b405c7\" Address: \"172.17.0.6:8300\") to voter 2018/12/03 07:27:43 [INFO] raft: Updating configuration with AddStaging (46ebd85c-5e96-f9bd-81e4-0a82d3b405c7, 172.17.0.6:8300) to [&#123;Suffrage:Voter ID:f217ca95-e83c-9a1f-9e87-3b5c1f5a82a3 Address:172.17.0.2:8300&#125; &#123;Suffrage:Voter ID:5b0b26fb-5e62-c390-0ced-b80e0f3293ef Address:172.17.0.3:8300&#125; &#123;Suffrage:Voter ID:3844affd-9b4e-ad3d-84f3-25fb77806e7c Address:172.17.0.4:8300&#125; &#123;Suffrage:Voter ID:45c059ba-bd5c-5a00-f7d6-e490324e7b52 Address:172.17.0.5:8300&#125; &#123;Suffrage:Voter ID:46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 Address:172.17.0.6:8300&#125;] 源码架构先来看Consul内部是如何做服务注册与发现的流程，下图是consul客户端向agent注册以及发现目标服务的时序图。 通过上图，我们大概知道了在consul agent中，功能分为了consul server和consul agent（client）。在前面架构介绍中我们已经阐述了server和client各自的职责。 consul源码中，server和client都是在一套代码中，通过指定启动参数的形势来运行consul server。这里我们先来重点讲解一下consul client的内部架构。 Consul Client架构 上图简要描述了consul client中的各重要服务，以及它们之间的关系。 lan serf主要职责是维护节点之间的关系，当有节点加入或者离开的时候，所有节点都会接收到对应的event，这里的lan serf就是指对这些event做处理的handler的go routine服务。 state sync在consul启动的时候，会启动该服务，它监听一个channel，当其他服务有向consul server同步配置的需求的时候，就会像channel中写入event信息；然后就会触发该服务向consul server同步配置信息。这里的同步又分为全同步和部分同步，主要是为了降低网路的负担。 gRPC router这是对连接到consul server的gRPC连接的维护和负载均衡机制。在该服务中心，一方面会基于lan serf对consul server节点的拓扑变更事件来维护server列表，另一方面也会对到存活server的connection做定期的ping来维护连接列表；除此之外，还能够对server连接做客户端负载均衡。 local state是一个本地的内存数据库，一般执行sync就是从server将数据同步过来保存到该db中；平时做一些配置更改也会对应更新该db。 apiconsul是提供了HTTP和CLI两种对外访问方式的，这里所谓的API并不是想说接口的细节，而指的是consul所提供对外API对应controller逻辑实现。比如下一节要讲到的服务注册的API，后面都做了什么业务逻辑，这是很重要的一部分，对于复杂的逻辑一般包括了：更新本地local state，启动对应的go routine来做事，使用gRPC向server更新数据，向sync channel发消息从而触发sync等操作。 服务注册流程基于前面一节的介绍，我们大概能够猜测到服务注册大概都需要些什么样的流程，接下来我们就将以下这块的逻辑。 上图是其服务注册API的controller中函数调用的一个简化流程。 首先s.agent.AddService函数要做的就是将接收到的服务信息做一通校验，然后整理成为local state的数据结构之后保存到本地；但是由于它是一个内存数据库，并不能够持久化，于是再将其保存到本地文件中做持久化。 干完这些操作之后，如果该服务没有指定healthcheck操作的话，接下来要做的就是将这个服务注册请求同步到consul server，让raft leader将数据真正持久化到server中，这部分我没有在图上体现出来，但是在代码中确实是这样实现的。 对于在注册的时候制定了healthcheck内容的服务，需要继续注册healthcheck。由于consul支持的healthcheck类型较多，这里对其所指定类型做了简单的校验，然后就开始干正事了。启动一个goroutine来专门为这个服务执行定期的健康检查操作，可见，如果该consul agent上注册的服务太多的话，势必消耗很多资源，这就要求我们部署方案要做好规划了。 当健康检查的结果与先前的结果不一致的时候，会触发对local state的更新，同时，需要局部同步该服务到consul server上的内容。为什么呢？因为服务的健康状态其实是保存到其check字段下的，而非是service的一个一级属性，这块大家可以下去查阅一下代码。另外，每次状态变更都会触发consul agent通过gRPC调用server的Catalog.Register来注册服务，我的理解其实是覆盖先前注册关于该服务的信息。 操作实践介绍consul agent的配置参数，以及各种使用场景下的命令。 consul agent参数12345678910111213141516171819202122-advertise 通知展现地址用来改变我们给集群中的其他节点展现的地址，一般情况下-bind地址就是展现地址-bootstrap 用来控制一个server是否在bootstrap模式，在一个datacenter中只能有一个server处于bootstrap模式，当一个server处于bootstrap模式时，可以自己选举为raft leader。-bootstrap-expect 在一个datacenter中期望提供的server节点数目，当该值提供的时候，consul一直等到达到指定sever数目的时候才会引导整个集群，该标记不能和bootstrap公用-bind 该地址用来在集群内部的通讯，集群内的所有节点到地址都必须是可达的，默认是0.0.0.0-client consul绑定在哪个client地址上，这个地址提供HTTP、DNS、RPC等服务，默认是127.0.0.1-config-file 明确的指定要加载哪个配置文件-config-dir 配置文件目录，里面所有以.json结尾的文件都会被加载-data-dir 提供一个目录用来存放agent的状态，所有的agent允许都需要该目录，该目录必须是稳定的，系统重启后都继续存在-dc 该标记控制agent允许的datacenter的名称，默认是dc1-encrypt 指定secret key，使consul在通讯时进行加密，key可以通过consul keygen生成，同一个集群中的节点必须使用相同的key-join 加入一个已经启动的agent的ip地址，可以多次指定多个agent的地址。如果consul不能加入任何指定的地址中，则agent会启动失败，默认agent启动时不会加入任何节点。-retry-join 和join类似，但是允许你在第一次失败后进行尝试。-retry-interval 两次join之间的时间间隔，默认是30s-retry-max 尝试重复join的次数，默认是0，也就是无限次尝试-log-level consul agent启动后显示的日志信息级别。默认是info，可选：trace、debug、info、warn、err。-node 节点在集群中的名称，在一个集群中必须是唯一的，默认是该节点的主机名-protocol consul使用的协议版本-rejoin 使consul忽略先前的离开，在再次启动后仍旧尝试加入集群中。-server 定义agent运行在server模式，每个集群至少有一个server，建议每个集群的server不要超过5个-syslog 开启系统日志功能，只在linux/osx上生效-ui-dir 提供存放web ui资源的路径，该目录必须是可读的-pid-file 提供一个路径来存放pid文件，可以使用该文件进行SIGINT/SIGHUP(关闭/更新)agent 常用命令开发模式最简单，可以用于本地微服务开发的时候，零时做服务注册与发现工具。请注意的是，开发模式下，consul不会做配置的持久化，当consul服务终止时，之前注册的服务和K/V都会随之丢失！ 1docker run -d --name=dev-consul -e CONSUL_BIND_INTERFACE=eth0 consul server模式1docker run -d --net=host -e 'CONSUL_LOCAL_CONFIG=&#123;\"skip_leave_on_interrupt\": true&#125;' consul agent -server -bind=&lt;external ip&gt; -retry-join=&lt;root agent ip&gt; -bootstrap-expect=&lt;number of server agents&gt; 启动server 1docker run -d -e 'CONSUL_LOCAL_CONFIG=&#123;\"skip_leave_on_interrupt\": true&#125;' consul agent -server -retry-join=172.17.0.2 -bootstrap-expect=3 client模式 启动client 1docker run -d -e 'CONSUL_LOCAL_CONFIG=&#123;\"skip_leave_on_interrupt\": true&#125;' consul agent -retry-join=172.17.0.2 暴露dns 1docker run -d --net=host -e 'CONSUL_ALLOW_PRIVILEGED_PORTS=' consul -dns-port=53 -recursor=8.8.8.8 查询 12docker exec -t dev-consul consul membersdig @localhost -p 8600 consul.service.consul 集群部署实践下面是部署两个server和一个agent的实例 s1: 10.200.204.104 1./consul agent -server -bootstrap-expect 1 -data-dir /etc/consul/data -node=s1 -bind=10.200.204.104 -ui -rejoin -config-dir=/etc/consul/conf -client 0.0.0.0 s2: 10.200.204.48 1./consul agent -server -bootstrap-expect 1 -data-dir /etc/consul/data -node=s2 -bind=10.200.204.48 -ui -rejoin -config-dir=/etc/consul/conf -client 0.0.0.0 -retry-join=10.200.204.104 c1: 10.200.204.133 1./consul agent -node=c1 -bind=10.200.204.133 -data-dir /etc/consul/data -ui -rejoin -config-dir=/etc/consul/conf -client 0.0.0.0 -retry-join=10.200.204.48 查看raft角色12345# consul operator raft list-peersNode ID Address State Voter RaftProtocolb603f61d1449 3844affd-9b4e-ad3d-84f3-25fb77806e7c 172.17.0.4:8300 follower true 3212149acfdcd 45c059ba-bd5c-5a00-f7d6-e490324e7b52 172.17.0.5:8300 follower true 3edb64d232050 46ebd85c-5e96-f9bd-81e4-0a82d3b405c7 172.17.0.6:8300 leader true 3 注册服务到consul1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &lt;&lt; EOF &gt;&gt; payload.json&#123; \"Datacenter\": \"dc1\", \"ID\": \"40e4a748-2192-161a-0510-9bf59fe950b5\", \"Node\": \"foobar\", \"Address\": \"192.168.10.10\", \"TaggedAddresses\": &#123; \"lan\": \"192.168.10.10\", \"wan\": \"10.0.10.10\" &#125;, \"NodeMeta\": &#123; \"somekey\": \"somevalue\" &#125;, \"Service\": &#123; \"ID\": \"redis1\", \"Service\": \"redis\", \"Tags\": [ \"primary\", \"v1\" ], \"Address\": \"127.0.0.1\", \"Meta\": &#123; \"redis_version\": \"4.0\" &#125;, \"Port\": 8000 &#125;, \"Check\": &#123; \"Node\": \"foobar\", \"CheckID\": \"service:redis1\", \"Name\": \"Redis health check\", \"Notes\": \"Script based health check\", \"Status\": \"passing\", \"ServiceID\": \"redis1\", \"Definition\": &#123; \"TCP\": \"localhost:8888\", \"Interval\": \"5s\", \"Timeout\": \"1s\", \"DeregisterCriticalServiceAfter\": \"30s\" &#125; &#125;, \"SkipNodeUpdate\": false&#125;EOF 1curl --request PUT --data @payload.json http://127.0.0.1:8500/v1/catalog/register 一般不推荐使用catalog注册，而是使用agent来注册，注册到agent如下: 1curl -X PUT -H 'application/json' -d '&#123;\"ID\": \"taobao\",\"Name\": \"taobao\",\"Tags\": [\"primary\",\"v1\"],\"Address\": \"140.205.94.189\",\"Port\": 80,\"Meta\": &#123;\"taobao_version\": \"4.0\"&#125;,\"EnableTagOverride\": false,\"Check\": &#123;\"DeregisterCriticalServiceAfter\": \"90m\",\"HTTP\": \"http://140.205.94.189:80/\",\"Interval\": \"10s\"&#125;,\"Weights\": &#123;\"Passing\": 10,\"Warning\": 1&#125;&#125;' http://127.0.0.1:8500/v1/agent/service/register 配置文件注册直接将以下json文件保存后存放到--config-dir目录下，重启consul服务 123456789101112131415&#123; \"service\":&#123; \"id\": \"jetty\", \"name\": \"jetty\", \"address\": \"14.215.177.38\", \"port\": 80, \"tags\": [\"dev\"], \"checks\": [ &#123; \"http\": \"http://14.215.177.38:80/\", \"interval\": \"5s\" &#125; ] &#125;&#125; 会发现，在哪一个client节点上注册的服务，对应client节点就会负责做healthcheck，也就意味着，这个节点非常重要，如果做不好高可用，所有注册到上面的服务都有被deregisterd的风险。 API 注册1curl -X PUT -d '&#123;\"id\": \"ljchen\",\"name\": \"ljchen\",\"address\": \"14.215.177.38\",\"port\": 80,\"tags\": [\"dev\"],\"checks\": [&#123;\"http\": \"http://14.215.177.38:80/\",\"interval\": \"5s\"&#125;]&#125;' http://127.0.0.1:8500/v1/agent/service/register 查询consul中的服务1curl http://127.0.0.1:8500/v1/catalog/service/redis?tag=v1 删除node节点1curl -X PUT -H 'application/json' -d '&#123;\"Datacenter\": \"dc1\",\"Node\": \"node-name\"&#125;' http://127.0.0.1:8500/v1/catalog/deregister","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"leader-election","slug":"leader-election","permalink":"http://ljchen.net/tags/leader-election/"},{"name":"consul","slug":"consul","permalink":"http://ljchen.net/tags/consul/"}]},{"title":"Go Modules依赖管理","slug":"Go-Modules依赖管理","date":"2018-11-24T04:12:55.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2018/11/24/Go-Modules依赖管理/","link":"","permalink":"http://ljchen.net/2018/11/24/Go-Modules依赖管理/","excerpt":"go modules是golang v1.11中引入的新功能，其主要用来解决传统的golang项目必须要放到$GOPATH目录下的问题。通过一套类似于maven的方式来将依赖放到系统指定目录，统一管理。 功能开关go module支持三种模式： on顾名思义，就是打开go mod off关闭go mod, 使用传统的go vendor模式 auto自动模式，当目录在$GOPATH下时，使用 go vendor模式；当在$GOPATH之外时，使用 go mod模式","text":"go modules是golang v1.11中引入的新功能，其主要用来解决传统的golang项目必须要放到$GOPATH目录下的问题。通过一套类似于maven的方式来将依赖放到系统指定目录，统一管理。 功能开关go module支持三种模式： on顾名思义，就是打开go mod off关闭go mod, 使用传统的go vendor模式 auto自动模式，当目录在$GOPATH下时，使用 go vendor模式；当在$GOPATH之外时，使用 go mod模式 go mod开启方式如下: 1export GO111MODULE=on 使用方法主要讲解go mod各子命令，以及各自的用法；先看下其都有哪些子命令：12345678910111213141516171819~/Project/gopath/src/gitee.com/chenleji/# go help mod...Usage: go mod &lt;command&gt; [arguments]The commands are: download download modules to local cache edit edit go.mod from tools or scripts graph print module requirement graph init initialize new module in current directory tidy add missing and remove unused modules vendor make vendored copy of dependencies verify verify dependencies have expected content why explain why packages or modules are neededUse \"go help mod &lt;command&gt;\" for more information about a command. 生成依赖新项目对于新项目，直接在项目更目录执行go mod init就可以初始化出go.mod文件。然后，根据需要可以通过命令或者手工的方式往里面添加require依赖。 除了手工添加依赖之外，如果你此时就是在$GOPATH下开发，并且也调试通过了，那么你可以使用go get -m ./... 让它自动查找依赖，并记录在go.mod文件中(你还可以指定 -tags,这样可以把tags的依赖都查找到)。 为了便于复制，我们把命令整理一下：12go mod initgo get -m ./... 旧项目对于传统使用go vendor的项目，在根目录下执行go mod init会自动分析vendor中的依赖，并自动写入到go.mod文件的require列表中。 下面是操作示例：1234~# go mod initgo: creating new go.mod: module gitee.com/chenleji/project-1go: copying requirements from vendor/vendor.json.... go.mod文件go.mod文件类似于先前govendor的vendor.json, 里面主要包含了各种依赖项目以及对应的版本号和commit id信息。当然，vendor.json还需要指定项目的路径，但是go mod统一将依赖放到$GOPATH/pkg/mod/目录下（其实这类似于maven，节约了磁盘空间）。 接下来看看go.mod文件的格式:123456789 ~# cat go.modmodule gitee.com/chenleji/project-1require ( github.com/Microsoft/go-winio v0.1.0 github.com/PuerkitoBio/purell v0.0.0-20170917143911-fd18e053af8a github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 ...) 依赖下载在前面已经提到，go mod将依赖项目统一放到$GOPATH/pkg/mod/目录下。对于一些先前没有使用过的依赖项目，目录下显然没有对应的代码，此时我们可以在代码根目录执行以下命令来下载代码依赖：12go mod download &lt;path@version&gt; # 参数&lt;path@version&gt;是非必写的，path是包的路径，version是包的版本。 依赖更新如果依赖有变动，go.mod没有及时的更新，可以使用该命令来先做校验： 验证依赖项1go mod verify 可能有些依赖，我们忘记了在什么时候需要，可以查看为什么需要改以来： 解释为什么需要依赖1go mod why xx 如果明确依赖需要更新，以下命令可以用来修改依赖： 格式化 go.mod 文件 1go mod edit -fmt 添加依赖或修改依赖版本，这里支持模糊匹配版本号 1go mod edit -require=path@version 从 go.mod 删除不需要的依赖、新增需要的依赖，这个操作不会改变依赖版本。 1go mod tidy 依赖项的版本可以指定模糊之，系统会寻找满足条件的最接近的版本来构建，具体如下：12345678910111213141516171819# go mod -require=bitbucket.org/bigwhite/c@v1.0.0# go mod -require=bitbucket.org/bigwhite/d@v1.1.0# cat go.modmodule hellorequire ( bitbucket.org/bigwhite/c v1.0.0 // indirect bitbucket.org/bigwhite/d v1.1.0 // indirect)# go mod -require='bitbucket.org/bigwhite/d@&lt;v1.3.0'# cat go.modmodule hellorequire ( bitbucket.org/bigwhite/c v1.1.0 // indirect bitbucket.org/bigwhite/d &lt;v1.3.0) 从go mod回到go vendor有一些特殊需求，比如传统go vendor模式为docker下编译项目提供了便利性，保障在持续集成的时候，每一次代码构建都从网络上获取依赖。因此，有时候将代码从go mod下转换回go vendor也是有这种需求的。 1go mod vendor 用于生成 vendor 文件夹，代码应该是从mod目录下将对应版本拷贝到vendor目录的。 代码构建在go mod模式下，同样支持使用vendor模式来构建代码： go build -mod=vendor在开启模块支持的情况下，用这个可以退回到使用 vendor 的时代 go build -mod=readonly防止隐式修改 go.mod，如果遇到有隐式修改的情况会报错，可以用来测试 go.mod 中的依赖是否整洁，但如果明确调用了 go mod、go get 命令则依然会导致 go.mod 文件被修改。 查看依赖 显示依赖关系 1go list -m all 显示详细依赖关系 1go list -m -json all 打印模块依赖图 1go mod graph 配置代理该代理在墙内使用体验不错，速度挺快。 12export GO111MODULE=onexport GOPROXY=https://goproxy.cn","categories":[{"name":"programming-language","slug":"programming-language","permalink":"http://ljchen.net/categories/programming-language/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://ljchen.net/tags/golang/"}]},{"title":"Kubelet的probe探针流程分析","slug":"kubelet的probe流程分析","date":"2018-11-16T10:38:57.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2018/11/16/kubelet的probe流程分析/","link":"","permalink":"http://ljchen.net/2018/11/16/kubelet的probe流程分析/","excerpt":"kubernetes提供了用于存活性检查的liveness和用于服务就绪检查的readiness功能，这两个功能为服务的可靠性提供了极大的帮组；接下来我们就来分析下在kubelet源码中是如何实现这块功能的。 概念kubelet中提供了liveness和readiness探针，这两种探针都支持基于HTTP/TCP/Command的形势；而且他们的配置都是一致的，只是各自的用途不同而已。","text":"kubernetes提供了用于存活性检查的liveness和用于服务就绪检查的readiness功能，这两个功能为服务的可靠性提供了极大的帮组；接下来我们就来分析下在kubelet源码中是如何实现这块功能的。 概念kubelet中提供了liveness和readiness探针，这两种探针都支持基于HTTP/TCP/Command的形势；而且他们的配置都是一致的，只是各自的用途不同而已。 liveness探针存活性检查，主要用于检测pod是否健康；一旦该配置指定的指标未能达标，kubelet即认为该pod不健康，就会试图杀掉该pod，然后重新启动一个副本，从而保障pod永远是“活着”的。 readiness探针就绪检查，主要用于检测某个pod所提供的服务当前是否可用；如果该配置指定的指标未能达标，kubelet就不会将该pod作为service的endpoints，也就意味着，不会将外部的访问流量分配给该pod处理。 实现流程代码实现逻辑首先从核心对象来了解主要的数据结构和封装，然后分别从readiness探测结果的守护go Routine到探针与Pod联动过程来了解探测和结果处理动作。 核心对象如上图所示，实现代码中主要需涉及以下核心对象： probe_manager封装了pod的状态变化与探针联动的所有操作以及probe对应的worker的添加、删除等。 worker封装了操作具体do_probe行为任务；探针探测的主要逻辑就是在该对象中执行。其方法分析如下： newWoker用户初始化出属于readiness或者liveness的worker； run中按照pod.probe.spec.PeriodSeconds中指定的周期，执行worker的w.doProbe操作； doProbe调用w.probeManager.prober.probe来执行探测，然后对结果进行处理。当成功或者失败的次数小于设置的threshold的时候，需要继续执行；否则将结果通过w.resultsManager.Set写到channel中； doProbe经过多层调用，最终命令是通过runProbe函数来执行的；在该函数中，分别支持了exec、http以及TCPsocket类型的调用探测方式。 probe具体探针探测的流程，按照三种探测类型来实现（HTTP/TCP/Command），对应代码细节如下： 1234567891011121314151617181920212223242526272829303132333435363738func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) &#123; timeout := time.Duration(p.TimeoutSeconds) * time.Second if p.Exec != nil &#123; command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) &#125; if p.HTTPGet != nil &#123; scheme := strings.ToLower(string(p.HTTPGet.Scheme)) host := p.HTTPGet.Host if host == \"\" &#123; host = status.PodIP &#125; port, err := extractPort(p.HTTPGet.Port, container) if err != nil &#123; return probe.Unknown, \"\", err &#125; path := p.HTTPGet.Path url := formatURL(scheme, host, port, path) headers := buildHeader(p.HTTPGet.HTTPHeaders) if probeType == liveness &#123; return pb.livenessHttp.Probe(url, headers, timeout) &#125; else &#123; // readiness return pb.readinessHttp.Probe(url, headers, timeout) &#125; &#125; if p.TCPSocket != nil &#123; port, err := extractPort(p.TCPSocket.Port, container) if err != nil &#123; return probe.Unknown, \"\", err &#125; host := p.TCPSocket.Host if host == \"\" &#123; host = status.PodIP &#125; return pb.tcp.Probe(host, port, timeout) &#125; return probe.Unknown, \"\", fmt.Errorf(\"Missing probe handler for %s:%s\", format.Pod(pod), container.Name)&#125; 可以从上面代码中看到，分别有三个判断，分别对应为：p.Exec、p.HTTPGet 和 p.TCPSocket。 启动流程 kubelet通过probe_manager.start()来启动probe服务，该服务只是持续的从readiness channel中获取readiness的结果（所以，不如说这是在启动readiness的服务）。 readinessManager的结果通过调用probeManager.Start()来从channel中将结果获取出来，并通过statusManager写出去。经过一连串的动作，最终到kubelet的主循环逻辑中触发podStatusChannel收消息后的业务逻辑。 Pod联动 当pod被kubelet处理时，如果该pod配置了liveness或者readiness探针规则，probeManager的AddPod方法中会启动分别goRoutine来为readiness和liveness启动各自的worker（这里可以参考”核心对象”节的描述）。 各自的worker都会调用相同的probeManager来执行probe操作；这里两种探针使用的probe是行为是一样的。当基于probeManager获取到探针探测结果后，会调用各自的resultManager来处理结果。这里readiness和liveness各自对结果的处理有些不一样。 readiness将结果写入到livenessManager channel，这就和前面“启动流程”一节分析的一致，只是写入channel后其他流程便由kubelet的主逻辑来处理。 123456func (m *manager) updateReadiness() &#123; update := &lt;-m.readinessManager.Updates() ready := update.Result == results.Success m.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)&#125; livenesslivenessManager的结果，直接在syncLoopIteration中读出来然后执行pod的update操作。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"liveness","slug":"liveness","permalink":"http://ljchen.net/tags/liveness/"},{"name":"readiness","slug":"readiness","permalink":"http://ljchen.net/tags/readiness/"}]},{"title":"Kubelet运行时介绍","slug":"kubelet运行时介绍","date":"2018-11-11T09:13:19.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2018/11/11/kubelet运行时介绍/","link":"","permalink":"http://ljchen.net/2018/11/11/kubelet运行时介绍/","excerpt":"kubernetes的核心竞争力是对各种workload的抽象和编排，在与计算、存储和网络对接中，分别对应衍生了其对接的事实标准CRI、CNI和CSI。今天我们来重点分析kubelet运行时，另外二者在接下来的文章中再做详细的介绍。 kubelet有以下的启动参数用来指定运行时： –container-runtime string The container runtime to use. Possible values: ‘docker’, ‘remote’, ‘rkt(deprecated)’. (default “docker”)–container-runtime-endpoint string [Experimental] The endpoint of remote runtime service. Currently unix socket is supported on Linux, and tcp is supported on windows.","text":"kubernetes的核心竞争力是对各种workload的抽象和编排，在与计算、存储和网络对接中，分别对应衍生了其对接的事实标准CRI、CNI和CSI。今天我们来重点分析kubelet运行时，另外二者在接下来的文章中再做详细的介绍。 kubelet有以下的启动参数用来指定运行时： –container-runtime string The container runtime to use. Possible values: ‘docker’, ‘remote’, ‘rkt(deprecated)’. (default “docker”)–container-runtime-endpoint string [Experimental] The endpoint of remote runtime service. Currently unix socket is supported on Linux, and tcp is supported on windows. 当指定-container-runtime为remote时，对应不会在kubelet启动docker-server，而直接调用CRI接口访问通过--container-runtime-endpoint指定的服务，一般这是一个containerd或者CRIO。 部署架构先从kubelet默认部署的dockershim运行时来分析。下面这张图展现了CRI客户端与服务端、以及服务端与底层服务是如何协调工作的。 客户端和服务端之间基于unix socket套接字/run/dockershim.sock，通过gRPC来通信。 服务端在kubelet源码中，整个服务端的代码都位于dockershim包中。它实现了对container、image、checkpoint、logs以及sandbox的操作，其本质都是调用docker和CNI plugin来实现的。 这里值得特殊提到的是：若只想kubelet启动CRI server，而不启动kubelet其他功能，可以在启动参数中指定--experimental-dockershim参数的值为true。 客户端 crictl我们知道，如今安装kubeadm的时候，都会安装对应依赖包，比如kubelet和crictl等。这里的crictl就是一个独立的客户端工具，当kubelet启动之后，可以基于它来调试CRI server。 为了便于理解cri的各功能，我将crictl的命令提示列出来，后面是对每个命令操作的描述。 12345678910111213141516171819202122232425262728293031323334353637[root@vm ~]# crictl --helpNAME: crictl - client for CRIUSAGE: crictl [global options] command [command options] [arguments...]VERSION: v1.12.0COMMANDS: attach Attach to a running container create Create a new container exec Run a command in a running container version Display runtime version information images List images inspect Display the status of one or more containers inspecti Return the status of one or more images inspectp Display the status of one or more pods logs Fetch the logs of a container port-forward Forward local port to a pod ps List containers pull Pull an image from a registry runp Run a new pod rm Remove one or more containers rmi Remove one or more images rmp Remove one or more pods pods List pods start Start one or more created containers info Display information of the container runtime stop Stop one or more running containers stopp Stop one or more running pods update Update one or more running containers config Get and set crictl options stats List container(s) resource usage statistics completion Output bash shell completion code help, h Shows a list of commands or help for one command kubelet在阅读kubelet源码的时候，有经常看到kubeGenericRuntimeManager这个类。它在kubelet初始化的时候被创建，是整个kubelet的runtime；一旦kubelet需要容器底层操作资源时，就会调动这个接口方法来处理。 这两个客户端都是通过gRPC来访问CRI server的，所以当使用crictl的时候，需要保障其版本与kubelet的版本一致性，否则可能protobuf定义的报文格式有差异，导致无法正常访问。 服务端服务端是通过dockershim包中的dockerService来实现的。它实现了CRI的所有接口（pod对容器运行时的需求），在这些接口的handler中包含了抽象的业务逻辑，包括网络、日志、流式读写、镜像管理等。 在这些抽象之下，最终依然是调用容器和CNI接口实现的功能。 使用dockerlib库，基于/run/docker.sock来访问docker engine； 从/etc/cni/*下查询pod所使用的CNI网络类型，然后调用/opt/cni/*下的CNI插件来为pod添加、卸载网络。 源码分析kubelet中，对容器运行时的源码主要包含以下几块（结合下图来分析）： 服务端（dockershim包中）dockershim.NewDockerService创建出dockerService类，它实现了CRI的所有接口，对应代码在action指定的文件中实现。同时，该类的属性包含了CNI、streamServicer、containerManager、checkPointHandler等，分别为CRI接口提供各个方面的功能。 启动服务端（kubelet中）图中上半部分是kubelet启动的时候，指定了--experimental-dockershim=true参数后，只启动CRI server的流程；而下半部分是完整的kubelet启动后的流程。 protobuf接口定义（api）也就是定义CRI的方法和消息结构的定义了，主要位于kubernetes项目的/pkg/kubelet/apis/cri/目录下。 客户端kubelet的客户端代码其实就是调用gRPC的接口了，主要位于GenericRuntimeManager中。在上图中可以看到，kubelet会通过klet.runtime调用操作pod各种资源的方法。 演变方向从基于dockerd到基于containerd，最后希望直接基于OCI标准，K8S试图取消中间所有环节，直接到达RunC。当然，k8s必须提供直接调试底层runC的工具来取代docker，否则kubelet已挂，整个机器就瘫痪了。当前crictl就是一个比较好的工具！ ContainerdCRI-Containerd在containerd早期的版本中，对CRI的支持是通过外部适配服务来实现的；自v1.1之后，containerd的势力逐渐往外扩展，将CRI作为plugin来运行，该plugin意图替代dockershim，除了提供CRI的支持之外，还提供对CNI的调用。 下图是containerd的架构图。从北向看，containerd在保留其对原有API的情况下，还对外向k8s提供了CRI API。从南向看，containerd除了支持runc外，也支持runhcs和kata等。 containerd的配置文件/etc/containerd/config.toml, 可以看到，其中包含了关于CNI的配置信息。 12345678910111213141516171819202122232425262728[plugins] [plugins.cri] [plugins.cri.containerd] [plugins.cri.containerd.default_runtime] [plugins.cri.cni] # bin_dir is the directory in which the binaries for the plugin is kept. bin_dir = \"/opt/cni/bin\" # conf_dir is the directory in which the admin places a CNI conf. conf_dir = \"/etc/cni/net.d\" # max_conf_num specifies the maximum number of CNI plugin config files to # load from the CNI config directory. By default, only 1 CNI plugin config # file will be loaded. If you want to load multiple CNI plugin config files # set max_conf_num to the number desired. Setting max_config_num to 0 is # interpreted as no limit is desired and will result in all CNI plugin # config files being loaded from the CNI config directory. max_conf_num = 1 # conf_template is the file path of golang template used to generate # cni config. # If this is set, containerd will generate a cni config file from the # template. Otherwise, containerd will wait for the system admin or cni # daemon to drop the config file into the conf_dir. # This is a temporary backward-compatible solution for kubenet users # who don't have a cni daemonset in production yet. # This will be deprecated when kubenet is deprecated. conf_template = \"\" 现在使用CRI-Containerd来安装kubelet不再需要那么繁琐了，需要安装的，只有以下三个包。在使用docker的时候，其实是docker的安装包帮我们一并安装的以下三个可执行文件。 Binary Name Support OS Architecture containerd seccomp, apparmor, overlay, btrfs linux amd64 containerd-shim overlay, btrfs linux amd64 runc seccomp, apparmor linux amd64 其最终运行之后，相互之间调用的通信是这样的： nvidia-dockerNvidia提供了k8s pod使用GPU的一整套解决方案。运行时方面，nvidia提供了特定的运行时，主要的功能是为了让container访问从node节点上分配的GPU资源。 如上图所示，libnvidia-container被整合进docker的runc中。通过在runc的prestart hook 中调用nvidia-container-runtime-hook来控制GPU。启动容器时，prestart hook会校验环境变量GPU-enabled来校验该容器是否需要启用GPU，一旦确认需要启用，就调用nvidia定制的运行时来启动容器，从而为容器分配limit指定个数的GPU。 环境变量 NVIDIA_VISIBLE_DEVICES : controls which GPUs will be accessible inside the container. By default, all GPUs are accessible to the container.NVIDIA_DRIVER_CAPABILITIES : controls which driver features (e.g. compute, graphics) are exposed to the container.NVIDIA_REQUIRE_* : a logical expression to define the constraints (e.g. minimum CUDA, driver or compute capability) on the configurations supported by the container. 下面是简单使用示例： 123456789101112131415161718192021222324252627282930313233$ sudo docker run -it --runtime=nvidia --shm-size=1g -e NVIDIA_VISIBLE_DEVICES=0,1 --rm nvcr.io/nvidia/pytorch:18.05-py3Copyright (c) 2006 Idiap Research Institute (Samy Bengio)Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)All rights reserved.Various files include modifications (c) NVIDIA CORPORATION. All rights reserved.NVIDIA modifications are covered by the license terms that apply to the underlying project or file.root@45cebefa1480:/workspace# nvidia-smiMon May 28 07:15:39 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 396.26 Driver Version: 396.26 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla V100-SXM2... On | 00000000:00:1B.0 Off | 0 || N/A 39C P0 36W / 300W | 0MiB / 16160MiB | 0% Default |+-------------------------------+----------------------+----------------------+| 1 Tesla V100-SXM2... On | 00000000:00:1C.0 Off | 0 || N/A 38C P0 35W / 300W | 0MiB / 16160MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 在k8s的调度方面，nvidia基于k8s的device plugin来实现了kubelet层GPU资源的上报，通过在pod的spec中指定对应的limit来声明对GPU个数的申请情况。在spec中必须指定limit的值（且必须为整数），reqire的值要么不设置，要么等于limit的值。 katakata因为其底层使用hypervisor的虚拟化技术，在安全和隔离性方面对很多场景都有较强的吸引力，其发展也很不错；下面是kata的适配图。由于篇幅所限，后续再抽个机会，专门写一篇关于kata的文章。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"dockershim","slug":"dockershim","permalink":"http://ljchen.net/tags/dockershim/"},{"name":"CRI","slug":"CRI","permalink":"http://ljchen.net/tags/CRI/"}]},{"title":"CKA考试知识总结-3","slug":"CKA考试知识总结-3","date":"2018-11-07T13:08:51.000Z","updated":"2020-04-06T09:24:01.465Z","comments":true,"path":"2018/11/07/CKA考试知识总结-3/","link":"","permalink":"http://ljchen.net/2018/11/07/CKA考试知识总结-3/","excerpt":"之所以要将这篇很长很长的博文拆分开，是因为站内查询因这篇长文而失效了，另外打开一个页面实在有些卡顿(⊙﹏⊙)b Come on baby! 操起键盘就是干，继续~","text":"之所以要将这篇很长很长的博文拆分开，是因为站内查询因这篇长文而失效了，另外打开一个页面实在有些卡顿(⊙﹏⊙)b Come on baby! 操起键盘就是干，继续~ 复习资料TLS Q: TLS bootstrapping, seehttps://coreos.com/kubernetes/docs/latest/openssl.htmlhttps://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/https://github.com/cloudflare/cfssl 一般证书的申请流程如下： 创建证书认证签名 CA （相当于证书颁发机构的证书和私钥）在现实中，这一步是要收钱的机构，直接忽略。 产生私钥 Key （请求认证方的私钥） 使用已生成的私钥产生证书签名请求CSR （请求认证方基于私钥生成向证书颁发机构所需的证书签名请求文件） 使用CA和CSR产生证书 CRT （使用证书颁发机构的证书、私钥以及请求认证放的证书请求文件生成目标证书） 下载cfssl软件包 访问：https://pkg.cfssl.org 下载：cfssl_linux-amd64 =&gt; cfssl 下载：cfssljson_linux-amd64 =&gt; cfssljson 下载：cfssl-certinfo-linux-arm64 =&gt; cfssl-certinfo 创建证书的流程 创建自签名的CA证书； 使用CA证书、CA私钥、CA的配置文件，以及客户的CSR生成客户的证书； 操作流程 生成default配置文件 12345678# 这一步就是产生CSR和CA的配置模板，方便按需修改root@test-9:~/henry# ./cfssl print-defaults listDefault configurations are available for: config csrroot@test-9:~/henry# ./cfssl print-defaults config &gt; ca-config.jsonroot@test-9:~/henry# ./cfssl print-defaults csr &gt; ca-csr.jsonroot@test-9:~/henry# 配置CA的CSR，为自己生成CERT： 123456789101112131415161718192021222324252627282930root@test-9:~/henry# #下面是修改之后的结果，CSR就是讲清楚你是\"谁\"（相当于是在生成中间认证机构的CSR）root@test-9:~/henry# cat ca-csr.json&#123; \"CN\": \"Chen Leiji CA\", \"key\": &#123; \"algo\": \"ecdsa\", \"size\": 256 &#125;, \"names\": [ &#123; \"C\": \"US\", \"L\": \"CA\", \"ST\": \"San Francisco\", \"O\": \"WISE2C\", \"OU\": \"development\" &#125; ]&#125;root@test-9:~/henry# #这一步是直接使用CSR来签署CA（相当于是在生成中间认证机构的CA）root@test-9:~/henry# ./cfssl gencert -initca ca-csr.json | ./cfssljson -bare ca2017/11/15 13:16:04 [INFO] generating a new CA key and certificate from CSR2017/11/15 13:16:04 [INFO] generate received request2017/11/15 13:16:04 [INFO] received CSR2017/11/15 13:16:04 [INFO] generating key: ecdsa-2562017/11/15 13:16:04 [INFO] encoded CSR2017/11/15 13:16:04 [INFO] signed certificate with serial number 84349438505086023342597169366385715026517648791root@test-9:~/henry# lsca-config.json ca.csr ca-key.pem ca.pem cfssl cfssljson csr.jsonroot@test-9:~/henry# 查看生成的CA证书： 12345678910111213141516171819202122232425262728293031323334353637383940414243root@test-9:~/henry# ./cfssl-certinfo -cert ca.pem&#123;\"subject\": &#123; \"common_name\": \"Chen Leiji CA\", \"country\": \"US\", \"organization\": \"WISE2C\", \"organizational_unit\": \"development\", \"locality\": \"CA\", \"province\": \"San Francisco\", \"names\": [ \"US\", \"San Francisco\", \"CA\", \"WISE2C\", \"development\", \"Chen Leiji CA\" ]&#125;,\"issuer\": &#123; \"common_name\": \"Chen Leiji CA\", \"country\": \"US\", \"organization\": \"WISE2C\", \"organizational_unit\": \"development\", \"locality\": \"CA\", \"province\": \"San Francisco\", \"names\": [ \"US\", \"San Francisco\", \"CA\", \"WISE2C\", \"development\", \"Chen Leiji CA\" ]&#125;,\"serial_number\": \"84349438505086023342597169366385715026517648791\",\"not_before\": \"2017-11-15T05:11:00Z\",\"not_after\": \"2022-11-14T05:11:00Z\",\"sigalg\": \"ECDSAWithSHA256\",\"authority_key_id\": \"D4:54:B3:F5:DF:CA:4A:22:E5:E:99:F0:BE:5A:4E:B:89:3C:DC:53\",\"subject_key_id\": \"D4:54:B3:F5:DF:CA:4A:22:E5:E:99:F0:BE:5A:4E:B:89:3C:DC:53\",\"pem\": \"-----BEGIN CERTIFICATE-----\\nMIICSjCCAfCgAwIBAgIUDsZcEST3fVKpcGgiDP+IvVG1ZZcwCgYIKoZIzj0EAwIw\\ncTELMAkGA1UEBhMCVVMxFjAUBgNVBAgTDVNhbiBGcmFuY2lzY28xCzAJBgNVBAcT\\nAkNBMQ8wDQYDVQQKEwZXSVNFMkMxFDASBgNVBAsTC2RldmVsb3BtZW50MRYwFAYD\\nVQQDEw1DaGVuIExlaWppIENBMB4XDTE3MTExNTA1MTEwMFoXDTIyMTExNDA1MTEw\\nMFowcTELMAkGA1UEBhMCVVMxFjAUBgNVBAgTDVNhbiBGcmFuY2lzY28xCzAJBgNV\\nBAcTAkNBMQ8wDQYDVQQKEwZXSVNFMkMxFDASBgNVBAsTC2RldmVsb3BtZW50MRYw\\nFAYDVQQDEw1DaGVuIExlaWppIENBMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE\\nCaeC0bFStNdMcaWjMWtc0/HyC/VrcuALsa7ie5xE1lB6wNtQE1JlDxQUPbOJvHXh\\nXJ8Lhtp+GKR3nPWiy6+j36NmMGQwDgYDVR0PAQH/BAQDAgEGMBIGA1UdEwEB/wQI\\nMAYBAf8CAQIwHQYDVR0OBBYEFNRUs/Xfykoi5Q6Z8L5aTguJPNxTMB8GA1UdIwQY\\nMBaAFNRUs/Xfykoi5Q6Z8L5aTguJPNxTMAoGCCqGSM49BAMCA0gAMEUCIQCIG5Hx\\n6Pmhj3LT2dRpGGJW3yj3+r9txDjMUH7+CtvJ/AIga5REzrYKSByjSKrMmoa2NPl2\\nIIKQ2hASUgXI3565qdc=\\n-----END CERTIFICATE-----\\n\"&#125;root@test-9:~/henry# 配置CA Profile选项(此处的profiles对应生成客户CA指定的–profile值)： 123456789101112131415161718192021222324252627282930313233343536root@test-9:~/henry# cat ca-config.json&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"168h\" &#125;, \"profiles\": &#123; \"server\": &#123; \"expiry\": \"8760h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] &#125;, \"client\": &#123; \"expiry\": \"8760h\", \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ] &#125;, \"peer\": &#123; \"expiry\": \"8760h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] &#125; &#125; &#125;&#125;root@test-9:~/henry# 修改客户CSR.json: 获取模板文件： 12345678910111213141516171819202122# 这才是需要申请证书的客户提交的CSR，里面有“hosts”信息root@test-9:~/henry# ./cfssl print-defaults csr &#123; \"CN\": \"example.net\", \"hosts\": [ \"example.net\", \"www.example.net\" ], \"key\": &#123; \"algo\": \"ecdsa\", \"size\": 256 &#125;, \"names\": [ &#123; \"C\": \"US\", \"L\": \"CA\", \"ST\": \"San Francisco\" &#125; ]&#125;root@test-9:~/henry# 修改CSR，主要涉及hosts的内容: 1234567891011121314151617181920root@test-9:~/henry# cat csr.json&#123; \"CN\": \"Chen Leiji Server\", \"key\": &#123; \"algo\": \"ecdsa\", \"size\": 256 &#125;, \"hosts\":[ \"www.wise2c.com\" ], \"names\": [ &#123; \"C\": \"US\", \"L\": \"CA\", \"ST\": \"San Francisco\", \"O\": \"WISE2C\", \"OU\": \"development\" &#125; ]&#125; 生成客户证书 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 需要使用证书颁发机构的CA和Key来为其生成CERTroot@test-9:~/henry# ./cfssl gencert -ca=ca.pem -ca-key=ca-key.pem --config=ca-config.json --hostname=\"www.wise2c.com\" --profile=\"server\" csr.json | ./cfssljson -bare server2017/11/15 14:34:07 [INFO] generate received request2017/11/15 14:34:07 [INFO] received CSR2017/11/15 14:34:07 [INFO] generating key: ecdsa-2562017/11/15 14:34:07 [INFO] encoded CSR2017/11/15 14:34:07 [INFO] signed certificate with serial number 408368599847170747880405926931506246283785194006root@test-9:~/henry#root@test-9:~/henry# lsca-config.json ca.csr ca-key.pem ca.pem cfssl cfssl-certinfo cfssljson csr.json server.csr server-key.pem server.pemroot@test-9:~/henry#root@test-9:~/henry# ./cfssl-certinfo -cert server.pem&#123;\"subject\": &#123; \"common_name\": \"Chen Leiji Server\", \"country\": \"US\", \"organization\": \"WISE2C\", \"organizational_unit\": \"development\", \"locality\": \"CA\", \"province\": \"San Francisco\", \"names\": [ \"US\", \"San Francisco\", \"CA\", \"WISE2C\", \"development\", \"Chen Leiji Server\" ]&#125;,\"issuer\": &#123; \"common_name\": \"Chen Leiji CA\", \"country\": \"US\", \"organization\": \"WISE2C\", \"organizational_unit\": \"development\", \"locality\": \"CA\", \"province\": \"San Francisco\", \"names\": [ \"US\", \"San Francisco\", \"CA\", \"WISE2C\", \"development\", \"Chen Leiji CA\" ]&#125;,\"serial_number\": \"408368599847170747880405926931506246283785194006\",\"sans\": [ \"www.wise2c.com\"],\"not_before\": \"2017-11-15T06:29:00Z\",\"not_after\": \"2018-11-15T06:29:00Z\",\"sigalg\": \"ECDSAWithSHA256\",\"authority_key_id\": \"D4:54:B3:F5:DF:CA:4A:22:E5:E:99:F0:BE:5A:4E:B:89:3C:DC:53\",\"subject_key_id\": \"1D:DB:C:A:34:9D:50:98:B0:F7:7D:E5:43:AF:3:D:9E:7D:92:C4\",\"pem\": \"-----BEGIN CERTIFICATE-----\\nMIICeTCCAiCgAwIBAgIUR4fhn28TfjY12WtKZvStTxZMyhYwCgYIKoZIzj0EAwIw\\ncTELMAkGA1UEBhMCVVMxFjAUBgNVBAgTDVNhbiBGcmFuY2lzY28xCzAJBgNVBAcT\\nAkNBMQ8wDQYDVQQKEwZXSVNFMkMxFDASBgNVBAsTC2RldmVsb3BtZW50MRYwFAYD\\nVQQDEw1DaGVuIExlaWppIENBMB4XDTE3MTExNTA2MjkwMFoXDTE4MTExNTA2Mjkw\\nMFowdTELMAkGA1UEBhMCVVMxFjAUBgNVBAgTDVNhbiBGcmFuY2lzY28xCzAJBgNV\\nBAcTAkNBMQ8wDQYDVQQKEwZXSVNFMkMxFDASBgNVBAsTC2RldmVsb3BtZW50MRow\\nGAYDVQQDExFDaGVuIExlaWppIFNlcnZlcjBZMBMGByqGSM49AgEGCCqGSM49AwEH\\nA0IABNS8mQT/DGMqig0Ju4VwcKtJoleoUF/lJokUhGKVudxIDRPMlgfHI7etIOBD\\nPPhgrDdBWMEZHqZ0qLhmvp2v4G6jgZEwgY4wDgYDVR0PAQH/BAQDAgWgMBMGA1Ud\\nJQQMMAoGCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFB3bDAo0nVCY\\nsPd95UOvAw2efZLEMB8GA1UdIwQYMBaAFNRUs/Xfykoi5Q6Z8L5aTguJPNxTMBkG\\nA1UdEQQSMBCCDnd3dy53aXNlMmMuY29tMAoGCCqGSM49BAMCA0cAMEQCIGou6e5c\\nhQR0E3+piwH7VDchIlFUvU3OEttxqPnwYUqSAiBOgjYLgbJH07nf2mYqbegRkmpY\\nwSmMdvZBSHvLyw81lA==\\n-----END CERTIFICATE-----\\n\"&#125;root@test-9:~/henry# 拷贝证书到系统，并更新证书库： 1234chmod 600 *-key.pemcp ~/cfssl/ca.pem /etc/ssl/certs/update-ca-trust Installation Q: Setting up K8s master components with a binaries/from tar balls Also, convert CRT to PEM: openssl x509 -in abc.crt -out abc.pem https://coreos.com/kubernetes/docs/latest/openssl.html https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md https://gist.github.com/mhausenblas/0e09c448517669ef5ece157fd4a5dc4b https://kubernetes.io/docs/getting-started-guides/scratch/ http://alexander.holbreich.org/kubernetes-on-ubuntu/maybe dashboard? https://kubernetes.io/docs/getting-started-guides/binary_release/ http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/ life-cycle Q: 对deployment做rollingUpdate，再滚回来 RollingUpdate (貌似对于deploy限制只能够设置其image、resource、selector、subject来实现) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@dev-7 henry]# kubectl run demo --image=nginx --port=80 --replicas=2 --labels=\"cka=true\"[root@dev-7 henry]#[root@dev-7 henry]# kubectl get deployNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdemo 2 2 2 2 4m[root@dev-7 henry]#[root@dev-7 henry]# kubectl get pod -l cka=trueNAME READY STATUS RESTARTS AGEdemo-2959463917-gbv3r 1/1 Running 0 1mdemo-2959463917-j76m9 1/1 Running 0 1m[root@dev-7 henry]#[root@dev-7 henry]# kubectl set --helpConfigure application resourcesThese commands help you make changes to existing application resources.Available Commands: image Update image of a pod template resources Update resource requests/limits on objects with pod templates selector Set the selector on a resource subject Update User, Group or ServiceAccount in a RoleBinding/ClusterRoleBinding[root@dev-7 henry]#[root@dev-7 henry]# kubectl set image deploy/demo demo=mysql[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout history deploy/demodeployments \"demo\"REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;[root@dev-7 henry]# kubectl rollout history deploy/demo --revison=2deployments \"demo\" with revision #2Pod Template: Labels: cka=true pod-template-hash=2216264665 Containers: demo: Image: mysql Port: 80/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout undo deploy/demo[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout history deploy/demodeployments \"demo\"REVISION CHANGE-CAUSE2 &lt;none&gt;3 &lt;none&gt;[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout history deploy/demo --revision=3deployments \"demo\" with revision #3Pod Template: Labels: cka=true pod-template-hash=1786957899 Containers: demo: Image: nginx Port: 80/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout undo deploy/demo --to-revision=2 一种较保守的做法是先将其锁住，等待操作完成，检查OK了再下发： 123456789101112[root@dev-7 henry]# kubectl rollout pause deploy/demodeployment \"demo\" paused[root@dev-7 henry]#[root@dev-7 henry]# kubectl set image deploy/demo demo=busyboxdeployment \"demo\" image updated[root@dev-7 henry]#[root@dev-7 henry]# kubectl set resources deploy/demo -c=demo --limits=cpu=200m,memory=512Mideployment \"demo\" resource requirements updated[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout resume deploy/demodeployment \"demo\" resumed[root@dev-7 henry]# 除此之外，rollingUpdate还可以通过kubectl apply来实现： 123456789101112[root@dev-7 henry]# kubectl apply -f demo.yaml --recorddeployment \"demo\" configured[root@dev-7 henry]#[root@dev-7 henry]# kubectl rollout history deploy/demodeployments \"demo\"REVISION CHANGE-CAUSE4 &lt;none&gt;5 &lt;none&gt;6 &lt;none&gt;7 &lt;none&gt;8 kubectl apply --filename=demo.yaml --record=true[root@dev-7 henry]# 自动弹性伸缩： 12[root@dev-7 henry]# kubectl autoscale deploy/demo --min=10 --max=15 --cpu-percent=80deployment \"demo\" autoscaled Hook Pod支持两种hook： postStart 在pod启动成功了后调用 preStop 在pod停止之前调用 支持两种hook handler： Exec HTTP 12345678910111213141516apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler &gt; /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] kubectl taint Q: 对node做taint (taint a node) 注意： taint指定的 key:value 与node的label之间没有任何关系！ 在添加taint的时候，需要指定: key=value:effect 在删除taint的时候，不需要指定 value，格式为: key:effect 12345678910111213141516171819202122232425262728293031323334353637383940root@test-9:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-5b444f5b58-dpvzq 1/1 Running 0 2m 10.244.0.7 test-9nginx-5b444f5b58-k6qxp 1/1 Running 0 2m 10.244.0.8 test-9nginx-5b444f5b58-n7prf 1/1 Running 0 2m 10.244.0.9 test-9nginx-5b444f5b58-r4265 1/1 Running 0 2m 10.244.0.11 test-9nginx-5b444f5b58-rs2hn 1/1 Running 0 2m 10.244.0.10 test-9nginx-5b444f5b58-v6r2x 1/1 Running 0 2m 10.244.0.6 test-9root@test-9:~#root@test-9:~# kubectl taint node test-9 taint=true:NoExecutenode \"test-9\" taintedroot@test-9:~#root@test-9:~# kubectl describe node test-9Name: test-9Roles: masterLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=test-9 node-role.kubernetes.io/master=Annotations: flannel.alpha.coreos.com/backend-data=&#123;\"VtepMAC\":\"9a:e5:cf:c9:fb:79\"&#125; flannel.alpha.coreos.com/backend-type=vxlan flannel.alpha.coreos.com/kube-subnet-manager=true flannel.alpha.coreos.com/public-ip=10.144.96.185 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=trueTaints: taint=true:NoExecuteCreationTimestamp: Mon, 13 Nov 2017 20:56:37 +0800root@test-9:~#root@test-9:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-5b444f5b58-2s5dw 1/1 Running 0 28s 10.244.1.24 test-10nginx-5b444f5b58-b6pds 1/1 Running 0 28s 10.244.1.23 test-10nginx-5b444f5b58-cg75j 1/1 Running 0 28s 10.244.1.21 test-10nginx-5b444f5b58-d8nbl 1/1 Running 0 28s 10.244.1.20 test-10nginx-5b444f5b58-pncbm 1/1 Running 0 28s 10.244.1.18 test-10nginx-5b444f5b58-zbc4h 1/1 Running 0 28s 10.244.1.22 test-10root@test-9:~#root@test-9:~# kubectl taint node test-9 taint:NoExecute-node \"test-9\" untaintedroot@test-9:~# Effect支持：NoSchedule/NoExecute/PreferNoSchedule 123kubectl taint nodes node1 key1=value1:NoSchedulekubectl taint nodes node1 key1=value1:NoExecutekubectl taint nodes node1 key2=value2:NoSchedule Tolerations支持： 指定匹配 key/value和effect tolerations: key: “key”operator: “Equal”value: “value”effect: “NoSchedule” 指定 key存在且指定effect tolerations: key: “key”operator: “Exists”effect: “NoSchedule” 只要有任何key存在 tolerations: operator: “Exists” 指定key存在 tolerations: key: “key”operator: “Exists” 代表往node添加taint后，多长时间之内，该pod依然可以存活（时间结束后，将被删除） tolerations: key: “key1”operator: “Equal”value: “value1”effect: “NoExecute”tolerationSeconds: 3600 例子： 12345678910111213141516171819202122232425262728293031323334353637root@test-9:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-5b444f5b58-2s5dw 1/1 Running 0 16m 10.244.1.24 test-10nginx-5b444f5b58-b6pds 1/1 Running 0 16m 10.244.1.23 test-10nginx-5b444f5b58-cg75j 1/1 Running 0 16m 10.244.1.21 test-10nginx-5b444f5b58-d8nbl 1/1 Running 0 16m 10.244.1.20 test-10nginx-5b444f5b58-pncbm 1/1 Running 0 16m 10.244.1.18 test-10nginx-5b444f5b58-zbc4h 1/1 Running 0 16m 10.244.1.22 test-10root@test-9:~#root@test-9:~# kubectl taint node test-9 taint=true:NoExecutenode \"test-9\" taintedroot@test-9:~#root@test-9:~# kubectl edit deploy nginxdeployment \"nginx\" editedroot@test-9:~#root@test-9:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-9bf4c9c69-27r6m 1/1 Running 0 17s 10.244.1.26 test-10nginx-9bf4c9c69-cnjk2 1/1 Running 0 23s 10.244.0.12 test-9nginx-9bf4c9c69-fttrd 1/1 Running 0 23s 10.244.1.25 test-10nginx-9bf4c9c69-jw7w2 1/1 Running 0 11s 10.244.1.27 test-10nginx-9bf4c9c69-s57h2 1/1 Running 0 12s 10.244.0.14 test-9nginx-9bf4c9c69-z8jrn 1/1 Running 0 18s 10.244.0.13 test-9root@test-9:~#root@test-9:~# kubectl get deploy nginx -o yaml | grep tolerations -C 5 dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 tolerations: - operator: Existsstatus: availableReplicas: 6 conditions: - lastTransitionTime: 2017-11-13T13:23:03Zroot@test-9:~# Secret generic 1234567891011121314151617181920212223242526272829root@test-9:~# kubectl create secret generic demo --from-literal=user=chenleji --from-literal=passwd=123secret \"demo\" createdroot@test-9:~#root@test-9:~# kubectl get secretNAME TYPE DATA AGEdefault-token-wgrhs kubernetes.io/service-account-token 3 1hdemo Opaque 2 4sroot@test-9:~#root@test-9:~# kubectl get secret demo -o yamlapiVersion: v1data: passwd: MTIz user: Y2hlbmxlamk=kind: Secretmetadata: creationTimestamp: 2017-11-13T14:12:00Z name: demo namespace: default resourceVersion: \"7108\" selfLink: /api/v1/namespaces/default/secrets/demo uid: 9da9b9f4-c87c-11e7-9401-525400545760type: Opaqueroot@test-9:~#root@test-9:~# echo -n MTIz | base64 --decode123root@test-9:~# echo -n Y2hlbmxlamk= | base64 --decodechenlejiroot@test-9:~#root@test-9:~# TLS 1kubectl create secret tls tls-secret --cert=path/to/tls.cert --key=path/to/tls.key Registry 12kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER--docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL volume mount 未指定挂载的具体文件名： 1234567891011121314151617181920212223242526272829303132root@test-9:~# kubectl get deploy -o yaml | grep volume -C 5 imagePullPolicy: Always name: nginx resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /secret name: secret dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 tolerations: - operator: Exists volumes: - name: secret secret: defaultMode: 420 secretName: demo status:root@test-9:~#root@test-9:~# kubectl exec -ti nginx-557769d5c5-45sdq /bin/bashroot@nginx-557769d5c5-45sdq:/# ls -l /secret/total 0lrwxrwxrwx 1 root root 13 Nov 13 14:23 passwd -&gt; ..data/passwdlrwxrwxrwx 1 root root 11 Nov 13 14:23 user -&gt; ..data/userroot@nginx-557769d5c5-45sdq:/#root@nginx-557769d5c5-45sdq:/# cat /secret/passwd123root@nginx-557769d5c5-45sdq:/# 指定挂载文件名：12345678910111213141516171819202122232425262728293031323334353637383940414243root@test-9:~# kubectl describe secret demoName: demoNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====passwd: 3 bytesuser: 8 bytesroot@test-9:~#root@test-9:~# kubectl get deploy nginx -o yaml | grep volume -C 8 spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /secret name: secret dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 tolerations: - operator: Exists volumes: - name: secret secret: defaultMode: 420 items: - key: user path: haha/xx secretName: demostatus:root@test-9:~#root@nginx-657c6dcd4c-56p5h:/# cat /secret/haha/xxchenlejiroot@nginx-657c6dcd4c-56p5h:/# env 12345678910111213141516171819root@test-9:~# kubectl get deploy nginx -o yaml | grep env -C 6 metadata: creationTimestamp: null labels: demo: \"true\" spec: containers: - env: - name: SECRET_USER valueFrom: secretKeyRef: key: user name: demo image: nginxroot@test-9:~#root@test-9:~# kubectl exec -ti nginx-548c9c4846-dgnbk /bin/bashroot@nginx-548c9c4846-dgnbk:/# env | grep SECRETSECRET_USER=chenlejiroot@nginx-548c9c4846-dgnbk:/# ENV Use Pod Field 123456789101112131415161718192021222324252627root@test-9:~# kubectl get deploy -o yaml | grep env -C 10 maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: demo: \"true\" spec: containers: - env: - name: MY_NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: SECRET_USER valueFrom: secretKeyRef: key: user name: demoroot@test-9:~#root@test-9:~# kubectl exec -ti nginx-f7d4dc847-skb74 /bin/bashroot@nginx-f7d4dc847-skb74:/# env | grep MY_NODEMY_NODE_NAME=test-10root@nginx-f7d4dc847-skb74:/# Use Container Filed 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: Podmetadata: name: dapi-envars-resourcefieldrefspec: containers: - name: test-container image: gcr.io/google_containers/busybox:1.24 command: [ \"sh\", \"-c\"] args: - while true; do echo -en '\\n'; printenv MY_CPU_REQUEST MY_CPU_LIMIT; printenv MY_MEM_REQUEST MY_MEM_LIMIT; sleep 10; done; resources: requests: memory: \"32Mi\" cpu: \"125m\" limits: memory: \"64Mi\" cpu: \"250m\" env: - name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu restartPolicy: Never 结束好吧，这次真没有了！你以为你就可以考过了吗？呵呵~~再好好看看官网的文档吧，另外，最后再附送一份k8s相关的资源大宝典。注意，需要翻墙！","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"CKA","slug":"CKA","permalink":"http://ljchen.net/tags/CKA/"}]},{"title":"CKA考试知识总结-2","slug":"CKA考试知识总结-2","date":"2018-11-07T13:07:51.000Z","updated":"2020-04-06T09:24:01.464Z","comments":true,"path":"2018/11/07/CKA考试知识总结-2/","link":"","permalink":"http://ljchen.net/2018/11/07/CKA考试知识总结-2/","excerpt":"之所以要将这篇很长很长的博文拆分开，是因为站内查询因这篇长文而失效了，另外打开一个页面实在有些卡顿(⊙﹏⊙)b Come on baby! 操起键盘就是干，继续~","text":"之所以要将这篇很长很长的博文拆分开，是因为站内查询因这篇长文而失效了，另外打开一个页面实在有些卡顿(⊙﹏⊙)b Come on baby! 操起键盘就是干，继续~ 复习资料initContainer Q: You have a Container with a volume mount. Add a init container that creates an empty file in the volume. (only trick is to mount the volume to init-container as well)https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: test-pdspec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] volumeMounts: - mountPath: /cache name: cache-volume initContainers: - name: init-touch-file image: busybox volumeMounts: - mountPath: /data name: cache-volume command: ['sh', '-c', 'echo \"\" &gt; /data/harshal.txt'] volumes: - name: cache-volume emptyDir: &#123;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127root@test-9:~/henry# cat init-container.yamlapiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] initContainers: - name: init-baidu image: busybox command: ['sh', '-c', 'until nslookup www.baidu.com; do echo waiting for baidu.com; sleep 2; done;'] - name: init-google image: busybox command: ['sh', '-c', 'until nslookup www.google.com; do echo waiting for google.com; sleep 2; done;']root@test-9:~/henry#root@test-9:~/henry# kubectl get pod -aNAME READY STATUS RESTARTS AGEmyapp-pod 1/1 Running 0 1mnginx2-2627548522-6f5kf 1/1 Running 0 2hnginx2-2627548522-8w87b 1/1 Running 0 2hroot@test-9:~/henry#root@test-9:~/henry# kubectl describe pod myapp-podName: myapp-podNamespace: defaultNode: test-9/10.144.96.185Start Time: Sun, 12 Nov 2017 17:43:49 +0800Labels: app=myappAnnotations: pod.alpha.kubernetes.io/init-container-statuses=[&#123;\"name\":\"init-baidu\",\"state\":&#123;\"terminated\":&#123;\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2017-11-12T09:43:54Z\",\"finishedAt\":\"2017-11-12T09:43:54Z\",\"c... pod.alpha.kubernetes.io/init-containers=[&#123;\"name\":\"init-baidu\",\"image\":\"busybox\",\"command\":[\"sh\",\"-c\",\"until nslookup www.baidu.com; do echo waiting for baidu.com; sleep 2; done;\"],\"resources\":&#123;&#125;,\"volu... pod.beta.kubernetes.io/init-container-statuses=[&#123;\"name\":\"init-baidu\",\"state\":&#123;\"terminated\":&#123;\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2017-11-12T09:43:54Z\",\"finishedAt\":\"2017-11-12T09:43:54Z\",\"co... pod.beta.kubernetes.io/init-containers=[&#123;\"name\":\"init-baidu\",\"image\":\"busybox\",\"command\":[\"sh\",\"-c\",\"until nslookup www.baidu.com; do echo waiting for baidu.com; sleep 2; done;\"],\"resources\":&#123;&#125;,\"volum...Status: RunningIP: 10.42.107.11Init Containers: init-baidu: Container ID: docker://9497c4dc7c111870022e5dd873daba13f00797308b505f6e82fd1f1545744062 Image: busybox Image ID: docker-pullable://busybox@sha256:bbc3a03235220b170ba48a157dd097dd1379299370e1ed99ce976df0355d24f0 Port: &lt;none&gt; Command: sh -c until nslookup www.baidu.com; do echo waiting for baidu.com; sleep 2; done; State: Terminated Reason: Completed Exit Code: 0 Started: Sun, 12 Nov 2017 17:43:54 +0800 Finished: Sun, 12 Nov 2017 17:43:54 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qfpj (ro) init-google: Container ID: docker://5ff45db07f52c51e40b0bb77ad650aa4fbd29aa7112a4197de33ed880a04376d Image: busybox Image ID: docker-pullable://busybox@sha256:bbc3a03235220b170ba48a157dd097dd1379299370e1ed99ce976df0355d24f0 Port: &lt;none&gt; Command: sh -c until nslookup www.google.com; do echo waiting for google.com; sleep 2; done; State: Terminated Reason: Completed Exit Code: 0 Started: Sun, 12 Nov 2017 17:43:59 +0800 Finished: Sun, 12 Nov 2017 17:43:59 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qfpj (ro)Containers: myapp-container: Container ID: docker://88cf1ddb39e7b468d9d06c37a7d3ff1ca0d39ae9b0f46d0cf2f1788cb1482118 Image: busybox Image ID: docker-pullable://busybox@sha256:bbc3a03235220b170ba48a157dd097dd1379299370e1ed99ce976df0355d24f0 Port: &lt;none&gt; Command: sh -c echo The app is running! &amp;&amp; sleep 3600 State: Running Started: Sun, 12 Nov 2017 17:44:04 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5qfpj (ro)Conditions: Type Status Initialized True Ready True PodScheduled TrueVolumes: default-token-5qfpj: Type: Secret (a volume populated by a Secret) SecretName: default-token-5qfpj Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.alpha.kubernetes.io/notReady:NoExecute for 300s node.alpha.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned myapp-pod to test-9 Normal SuccessfulMountVolume 1m kubelet, test-9 MountVolume.SetUp succeeded for volume \"default-token-5qfpj\" Normal Pulling 1m kubelet, test-9 pulling image \"busybox\" Normal Pulled 1m kubelet, test-9 Successfully pulled image \"busybox\" Normal Created 1m kubelet, test-9 Created container Normal Started 1m kubelet, test-9 Started container Normal Pulling 1m kubelet, test-9 pulling image \"busybox\" Normal Pulled 1m kubelet, test-9 Successfully pulled image \"busybox\" Normal Created 1m kubelet, test-9 Created container Normal Started 1m kubelet, test-9 Started container Normal Pulling 1m kubelet, test-9 pulling image \"busybox\" Normal Pulled 1m kubelet, test-9 Successfully pulled image \"busybox\" Normal Created 1m kubelet, test-9 Created container Normal Started 1m kubelet, test-9 Started containerroot@test-9:~/henry# Volume Q: When running a redis key-value store in your pre-production environments many deployments are incoming from CI and leaving behind a lot of stale cache data in redis which is causing test failures. The CI admin has requested that each time a redis key-value-store is deployed in staging that it not persist its data.Create a pod named non-persistent-redis that specifies a named-volume with name app-cache, and mount path /data/redis. It should launch in the staging namespace and the volume MUST NOT be persistent.Create a Pod with EmptyDir and in the YAML file add namespace: CI Yaml格式123456789101112131415apiVersion: v1kind: Podmetadata: name: test-pdspec: containers: - image: gcr.io/google_containers/busybox:latest name: test-container command: [\"/bin/sh\", \"-c\", \"sleep 9999\"] volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 挂载文件到pod中：12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: v1items:- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: io.wise2c.service: xx io.wise2c.stack: stack001 name: stack001-xx spec: replicas: 1 template: metadata: labels: io.wise2c.service: xx io.wise2c.stack: stack001 spec: containers: image: nginx name: xx resources: limits: cpu: 200m memory: 1073741824 volumeMounts: - mountPath: /etc/resolv.conf name: xx subPath: resolv.conf volumes: - configMap: name: stack001-xx name: xx- apiVersion: v1 data: resolv.conf: \"\\nnameserver 10.96.0.10 \\n\\nsearch stack001.ns-team-2-env-44.svc.cluster.local\\ \\ ns-team-2-env-44.svc.cluster.local svc.cluster.local cluster.local\\noptions\\ \\ ndots:6\" kind: ConfigMap metadata: labels: io.wise2c.stack: stack001 name: stack001-xxkind: List 挂载同一个文件到不同pod中，指定不同的名字： 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: my-lamp-sitespec: containers: - name: mysql image: busybox command: [\"/bin/sh\", \"-c\", \"sleep 999\"] volumeMounts: - mountPath: /haha/mysql name: site-data subPath: mysql - name: php image: busybox command: [\"/bin/sh\", \"-c\", \"sleep 999\"] volumeMounts: - mountPath: /haha/html name: site-data subPath: html volumes: - name: site-data hostPath: path: /data 两种类型的持久卷 PV, 使用静态的PV来挂载，需要用户自己创建PV. 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata:name: pv0003spec:capacity: storage: 5GiaccessModes: - ReadWriteOncepersistentVolumeReclaimPolicy: RecyclestorageClassName: slownfs: path: /tmp server: 172.17.0.2 PVC, 用户不用关心PV，只需要说需要什么类型的存储，即创建PVC，然后PVC自动从Storage Class中创建对应的PV。 12345678910111213141516kind: PersistentVolumeClaimapiVersion: v1metadata: name: myclaimspec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: gold selector: matchLabels: release: \"stable\" matchExpressions: - &#123;key: environment, operator: In, values: [dev]&#125; Storage Class: 12345678kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: goldprovisioner: kubernetes.io/cinderparameters: type: fast availability: nova Pod: 123456789101112131415kind: PodapiVersion: v1metadata: name: mypodspec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \"/var/www/html\" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim Log Q: Find the error message with the string “Some-error message here”.https://kubernetes.io/docs/concepts/cluster-administration/logging/see kubectl logs and /var/log for system services 12[root@dev-7 henry]# kcc logs -f --tail=10 orchestration-2080965958-khwfx -c orchestration[root@dev-7 henry]# kcc logs -f --since=1h orchestration-2080965958-khwfx -c orchestration kubelet日志位于/var/log/kubelet下 static pod Q: Run a Jenkins Pod on a specified node only.https://kubernetes.io/docs/tasks/administer-cluster/static-pod/Create the Pod manifest at the specified location and then edit the systemd service file for kubelet(/etc/systemd/system/kubelet.service) to include --pod-manifest-path=/specified/path. Once done restart the service. Choose a node where we want to run the static pod. In this example, it’s my-node1. 1[joe@host ~] $ ssh my-node1 Choose a directory, say /etc/kubelet.d and place a web server pod definition there, e.g. /etc/kubelet.d/static-pod.yaml: 123456789101112[root@my-node1 ~] $ mkdir /etc/kubernetes.d/ [root@my-node1 ~] $ cat &lt;&lt;EOF &gt;/etc/kubernetes.d/static-pod.yaml apiVersion: v1kind: Podmetadata: name: static-podspec: containers: - image: busybox name: test-container command: [\"/bin/sh\", \"-c\", \"sleep 9999\"]EOF Configure your kubelet daemon on the node to use this directory by running it with --pod-manifest-path=/etc/kubelet.d/ argument. On Fedora edit /etc/kubernetes/kubelet to include this line: 1KUBELET_ARGS=\"--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/\" Instructions for other distributions or Kubernetes installations may vary. Restart kubelet. On Fedora, this is: 1[root@my-node1 ~] $ systemctl restart kubelet 效果如下：12345678910111213141516171819202122232425262728293031323334353637383940414243[root@dev-9 manifests]# kubectl get podNAME READY STATUS RESTARTS AGEstatic-pod-dev-9 1/1 Running 0 34s[root@dev-9 manifests]#[root@dev-9 manifests]# kubectl describe pod static-pod-dev-9Name: static-pod-dev-9Namespace: defaultNode: dev-9/192.168.1.190Start Time: Sun, 12 Nov 2017 21:21:48 +0800Labels: &lt;none&gt;Annotations: kubernetes.io/config.hash=1dcad4affd910f45b5c3a8dbdeec8933 kubernetes.io/config.mirror=1dcad4affd910f45b5c3a8dbdeec8933 kubernetes.io/config.seen=2017-11-12T21:21:48.15196949+08:00 kubernetes.io/config.source=fileStatus: RunningIP: 10.244.3.45Containers: test-container: Container ID: docker://ef3e28e45e280e4a50942fc472fd025cb84a7014a64dbc57308cddbfeb1bd979 Image: busybox Image ID: docker-pullable://busybox@sha256:bbc3a03235220b170ba48a157dd097dd1379299370e1ed99ce976df0355d24f0 Port: &lt;none&gt; Command: /bin/sh -c sleep 9999 State: Running Started: Sun, 12 Nov 2017 21:21:52 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: &lt;none&gt;Conditions: Type Status Initialized True Ready True PodScheduled TrueVolumes: &lt;none&gt;QoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: :NoExecuteEvents: &lt;none&gt;[root@dev-9 manifests]# DNS Q: Use the utility nslookup to look up the DNS records of the service and pod.From this guide, https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/Look for “Quick Diagnosis” Services1$ kubectl exec -ti busybox -- nslookup mysvc.myns.svc.cluster.local Naming conventions for services and pods: For a regular service, this resolves to the port number and the CNAME: (解析到Cluster-IP) my-svc.my-namespace.svc.cluster.local. 1234567root@test-9:~/henry# kubectl exec -ti busybox-2520568787-kkmrw -- nslookup nginx.default.svc.cluster.localServer: 10.43.0.10Address 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.localName: nginx.defaultAddress 1: 10.43.120.19 nginx.default.svc.cluster.localroot@test-9:~/henry# For a headless service, this resolves to multiple answers（RR解析到多个Pod IP), one for each pod that is backing the service, and contains the port number and a CNAME of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local PodsWhen enabled, pods are assigned a DNS A record in the form of pod-ip-address.my-namespace.pod.cluster.local For example, a pod with IP 1.2.3.4 in the namespace default with a DNS name of cluster.local would have an entry: 1-2-3-4.default.pod.cluster.local 1234567root@test-9:~/henry# kubectl exec -ti busybox-2520568787-kkmrw -- nslookup 10-42-236-215.default.pod.cluster.localServer: 10.43.0.10Address 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.localName: 10-42-236-215.default.pod.cluster.localAddress 1: 10.42.236.215root@test-9:~/henry# Ingress Q 17: Create an Ingress resource, Ingress controller and a Service that resolves to cs.rocks.ch. First, create controller and default backend12kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/nginx/examples/default-backend.yamlkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml Second, create service and expose 12kubectl run ingress-pod --image=nginx --port 80kubectl expose deployment ingress-pod --port=80 --target-port=80 --type=NodePort Create the ingress 1234567891011121314cat &lt;&lt;EOF &gt;ingress-cka.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-servicespec: rules: - host: \"cs.rocks.ch\" http: paths: - backend: serviceName: ingress-pod servicePort: 80EOF To test, run a curl pod 12kubectl run -i --tty client --image=tutum/curlcurl -I -L --resolve cs.rocks.ch:80:10.240.0.5 http://cs.rocks.ch/ 我认为，要访问ingress，在flannel网络中，应该还可以使用hostPort来暴露出ingress-nginx的80和443端口。 Mandatory commands 123456789curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml | kubectl apply -f -curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml | kubectl apply -f -curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml | kubectl apply -f -curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml | kubectl apply -f -curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml | kubectl apply -f - Install with RBAC roles 123curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml | kubectl apply -f -curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml | kubectl apply -f - Verify installation: 1kubectl get pods --all-namespaces -l app=ingress-nginx --watch 接下来还有，请抽根烟继续！","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"CKA","slug":"CKA","permalink":"http://ljchen.net/tags/CKA/"}]},{"title":"CKA考试知识总结-1","slug":"CKA考试知识总结","date":"2018-11-07T13:06:51.000Z","updated":"2020-04-06T09:24:01.465Z","comments":true,"path":"2018/11/07/CKA考试知识总结/","link":"","permalink":"http://ljchen.net/2018/11/07/CKA考试知识总结/","excerpt":"这是一篇很长很长的文章，是去年CKA考试刚开始推出来的时候，我参与考试复习做过的一些知识点。基于做过的题，大概列出了具体的知识点，当时考试的时候还在使用v1.7版本，现在应该都要到v1.12了。 补充：这周收到CNCF的邮件，说是之前认证两年到期的CKA证书又延长了一年了（感叹：这是为了诱惑大家都来考证吗^_^）。","text":"这是一篇很长很长的文章，是去年CKA考试刚开始推出来的时候，我参与考试复习做过的一些知识点。基于做过的题，大概列出了具体的知识点，当时考试的时候还在使用v1.7版本，现在应该都要到v1.12了。 补充：这周收到CNCF的邮件，说是之前认证两年到期的CKA证书又延长了一年了（感叹：这是为了诱惑大家都来考证吗^_^）。 CKA证书随着k8s声名大噪，国内一大堆公司推各种高价的包过培训班；我只想说：CNCF还真有些缺乏社区精神，更多的还是商业模式。但是，只要能够推动整个云原生的发展，随它吧~ 为了让本文显得有说服力，我也把证书贴出来炫炫（认证ID末尾是0100，照理说刚好是第100位通过考试的），勿拍砖！ 复习资料废话不多讲，现在进入主题。 Job Q: Create a Job that run 60 time with 2 jobs running in parallel 参考资料：https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ yaml1234567891011121314151617apiVersion: batch/v1kind: Jobmetadata: name: pispec: completions: 10 parallelism: 2 activeDeadlineSeconds: 2 template: metadata: name: pi spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never 并行job Job类型 使用示例 行为 completions Parallelism 备注 一次性Job 数据库迁移 创建一个Pod直至其成功结束 1 1 固定结束次数的Job 处理工作队列的Pod 依次创建一个Pod运行直至completions个成功结束 2+ 1 固定结束次数的并行Job 多个Pod同时处理工作队列 依次创建多个Pod运行直至completions个成功结束 2+ 2+ 并行Job 多个Pod同时处理工作队列 创建一个或多个Pod直至有一个成功结束 1 2+ 不会创建多个，直接创建出一个 kubectl scale jobA job can be scaled up using the kubectl scale command. For example, the following command sets .spec.parallelism of a job called myjob to 10: 12$ kubectl scale --replicas=10 jobs/myjobjob \"myjob\" scaled 注意 parallelism: 表示并行执行的数量； completions：表示成功运行多少次就结束job； RestartPolicy仅支持Never或OnFailure； activeDeadlineSeconds标志失败Pod的重试最大时间，超过这个时间不会继续重试； kubectl scale其实是修改了job的parallelism属性，并不会对completetions产生影响。 Cronjobcron 表达式格式: 如果某一位为*/5 就表示每隔5x； 比如在min位的话，代表每隔5分钟 12345678910111213141516root@test-9:~# kubectl run cronjob --schedule=\"*/1 * * * *\" --restart=OnFailure --image=busybox -- /bin/sh -c \"sleep 99\"cronjob \"cronjob\" createdroot@test-9:~#root@test-9:~# kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEcronjob */1 * * * * False 0 &lt;none&gt;root@test-9:~#root@test-9:~# kubectl get jobNAME DESIRED SUCCESSFUL AGEcronjob-1510581480 1 0 1mcronjob-1510581540 1 0 14sroot@test-9:~#root@test-9:~# kubectl get podNAME READY STATUS RESTARTS AGEcronjob-1510581480-r49rq 1/1 Running 0 1mcronjob-1510581540-tl4hn 1/1 Running 0 16s kubectl top Q: Find which Pod is taking max CPUUse kubectl top to find CPU usage per pod kubectl top node12345678910root@test-9:~/henry# kubectl top nodesNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%test-10 41m 1% 2230Mi 14%test-9 104m 2% 4931Mi 31%root@test-9:~/henry#root@test-9:~/henry#root@test-9:~/henry# kubectl top nodes | awk '&#123;print $1 \"\\t\" $3|\"sort -r -n\"&#125;'test-9 2%test-10 1%NAME CPU% sort的参数：-r 表示反序排列； -n 表示按照数字排序awk print的时候，使用”\\t” 来区分两个列，同时，使用管道来排序 输出排序 Q: List all PersistentVolumes sorted by their nameUse kubectl get pv --sort-by= &lt;- this problem is buggy &amp; also by default kubectl give the output sorted by name. 排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354root@test-9:~/henry# kcs get svc --sort-by=.metadata.uidNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtiller-deploy ClusterIP 10.43.155.15 &lt;none&gt; 44134/TCP 2hmonitoring-influxdb ClusterIP 10.43.227.43 &lt;none&gt; 8086/TCP 2hmonitoring-grafana ClusterIP 10.43.217.185 &lt;none&gt; 80/TCP 2hkube-dns ClusterIP 10.43.0.10 &lt;none&gt; 53/UDP,53/TCP 2hkubernetes-dashboard ClusterIP 10.43.36.245 &lt;none&gt; 9090/TCP 2hheapster ClusterIP 10.43.250.217 &lt;none&gt; 80/TCP 2hroot@test-9:~/henry#root@test-9:~/henry# kcs get svc --sort-by=.metadata.nameNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheapster ClusterIP 10.43.250.217 &lt;none&gt; 80/TCP 2hkube-dns ClusterIP 10.43.0.10 &lt;none&gt; 53/UDP,53/TCP 2hkubernetes-dashboard ClusterIP 10.43.36.245 &lt;none&gt; 9090/TCP 2hmonitoring-grafana ClusterIP 10.43.217.185 &lt;none&gt; 80/TCP 2hmonitoring-influxdb ClusterIP 10.43.227.43 &lt;none&gt; 8086/TCP 2htiller-deploy ClusterIP 10.43.155.15 &lt;none&gt; 44134/TCP 2hroot@test-9:~/henry# kcs get svc heapster -o json&#123; \"apiVersion\": \"v1\", \"kind\": \"Service\", \"metadata\": &#123; \"creationTimestamp\": \"2017-11-12T03:27:51Z\", \"labels\": &#123; \"kubernetes.io/cluster-service\": \"true\", \"kubernetes.io/name\": \"Heapster\", \"task\": \"monitoring\" &#125;, \"name\": \"heapster\", \"namespace\": \"kube-system\", \"resourceVersion\": \"229\", \"selfLink\": \"/api/v1/namespaces/kube-system/services/heapster\", \"uid\": \"769529c5-c759-11e7-8dee-02cdc7a8bd69\" &#125;, \"spec\": &#123; \"clusterIP\": \"10.43.250.217\", \"ports\": [ &#123; \"port\": 80, \"protocol\": \"TCP\", \"targetPort\": 8082 &#125; ], \"selector\": &#123; \"k8s-app\": \"heapster\" &#125;, \"sessionAffinity\": \"None\", \"type\": \"ClusterIP\" &#125;, \"status\": &#123; \"loadBalancer\": &#123;&#125; &#125;&#125; 查询资源12345678910111213141516171819202122232425262728293031323334# Get commands with basic output$ kubectl get services # List all services in the namespace$ kubectl get pods --all-namespaces # List all pods in all namespaces$ kubectl get pods -o wide # List all pods in the namespace, with more details$ kubectl get deployment my-dep # List a particular deployment$ kubectl get pods --include-uninitialized # List all pods in the namespace, including uninitialized ones# Describe commands with verbose output$ kubectl describe nodes my-node$ kubectl describe pods my-pod$ kubectl get services --sort-by=.metadata.name # List Services Sorted by Name# List pods Sorted by Restart Count$ kubectl get pods --sort-by='.status.containerStatuses[0].restartCount'# Get the version label of all pods with label app=cassandra$ kubectl get pods --selector=app=cassandra rc -o \\ jsonpath='&#123;.items[*].metadata.labels.version&#125;'# Get ExternalIPs of all nodes$ kubectl get nodes -o jsonpath='&#123;.items[*].status.addresses[?(@.type==\"ExternalIP\")].address&#125;'# List Names of Pods that belong to Particular RC# \"jq\" command useful for transformations that are too complex for jsonpath, it can be found at https://stedolan.github.io/jq/$ sel=$&#123;$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?&#125;$ echo $(kubectl get pods --selector=$sel --output=jsonpath=&#123;.items..metadata.name&#125;)# Check which nodes are ready$ JSONPATH='&#123;range .items[*]&#125;&#123;@.metadata.name&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;=&#123;@.status&#125;;&#123;end&#125;&#123;end&#125;' \\ &amp;&amp; kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\"# List all Secrets currently in use by a pod$ kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq 常用命令kubectl run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677root@test-9:~# kubectl run demo-1 --image=busybox:latest --env=\"env1=wise2c\" --port=80 --hostport=30098 --restart='Always' --image-pull-policy='Always' --limits=\"cpu=200m,memory=512Mi\" --replicas=2 -- sleep 60deployment \"demo-1\" createdroot@test-9:~#root@test-9:~# kubectl get podNAME READY STATUS RESTARTS AGEdemo-1-4031462666-1m6lc 0/1 ContainerCreating 0 4sdemo-1-4031462666-3sph3 0/1 ContainerCreating 0 4sroot@test-9:~#root@test-9:~# kubectl get deploy demo-1 -o yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: 2017-11-12T06:20:52Z generation: 1 labels: run: demo-1 name: demo-1 namespace: default resourceVersion: \"13667\" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/demo-1 uid: a24a6b2b-c771-11e7-8dee-02cdc7a8bd69spec: replicas: 2 selector: matchLabels: run: demo-1 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: run: demo-1 spec: containers: - args: - sleep - \"60\" env: - name: env1 value: wise2c image: busybox:latest imagePullPolicy: Always name: demo-1 ports: - containerPort: 80 hostPort: 30098 protocol: TCP resources: limits: cpu: 200m memory: 512Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30status: availableReplicas: 2 conditions: - lastTransitionTime: 2017-11-12T06:22:03Z lastUpdateTime: 2017-11-12T06:22:03Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available observedGeneration: 1 readyReplicas: 2 replicas: 2 updatedReplicas: 2 kubectl expose12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758root@test-9:~# kubectl expose deploy nginx2 --name=nginx --port=80 --target-port=80 --protocol=TCP --type=ClusterIPservice \"nginx\" exposedroot@test-9:~# kubectl get svc nginx -o yamlapiVersion: v1kind: Servicemetadata: creationTimestamp: 2017-11-12T07:32:48Z labels: run: nginx2 name: nginx namespace: default resourceVersion: \"20097\" selfLink: /api/v1/namespaces/default/services/nginx uid: ae7774d4-c77b-11e7-8dee-02cdc7a8bd69spec: clusterIP: 10.43.221.216 ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx2 sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; root@test-9:~#root@test-9:~#root@test-9:~#root@test-9:~#root@test-9:~# kubectl expose deploy nginx2 --name=nginx --port=80 --target-port=80 --protocol=TCP --type=NodePortservice \"nginx\" exposedroot@test-9:~# kubectl get svc nginx -o yamlapiVersion: v1kind: Servicemetadata: creationTimestamp: 2017-11-12T07:35:21Z labels: run: nginx2 name: nginx namespace: default resourceVersion: \"20296\" selfLink: /api/v1/namespaces/default/services/nginx uid: 0a19d690-c77c-11e7-8dee-02cdc7a8bd69spec: clusterIP: 10.43.120.19 externalTrafficPolicy: Cluster ports: - nodePort: 30014 port: 80 protocol: TCP targetPort: 80 selector: run: nginx2 sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125; port-forward12345678910111213141516171819202122232425262728root@test-9:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEnginx-f7d4dc847-bzlzq 1/1 Running 0 11h 10.244.0.24 test-9nginx-f7d4dc847-lcq57 1/1 Running 0 11h 10.244.1.45 test-10nginx-f7d4dc847-qs28j 1/1 Running 0 11h 10.244.0.25 test-9nginx-f7d4dc847-s4xml 1/1 Running 0 11h 10.244.1.44 test-10nginx-f7d4dc847-skb74 1/1 Running 0 11h 10.244.1.43 test-10nginx-f7d4dc847-x9vh4 1/1 Running 0 11h 10.244.0.26 test-9root@test-9:~# kubectl port-forward nginx-f7d4dc847-bzlzq 9090:80Forwarding from 127.0.0.1:9090 -&gt; 80Handling connection for 9090root@test-9:~#root@test-9:~# curl 127.0.0.1:9090&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;...&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;root@test-9:~# NetworkPolicy Q: Create a NetworkPolicy to allow connect to port 8080 by busybox pod onlyhttps://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/Make sure to use apiVersion: extensions/v1beta1 which works on both 1.6 and 1.7 在生效之前，必须先配置annotation来阻止所有的请求； podSelector.matchLablesl：定义了该规则对哪些pod（destination）有效； ingress：指定了允许带标签“access=true” 的pod访问这些服务； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172root@test-9:~# kubectl annotate ns default \"net.beta.kubernetes.io/network-policy=&#123;\\\"ingress\\\": &#123;\\\"isolation\\\": \\\"DefaultDeny\\\"&#125;&#125;\"namespace \"default\" annotatedroot@test-9:~#root@test-9:~#root@test-9:~# kubectl describe ns defaultName: defaultLabels: &lt;none&gt;Annotations: net.beta.kubernetes.io/network-policy=&#123;\"ingress\": &#123;\"isolation\": \"DefaultDeny\"&#125;&#125;Status: ActiveNo resource quota.No resource limits.root@test-9:~#root@test-9:~/henry# kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx2-2627548522-6f5kf 1/1 Running 0 22m pod-template-hash=2627548522,run=nginxnginx2-2627548522-8w87b 1/1 Running 0 22m pod-template-hash=2627548522,run=nginxroot@test-9:~/henry# kubectl get svc nginx --show-labelsNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE LABELSnginx NodePort 10.43.120.19 &lt;none&gt; 80:30014/TCP 16m run=nginxroot@test-9:~/henry# cat network-policy.yamlkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginxspec: podSelector: matchLabels: run: nginx ingress: - from: - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: access: \"true\" ports: - protocol: TCP port: 80root@test-9:~/henry# kubectl get netpolNAME POD-SELECTOR AGEaccess-nginx run=nginx 2mroot@test-9:~/henry# kubectl get netpol access-nginx -o yamlapiVersion: extensions/v1beta1kind: NetworkPolicymetadata: creationTimestamp: 2017-11-12T07:40:38Z generation: 1 name: access-nginx namespace: default resourceVersion: \"20699\" selfLink: /apis/extensions/v1beta1/namespaces/default/networkpolicies/access-nginx uid: c72191d1-c77c-11e7-8dee-02cdc7a8bd69spec: ingress: - from: - podSelector: matchLabels: access: \"true\" ports: - port: 80 protocol: TCP podSelector: matchLabels: run: nginxroot@test-9:~/henry#root@test-9:~/henry# kubectl run busybox --rm -ti --labels=\"access=true\" --image=busybox /bin/shIf you don't see a command prompt, try pressing enter./ # wget nginxConnecting to nginx (10.43.120.19:80)index.html 100% |********************************************************************************************| 612 0:00:00 ETA/ # Node Broken Q: fixing broken nodes, seehttps://kubernetes.io/docs/concepts/architecture/nodes/ Node参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root@test-9:~# kubectl describe nodesName: test-10Roles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=rancher beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=Region1 failure-domain.beta.kubernetes.io/zone=FailureDomain1 io.rancher.host.docker_version=1.12 io.rancher.host.linux_kernel_version=4.4 kubernetes.io/hostname=test-10Annotations: io.rancher.labels.io.rancher.host.docker_version= io.rancher.labels.io.rancher.host.linux_kernel_version= node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=trueTaints: &lt;none&gt;CreationTimestamp: Sun, 12 Nov 2017 11:27:45 +0800Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Sun, 12 Nov 2017 15:16:39 +0800 Sun, 12 Nov 2017 11:27:45 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Sun, 12 Nov 2017 15:16:39 +0800 Sun, 12 Nov 2017 11:27:45 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sun, 12 Nov 2017 15:16:39 +0800 Sun, 12 Nov 2017 11:27:45 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Sun, 12 Nov 2017 15:16:39 +0800 Sun, 12 Nov 2017 11:27:45 +0800 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 10.144.102.117 ExternalIP: 10.144.102.117 Hostname: test-10Capacity: cpu: 4 memory: 16301460Ki pods: 110Allocatable: cpu: 4 memory: 16199060Ki pods: 110System Info: Machine ID: System UUID: 4ABB25CA-B353-450A-9787-28477ED72344 Boot ID: 689e31dc-e05d-48de-9068-e8460d15a9b6 Kernel Version: 4.4.0-91-generic OS Image: Ubuntu 16.04.1 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.12.6 Kubelet Version: v1.7.7-rancher1 Kube-Proxy Version: v1.7.7-rancher1ExternalID: 3cb02e3d-cb58-42c6-9a54-2fb5cfb836d2Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default demo-1-4031462666-1m6lc 200m (5%) 200m (5%) 512Mi (3%) 512Mi (3%) default nginx-4217019353-k3mqk 0 (0%) 0 (0%) 0 (0%) 0 (0%) kube-system kube-dns-638003847-q28hb 260m (6%) 0 (0%) 110Mi (0%) 170Mi (1%) kube-system kubernetes-dashboard-716739405-42t14 100m (2%) 100m (2%) 50Mi (0%) 50Mi (0%)Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 560m (14%) 300m (7%) 672Mi (4%) 732Mi (4%)Events: &lt;none&gt; Etcd Q: etcd backup, seehttps://kubernetes.io/docs/getting-started-guides/ubuntu/backups/https://www.mirantis.com/blog/everything-you-ever-wanted-to-know-about-using-etcd-with-kubernetes-v1-6-but-were-afraid-to-ask/ Start Etcd 123456789101112131415161718192021#start script:#========================================etcd --name 'default' \\--data-dir '/root/data.etcd' \\--ca-file '/pki/ca.crt' --cert-file '/pki/cert.crt' --key-file '/pki/key.key' \\--peer-ca-file '/pki/ca.crt' --peer-cert-file '/pki/cert.crt' --peer-key-file '/pki/key.key' \\--client-cert-auth \\--peer-client-cert-auth \\--listen-peer-urls https://localhost:2380 \\--listen-client-urls https://localhost:2379 \\--advertise-client-urls https://localhost:2379 \\--initial-advertise-peer-urls https://localhost:2380 \\--initial-cluster default=https://localhost:2380 \\--initial-cluster-state 'new' \\--initial-cluster-token 'etcd-cluster' \\--debug#operate:#========================================etcdctl --endpoint=https://localhost:2379 --ca-file=/pki/ca.crt --cert-file=/pki/cert.crt --key-file=/pki/key.key ls / 如果要设置证书： 需要把访问的URL加上https 需要设置上图中红色部分的内容 Replacing a failed etcd member Get the member ID of the failed member1: 1etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list The following message is displayed: 1238211f1d0f64f3269, started, member1, http://10.0.0.1:12380, http://10.0.0.1:237991bc3c398fb3c146, started, member2, http://10.0.0.1:2380, http://10.0.0.2:2379fd422379fda50e48, started, member3, http://10.0.0.1:2380, http://10.0.0.3:2379 Remove the failed member: 1etcdctl member remove 8211f1d0f64f3269 The following message is displayed: 1Removed member 8211f1d0f64f3269 from cluster Add the new member: 1./etcdctl member add member4 --peer-urls=http://10.0.0.4:2380 The following message is displayed: 1Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4 Start the newly added member on a machine with the IP 10.0.0.4: 1bash export ETCD_NAME=\"member4\" export ETCD_INITIAL_CLUSTER=\"member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380\" export ETCD_INITIAL_CLUSTER_STATE=existing etcd [flags] 需要知道，先从集群中添加，然后再启动对应的etcd member。另外，对于新启动的etcd member需要指定启动的状态为“existing”。 Backing up an etcd cluster 12345678910ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb# exit 0# verify the snapshotETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| fe01cf57 | 10 | 7 | 2.1 MB |+----------+----------+------------+------------+ 到此结束，别以为你已经学完了，后面还有呢。看完这一篇，还有无数篇！My God~","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"CKA","slug":"CKA","permalink":"http://ljchen.net/tags/CKA/"}]},{"title":"K8s存储概述","slug":"k8s存储概述","date":"2018-11-05T13:01:46.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2018/11/05/k8s存储概述/","link":"","permalink":"http://ljchen.net/2018/11/05/k8s存储概述/","excerpt":"一直想写一篇关于kubernetes存储的文章，但是细想来又包含了太多方面，索性今天就先聊聊k8s存储的梗概。 概念从用法上来讲，k8s支持通过两种方式使用存储；这两种方式，各自有各自的优势，都不可或缺。 volume暂且就叫它“卷挂载”吧！有人把这种方式叫做“静态卷”。通过指定volume的类型，可以支持绝大多数的存储；这种方式的优点就是即挂即用，方便快捷。当然，弊端就是如果每次使用存储都需要去配置存储参数，比较繁琐。","text":"一直想写一篇关于kubernetes存储的文章，但是细想来又包含了太多方面，索性今天就先聊聊k8s存储的梗概。 概念从用法上来讲，k8s支持通过两种方式使用存储；这两种方式，各自有各自的优势，都不可或缺。 volume暂且就叫它“卷挂载”吧！有人把这种方式叫做“静态卷”。通过指定volume的类型，可以支持绝大多数的存储；这种方式的优点就是即挂即用，方便快捷。当然，弊端就是如果每次使用存储都需要去配置存储参数，比较繁琐。 pvc这种方式刚好克服了volume方式的弊端，本质上是对存储资源的一种抽象。首先，它将存储资源按照种类、介质或者性能等特征划分为不同的类别，每一个类别就是一种storageclass。在需要使用存储的时候，只需要提出要求（比如storageclass-A类型的存储100G），存储卷的创建、挂载等动作均由k8s在后台完成，极大的省去了配置的麻烦。 针对pvc方式使用存储，这里的几个概念，我简单介绍一下： storageclass: 某一种类型的存储后端的抽象； pvc: PV claim,即对存储卷的声明或者干脆叫做需求； pv: 由pvc声明后，被k8s创建出来的存储卷； provisioner: 将某种存储资源提供给k8s使用的适配组件，或者叫做k8s对某种存储的驱动程序（k8s内置了多种存储的provisioner）。 软件架构先了解了k8s存储的使用方式和一些概念之后，我们再来看看源码的结构。 架构图上图橙色部分为kube-controller-manager中存储相关的内容，这个部分重点包含两块功能： pv/pvc controller: 主要负责监听pvc，并基于pvc创建出对应的persistent volume。 attach/dettach controller: 负责将远端的存储挂载到pod所在的宿主机对应目录，一般位于/var/lib/kubelet/pods/&lt;pod-uid&gt;/volumes/ 图中下方蓝色部分为volume-manager, 主要位于kubelet中。volume-manager 主要是监听调度到本节点的pod的volume信息，整理出所有volume的挂载需求，然后执行reconcile函数来保障actual state满足desire state。 针对于某一种特定类型的存储，其创建pv、挂载到宿主机，以及在宿主机上将存储mount到容器目录的这一系列操作都是有专业性的。因此，k8s按照存储类型，以库的方式实现了一套存储的plugin；无论是controller-manager还是kubelet都可以通过源码import的方式来调用plugin库中的实现。因此，右侧绿色部分的plugins是贯穿到上下两部分的。 功能结构总结一下，k8s的存储功能主要有四大部分（其中后两部分都运行在controller-manager上），各部分的功能如下： volume plugin 对k8s存储接口的实现库, 包含了各类存储提供者的plugin实现； 实现自定义的Plugins可以通过FlexVolume； 支持CSI。 volume manager 在kubelet运行，使命是保障actual state满足desire state，主要是mount/unmount（attach/detach可选）； 在pod创建的过程中，会等待volume-manager将volume mount完成，才会继续创建pod。 pv/pvc controller 运行在controller-manager中，主要实现provision/delete（当为out-tree实现时，也需要实现这两个接口）; pv-controller和K8S其它组件一样监听api-server中的资源更新，对于卷管理主要是监听PV，PVC，SC三类资源，当监听到这些资源的创建、删除、修改时，pv-controller经过判断是需要做创建、删除动作。 attach/dettach controller 运行在controller-manager中，主要实现块设备的attach/detach（并非每种存储都需要该操作）； 职责是当api-server中有卷声明的pod与node间的关系发生变化时，决定是通过调用存储插件将这个pod关联的卷attach到对应node的宿主机上，还是将卷从node上detach掉. CSI现在k8s已经逐步将存储都使用CSI来实现了，这里简单整理一下CSI的流程。 部署架构 细节参考以下两篇文档： https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.mdhttps://kubernetes-csi.github.io/docs/Home.html 标准接口 Controller Server CreateVolume =&gt; provision (external-provisioner)DeleteVolume =&gt; delete (external-provisioner)ControllerPublishVolume =&gt; attach (external-attacher)ControllerUnpublishVolume =&gt; detach (external-attacher) Node Server NodeStageVolume =&gt; mount (kubelet)NodeUnstageVolume =&gt; umount (kubelet)NodePublishVolume =&gt; mount (kubelet)NodeUnpublishVolume =&gt; umount (kubelet) 操作 provision/delete CSI proxy通过监听API Server有PVC的Add/Delete操作后通过host与container的socket调用CSI接口，CSI Driver接收到调用后，调用存储设备实现卷的增删。 attach/dettach AD Controller监听API Server的pod，node状态判断是否进行attach/detach操作，如果需要进行，CSI Plugin则通过API Server创建/删除attachvolume(内部API对象). CSI Proxy Container中的attacher监听到API Server中attachvolume的增删后，通过本地socket调用另一个容器中的CSI Driver执行attach/detach操作（注意，CSI接口不是这个名称），CSI Driver再通过调用存储后端完成attach/detach操作。操作完成后，CSI Proxy Container更新attachvolume状态。 mount/unmount Kubelet判断需要做mount操作，通过Host到container的socket调用CSI Driver，CSI Driver在容器内部通过挂载到容器里的Mount Point卷进行bind mount操作。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"}]},{"title":"Kubelet资源eviction机制","slug":"kubelet资源eviction机制","date":"2018-10-30T12:25:37.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2018/10/30/kubelet资源eviction机制/","link":"","permalink":"http://ljchen.net/2018/10/30/kubelet资源eviction机制/","excerpt":"在上一篇文章中，我们提到container GC在后续kuberntes版本会放入eviction manager中来处理。今天我们就来简单的分析一下kubelet eviction的实现机制。","text":"在上一篇文章中，我们提到container GC在后续kuberntes版本会放入eviction manager中来处理。今天我们就来简单的分析一下kubelet eviction的实现机制。 理论基础kubelet支持两种eviction的方式，每一种方式有支持多种指标类型。 驱逐方式 --eviction-hard只要达到eviction中特定类型定义的指标（此处为峰值就干掉），直接干掉pod或者image --eviction-soft驱逐条件达到且持续指定时间--eviction-soft-grace-period才开始真正干掉pod；这里还有一个真正用于kill pod优雅退出的时间指标--eviction-max-pod-grace-period kubelet的两种驱逐方式均支持以下指标类型： 指标类型 用途 memory.available 可用的内存 nodefs.available 系统可用的volume空间 nodefs.inodesFree inode可用的volume空间 imagefs.available 系统可用的镜像空间 imagefs.inodesFree inode可用的镜像空间 allocatableMemory.available 可分配给pod的内存大小 allocatableNodeFs.available 可分配给pod的存储大小 避免波动 内部 --eviction-minimum-reclaim在执行驱逐的时候，为了避免资源在thresholds指标的上下波动，kubelet引入了该参数。该参数定义了每次驱逐操作必须至少清理出多少资源才能够停止执行。（比如：imagefs.available=2Gi） 外部 --eviction-pressure-transition-period该参数定义了 kubelet 多久才上报api-server当前节点的状态，这将避免scheduler将驱逐的pod立即重新调度到该节点，再次触发资源压力的死循环。 Pod优先级12345678910111213 ~  kc explain pod.spec.priorityKIND: PodVERSION: v1FIELD: priority &lt;integer&gt;DESCRIPTION: The priority value. Various system components use this field to find the priority of the pod. When Priority Admission Controller is enabled, it prevents users from setting this field. The admission controller populates this field from PriorityClassName. The higher the value, the higher the priority. ~  实现逻辑条件准备eviction manager是在UpdateRuntimeUp中被启动的，该函数只会调用一次initializeRuntimeDependentModules。在开始eviction manager前，先启动了cadvisor和container manager。其中cadvisor为container manager获取系统的资源使用状况，包括内存和磁盘信息等；container manager起来之后，会校验系统上是否有可分配资源，其算法就是: 1available = capacity - system reserved - kube reserved - hard eviction threshold(百分比或者资源量) 最后，就进入到我们今天的主角，启动eviction manager。具体流程如下图： evictionManager的start函数会启动一个goRoutine来处理eviction（synchronize函数）；如果synchronize函数没有找到需要eviction的pods的话，goRoutine中会等待10秒后再继续调用synchronize。 synchronize函数eviction的核心逻辑在synchronize函数里实现，我将其繁琐的操作整理成以下几个大的方面，具体如下： 信息获取 对应不同的资源类型，生成对应的排序函数（用于排序pod）和资源回收（处理资源的释放操作）的函数； 从cadvisor采集的数据中，获取汇总的统计信息；并从汇总信息中转换出observations数据； 计算资源 基于用户配置的thresholds与observations比较，计算出满足超标的thresholdsMet，我们叫它 A； 将先前扫描中超标，但是未满足grace-period时间的thresholds与observations比较，得到这次扫描仍然超标的thresholds（此处引入了eviction-minimum-reclaim参数计算），我们叫它 B； 最后将B与上一步的结果A做合并，得到最终的thresholds； nodeCondition计算 基于thresholds计算出nodeCondition，该过程会比较出现该nodeCondition的持续时间是否超过用户配置的参数eviction-pressure-transition-period，如果不超过就将其零时存起来供下次再比较； 释放资源 通过thresholds获取到当前饥饿的资源列表，并对资源做排序，找出最饥饿的资源； 在node级别，如果该资源有节点级别的资源释放函数，直接在节点级别释放资源； 在Pod级别针，使用该资源对应的排序函数对当前active的pod进行排序； 针对排序后的active pods列表，从中获取最前面的pod执行killPodFunc； Pod排序算法下面来重点分析一下在“计算资源”阶段使用到的pod基于饥饿资源的排序算法： 内存 按照pod使用的内存是否超过pod对内存的request值； 按照设置的pod优先级； 按照pod消耗的内存数值与request差值（标准为 consume-request） 123func rankMemoryPressure(pods []*v1.Pod, stats statsFunc) &#123; orderedBy(exceedMemoryRequests(stats), priority, memory(stats)).Sort(pods)&#125; 磁盘/镜像 这里需要首先提到一个公共的排序函数，因为在基于磁盘和镜像对pod的排序中都使用到了它： 123return func(pods []*v1.Pod, stats statsFunc) &#123; orderedBy(exceedDiskRequests(stats, fsStatsToMeasure, diskResource), priority, disk(stats, fsStatsToMeasure, diskResource)).Sort(pods)&#125; 该函数的逻辑同内存，首先比较超过request的值，然后是pod优先级，最后是资源使用量与request的差值。 磁盘和镜像的排序其实比较的指标都是文件系统，这里就需要区分镜像是否使用了独立的文件系统(ImageFs)。 使用ImageFs 资源 评估指标 nodeFS logs + localVolume imageFS rootfs 未使用ImageFs 资源 评估指标 nodeFS rootfs + logs + localVolume imageFS rootfs + logs + localVolume 三种指标的描述如下： localVolumeSource identifies stats for pod local volume sources. logs identifies stats for pod logs. rootfs identifies stats for pod container writable layers. 最新版本由于引入了用户自定义的priority，再结合QOS，调度逻辑如下： 对于BestEffort和Burstable的pod，基于priority以及所使用资源量来做驱逐； 理论上Guaranteed类别的pod需要保障的，但是一旦系统服务受到资源稀缺影响时，依然按照priority和所用资源量来对齐做驱逐；","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"}]},{"title":"Kubelet配置源和垃圾回收流程","slug":"kubelet配置源和垃圾回收流程","date":"2018-10-28T10:12:46.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2018/10/28/kubelet配置源和垃圾回收流程/","link":"","permalink":"http://ljchen.net/2018/10/28/kubelet配置源和垃圾回收流程/","excerpt":"在上一篇文章中，概括的划分了kubelet的核心模块，这一篇文章重点来分析一下其中Config和GC在源码中是如何实现的。 Config这部分其实就是kubelet的业务入口，此处所谓的config不是指kubelet的配置参数，而是指业务（也就是pod）配置的意思。形象的说，当你在/etc/kubernetes/manifests下放上pod的yaml文件时，kubelet就通过config模块来扫描到pod配置文件的内容，然后在kubelet中创建出对应的static pod。同理，当你通过kubectl创建的pod被scheduler调度到该节点时，config模块就会感知到Api-Server上的配置的变动，然后对应的将pod创建出来。","text":"在上一篇文章中，概括的划分了kubelet的核心模块，这一篇文章重点来分析一下其中Config和GC在源码中是如何实现的。 Config这部分其实就是kubelet的业务入口，此处所谓的config不是指kubelet的配置参数，而是指业务（也就是pod）配置的意思。形象的说，当你在/etc/kubernetes/manifests下放上pod的yaml文件时，kubelet就通过config模块来扫描到pod配置文件的内容，然后在kubelet中创建出对应的static pod。同理，当你通过kubectl创建的pod被scheduler调度到该节点时，config模块就会感知到Api-Server上的配置的变动，然后对应的将pod创建出来。 Config关注的源头有三个： SourceFile就是启动kubelet指定的静态pod目录 SourceURL觉得就是SourceFile的web版，具体没有用过 SourceApiServer就是kube-api-server 该部分代码的大图如下 makePodSourceConfig函数首先调用了NewPodConfig函数，用于创建出podConfig。该函数非常重要，里面一层层通过newPodStorage将pod配置放到内存中的存储，它的merge方法用于计算pod最终体现出来到底是添加、删除还是更改或者恢复。按照kubelet启动参数的配置，通过对应的三个config的方法（config.NewSourceFile， config.NewSourceURL以及config.NewSourceApiserver）分别初始化了三种Config关注的源头。 由于篇幅的原因，图上没有画出，但是却很重要（有助于理解代码）的是：这三个config方法分别运行了goRoutine作为守护进程，用于监听配置的变化。 当有检测到配置变化后，三种源头的handler会做一些处理，然后将pod spec及其操作动作写入到cfg.Channel中。而这个channel刚好就是后续我们在主loop中等到的配置更新的channel。在主loop中，会基于对pod执行的操作，再分走不通的路径去处理pod，最终保障kubelet上的pod与配置一致。 GarbageCollection(GC)docker容器的本质是宿主机上的一个进程，为了将容器做差异化的封装，docker借助于类似AUFS之类的文件系统做了很多事情。容器停止执行后，这些文件系统并不会自动清除，通过docker ps -a也能够看到这些资源（这是为了下次可以快速启动）。kubelet有一套container gc的方案，专门用于清理宿主机上的非所需容器。 另外，容器镜像较耗存储资源，但是每一台k8s node的存储空间都是有限的，kubelet上运行的pod生命周期可能很短，但是每个pod可能都使用不同的镜像，这就会导致宿主机上会留下很多不再需要的容器镜像，为了将有限的空间腾出来高效利用，kubelet设计了一套image gc的方案。 下图是这部分代码的大致结构： 通过代码可以看到，GC机制即将被eviction替代，在kubelet参数中已经有对应的提示信息。 容器GC容器GC的业务逻辑主要在(m *kubeGenericRuntimeManager)GarbageCollect中，下面是对m.containerGC.GarbageCollect注释的引用，描述了主要的业务逻辑。 GarbageCollect removes dead containers using the specified container gc policy. Note that gc policy is not applied to sandboxes. Sandboxes are only removed when they are not ready and containing no containers. GarbageCollect consists of the following steps: gets evictable containers which are not active and created more than gcPolicy.MinAge ago. removes oldest dead containers for each pod by enforcing gcPolicy.MaxPerPodContainer. removes oldest dead containers by enforcing gcPolicy.MaxContainers. gets evictable sandboxes which are not ready and contains no containers. removes evictable sandboxes. 上面注释中提到了三个指标，这里简单介绍一下这三个指标对应的参数： minimum-container-ttl-duration (gcPolicy.MinAge)容器结束运行多久之后才能被回收；比如: ‘300ms’, ‘10s’ or ‘2h45m maximum-dead-containers-per-container (gcPolicy.MaxPerPodContainer)基于容器为单位，指每个容器最大可以保存多少个已结束的容器，默认是1，负数表示不限制，这些容器会浪费磁盘空间 maximum-dead-containers (gcPolicy.MaxContainers)基于节点为单位，指节点上最多允许保留多少个已结束的容器，默认是-1，表示不做限制 于是，整个流程串起来就是这样的情景： 顶层函数会每分钟被调用，触发container gc操作； 该操作会以container的结束时间是否超过gcPolicy.MinAge为依据，查询出那些满足条件的容器，并组织成为按照pod为key，container列表为值的字典；这一步并没有做实际删除，但是其操作结果为后两部奠定了数据依据； 对字典中的每个pod的container做处理，找出该pod超过gcPolicy.MaxPerPodContainer的容器，然后对它们按照结束时间排序，执行删除，保障每个pod下已结束的container数满足配置参数； 经过上一部的删除后，针对node来讲，如果节点上待删除的容器数依然大于gcPolicy.MaxContainers, 就执行反向的运算。把node允许保留的最大容器数平分给每个pod，再按照该标准对每个pod执行一轮删除； 如果依然还不满足要求的数量（若kubelet有情感的话，已经崩溃掉了），就不再按照pod做key，直接将所有的container拍扁平，按照时间顺序先删除最旧的容器，直到满足总数小于gcPolicy.MaxContainers。 镜像GC分析完了容器的GC，我们再来聊聊镜像GC。kubelet也为镜像的GC提供了对应的配置参数，具体如下： minimum-image-ttl-duration最少这么久镜像都未被使用，才允许清理；比如:’300ms’, ‘10s’ or ‘2h45m’.” image-gc-high-thresholdimageFS磁盘使用率的上限，当达到该值时触发镜像清理。默认值为 90% image-gc-low-thresholdimageFS磁盘使用率的下限，每次清理直到使用率低于这个值或者没有可以清理的镜像了才会停止。默认值为 80% 具体流程比较简单； 与容器GC比较起来，镜像GC顶层函数被触发的周期更长，为5分钟触发一次。 通过cadvisor获取到节点上imageFS的详情，得到capacity、avaiable，据此推算磁盘使用率等信息； 当磁盘使用率大于image-gc-high-threshold参数中指定的值时，触发镜像的GC操作； 找出当前未被使用的镜像列表并按时间排序，过滤掉那些小于minimum-image-ttl-duration的镜像； 正式从节点上删除镜像；每次都比较是否删除的镜像数以满足所需释放的bytesToFree，若满足就停止删除。 值得一提的是，这里的image-gc-low-threshold并非用于每次执行删除镜像时重复的去比较是否满足条件，而是在在触发GC的时候，用于计算需要删除多少bytes即bytesToFree。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"}]},{"title":"Kubelet源码架构简介","slug":"kubelet源码架构简介","date":"2018-10-28T04:38:57.000Z","updated":"2020-04-06T09:24:01.484Z","comments":true,"path":"2018/10/28/kubelet源码架构简介/","link":"","permalink":"http://ljchen.net/2018/10/28/kubelet源码架构简介/","excerpt":"最近将kubelet的代码整体过了一遍，其实也没有想象中复杂。代码涉及到的细枝末节较多，如果先前对kubernetes的产品功能不够了解的话，可能直接上源码分析会云里雾里。我的建议是“先把握框架和主干，然后再逐步深入到各个细节”，做架构要有分层的思想，读代码也如此。写这篇文章一方面是为了记录下当下转瞬即逝的一些思路，另一方面如果后续有人在网上爬到，也希望它能对你有一点点帮助！ 言归正传，下面先总体上谈谈kubelet代码的框架，然后分别介绍kubelet的各主要模块。 概况下面是大图(确实很大！看细节请自行放大)，简要汇总了kubelet从初始化到Run成功后，整个都处于运行中的goRoutines。就是靠这些goRoutine的紧密协作，保障了pod整个生命周期中，完全按照我们的要求来运行。","text":"最近将kubelet的代码整体过了一遍，其实也没有想象中复杂。代码涉及到的细枝末节较多，如果先前对kubernetes的产品功能不够了解的话，可能直接上源码分析会云里雾里。我的建议是“先把握框架和主干，然后再逐步深入到各个细节”，做架构要有分层的思想，读代码也如此。写这篇文章一方面是为了记录下当下转瞬即逝的一些思路，另一方面如果后续有人在网上爬到，也希望它能对你有一点点帮助！ 言归正传，下面先总体上谈谈kubelet代码的框架，然后分别介绍kubelet的各主要模块。 概况下面是大图(确实很大！看细节请自行放大)，简要汇总了kubelet从初始化到Run成功后，整个都处于运行中的goRoutines。就是靠这些goRoutine的紧密协作，保障了pod整个生命周期中，完全按照我们的要求来运行。 初始化从图上可以看到，RunKubelet在启动最终启动kubelet的之前，通过CreateAndInitKubelet来做了很多初始化工作，包括container和image的GC也是在这个阶段就被启动起来了。我简单的分了一下类，这里重点需要掌握的是下面两个服务： Configkubelet所处理pod的配置来源逻辑，主要包含对三种来源：API-Server、URL和File的监听和操作对象的处理 GarbageCollection(GC)对节点上容器和镜像的垃圾回收逻辑 主要模块然后，终于进入核心区域，kubelet的Run函数就是源码的重点了，这里我使用红色背景列出了各个重要角色。 updateRuntimeUp主要涉及eviction操作 syncNodeStatus将节点注册到k8s集群，并收集节点信息定期上报到api-server volumeManager容器的镜像挂载、解绑等逻辑，保障存储与容器状态一致 probeManager主要涉及liveness和readiness的逻辑 syncNetworkStatus从CNI plugin获取状态 podKiller用于pod销毁的goRoutine statusManager负责将Pod状态及时更新到Api-Server pleg全称是Pod Lifecycle Event Generator，主要负责将Pod状态变化记录event以及触发Pod同步 syncNetworkUtil配置节点的防火墙和Masquerade syncLoopkubelet的核心主循环，响应各个模块的channel消息，并集中处理pod状态 在接下来的几篇文章中，我将基于这几个核心领域，分别介绍其大致作用和工作流程。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"}]},{"title":"基于阿里云镜像站安装Kubernetes","slug":"基于阿里云镜像站安装kubernetes","date":"2018-10-23T13:25:51.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2018/10/23/基于阿里云镜像站安装kubernetes/","link":"","permalink":"http://ljchen.net/2018/10/23/基于阿里云镜像站安装kubernetes/","excerpt":"kubernetes官网的文档比较详细，但是所有的安装步骤都有个前提(你有足够自由的互联络)，之前在香港和亚马逊的服务器都是直接照着手册执行脚本一路顺畅。无奈天朝的网络只能够借助于阿里云镜像站了，先前只是在使用该站点的各种linux发行版安装包，最近发现还支持了kubernetes。具体可以访问阿里巴巴开源镜像站。","text":"kubernetes官网的文档比较详细，但是所有的安装步骤都有个前提(你有足够自由的互联络)，之前在香港和亚马逊的服务器都是直接照着手册执行脚本一路顺畅。无奈天朝的网络只能够借助于阿里云镜像站了，先前只是在使用该站点的各种linux发行版安装包，最近发现还支持了kubernetes。具体可以访问阿里巴巴开源镜像站。 安装docker-ce以下适用于centos 7 1234567891011121314151617181920212223242526272829# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum-config-manager --disable docker-ce-edgeyum-config-manager --disable docker-ce-test# Step 3: 更新并安装 Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo service docker start# Step 5: 更改cgroup drivercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ]&#125;EOF 以下命令适用于ubuntu 12345678910111213141516171819202122232425262728# step 1: 安装必要的一些系统工具sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# Step 3: 写入软件源信息sudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"# Step 4: 更新并安装 Docker-CEsudo apt-get -y updatesudo apt-get -y install docker-ce# Step 5: 更改cgroup drivercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ]&#125;EOF 安装二进制文件主要是安装kubelet、kubeadm以及kubectl这三个可执行文件。其中kubeadm是官方的安装工具，kubectl是客户端，kubelet这个就不用介绍了。 安装阿里云的k8s-yum源以下是针对于CentOS的yum源，官方也有针对Ubuntu的源。 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装kubelet如果你希望直接安装最新发布版本的k8s，请直接执行（最终安装的版本关键看你安装的kubeadm版本）。 1234setenforce 0yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetessystemctl enable docker &amp;&amp; systemctl start dockersystemctl enable kubelet &amp;&amp; systemctl start kubelet 调参运行照着执行就行了。 123456789cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOFsysctl --systemsystemctl daemon-reloadsystemctl restart kubelet 容器组件拉取镜像google和docker似乎是有意要对着干的，虽然阿里云也有docker registry的加速器，但是google并没有将kubernetes的镜像放到docker hub上。所以，我们需要先使用脚本，从阿里云的google_containers命名空间下载对应的克隆镜像，然后再通过docker tag将其labels修改为kubeadm生成的static-pod yaml文件对应的镜像标签。从而欺骗kubeadm，所有镜像都已经ready了，不用再去公网上拉取了。 具体操作如下： 镜像列表你肯定会疑问，我怎么知道我要使用哪些镜像？ 好在v1.12.2以上的版本，kubeadm提示可以使用以下命令来获取到镜像信息：12345678[root@k8s-master manifests]# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.12.2k8s.gcr.io/kube-controller-manager:v1.12.2k8s.gcr.io/kube-scheduler:v1.12.2k8s.gcr.io/kube-proxy:v1.12.2k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.2 从阿里云拉取镜像1234567[root@k8s-master manifests]# cat ./pull.shfor i in `kubeadm config images list`; do imageName=$&#123;i#k8s.gcr.io/&#125; docker pull registry.aliyuncs.com/google_containers/$imageName docker tag registry.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.aliyuncs.com/google_containers/$imageNamedone; 安装k8s这就是kubeadm的安装流程了；下面是部署单节点k8s的命令，如果需要部署k8s集群，可以通过指定config文件的方式来指定其etcd集群，并使用相同的方式部署多个api-server、controller-manager以及scheduler。 k8s123456[root@k8s-master manifests]# kubeadm init --kubernetes-version=$(kubeadm version -o short) --pod-network-cidr=10.244.0.0/16[init] using Kubernetes version: v1.12.2[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' 安装网络组件我比较喜欢使用flannel，可以配置不同的backend来支持多种类型的网络。当然，如果对网络安全有特殊的限制，可以考虑其他的组件. 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml 取消污点这是因为我就只有一台机器，如果不干掉这个taint就无法调度pod。 1kubectl taint nodes --all node-role.kubernetes.io/master- 安装dashboard从官网拷贝dashboard的yaml文件到本地，保存为dashboard.yaml，需要注意版本。 12345678#Create a new ServiceAccountkubectl create serviceaccount k8sadmin -n kube-system#Create a ClusterRoleBinding with Cluster Admin Privilegeskubectl create clusterrolebinding k8sadmin --clusterrole=cluster-admin --serviceaccount=kube-system:k8sadmin#Get the tokenkubectl get secret -n kube-system | grep k8sadmin | cut -d \" \" -f1 | xargs -n 1 | xargs kubectl get secret -o 'jsonpath=&#123;.data.token&#125;' -n kube-system | base64 --decode 执行以上命令，最后一条命令或获取到一串token，直接使用得到的token登录dashboard。 12# 安装dashboardkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 如果无法下载墙外镜像，修改image信息： k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1=&gt; ljchen/k8s_gcr_io_kubernetes-dashboard-amd64:v1.10.1 安装WeaveScope1234kubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"# 设置端口映射kubectl port-forward --address=0.0.0.0 -n weave \"$(kubectl get -n weave pod --selector=weave-scope-component=app -o jsonpath='&#123;.items..metadata.name&#125;')\" 4040 执行完以上命令后，在浏览器里面访问执行命令所在的节点的http://{IP}:4040，将看到以下界面。是不是很炫酷！ 阿里巴巴开源镜像站 站点附图，请自行点击kubernetes的帮助。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"}]},{"title":"MySQL总结","slug":"mySQL总结","date":"2018-09-22T14:04:55.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2018/09/22/mySQL总结/","link":"","permalink":"http://ljchen.net/2018/09/22/mySQL总结/","excerpt":"本文主要涉及MySQL核心组件、查询过程、索引等原理，另外还涉及一些不常用但较重要的命令。除此之外，一些运维的理论和命令也有提及。 组成原理主要介绍MySQL的核心组件，以及查询执行过程。","text":"本文主要涉及MySQL核心组件、查询过程、索引等原理，另外还涉及一些不常用但较重要的命令。除此之外，一些运维的理论和命令也有提及。 组成原理主要介绍MySQL的核心组件，以及查询执行过程。 组件直接上图 查询过程 客户端发送一条查询给服务器； 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器生成优化后对应的执行计划； 根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 存储引擎常用MySql存储引擎比较，这里重点讲MyISAM和InnoDB。 指标 MyISAM InnoDB 事务 不支持 支持 读写效率 高 低 索引 支持全文索引 不支持全文索引 外键 不支持 支持 锁 表锁 行锁 文件存储形式 .MYD .MYI *.FRM *.FRM(默认为共享表空间，可修改) 适用场景 大量select语句 大量update语句 删除表后数据文件是否存在 自动清除 不自动清除 锁三种类型 表级锁：(MyISAM/Memory引擎) 开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁：(InnoDB引擎) 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁：(BDB引擎) 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 两种模式 共享锁（读锁), 不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。 排它锁（写锁), 会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。 索引索引: 是一种用来实现MySQL高效获取数据的数据结构。在某个字段上建索引, 就是让MySQL对该字段以索引这种数据结构来存储, 然后查找的时候就有对应的查找算法。 InnoDB使用B+Tree, B+Tree中的B是指balance, 意为平衡。需要注意的是, B+树索引并不能找到一个给定键值的具体行, 它找到的只是被查找数据行所在的页, 接着数据库会把页读入到内存, 再在内存中进行查找, 最后得到要查找的数据。 算法介绍平衡二叉树 可以是空树 如果不是空树，任何一个结点的左子树与右子树都是平衡二叉树，并且高度之差的绝对值不超过1 B-树 B-树是一种多路自平衡的搜索树, 它类似普通的平衡二叉树，不同的一点是B-树允许每个节点有更多的子节点。 B+树 所有关键字存储在叶子节点出现, 内部节点(非叶子节点并不存储真正的 data) 为所有叶子结点增加了一个链指针 索引类型 普通索引 index：没有任何限制 唯一索引 unique：允许为空值 主键 primaryKey：有值且唯一 操作命令 创建索引 1234567mysql&gt; create index index1 on t_table(name);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; alter table t_table add index index2 (grade);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0 查看索引 12345678mysql&gt; show index from t_table;+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| t_table | 1 | index1 | 1 | name | A | NULL | NULL | NULL | YES | BTREE | | || t_table | 1 | index2 | 1 | grade | A | NULL | NULL | NULL | YES | BTREE | | |+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+2 rows in set (0.00 sec) 删除索引 1234567mysql&gt; alter table t_table drop index index1;Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; drop index index2 on t_table;Query OK, 2 rows affected (0.01 sec)Records: 2 Duplicates: 0 Warnings: 0 SQL语句SQL语句可以划分为三个类别： DDL(Data Definition Languages)语句数据定义语言，这些语句定义了不同的数据段、 数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter 等。 DML(Data Manipulation Language)语句数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、update 和 select 等。 DCL(Data Control Language)语句数据控制语句，用于控制不同数据段直接的许可和 访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。 主从复制基本流程 slave端的IO线程连接上master端，并请求从指定binlog日志文件的指定pos节点位置(或者从最开始的日志)开始复制之后的日志内容。 master端在接收到来自slave端的IO线程请求后，通知负责复制进程的IO线程，根据slave端IO线程的请求信息，读取指定binlog日志指定pos节点位置之后的日志信息，然后返回给slave端的IO线程。该返回信息中除了binlog日志所包含的信息之外，还包括本次返回的信息在master端的binlog文件名以及在该binlog日志中的pos节点位置。 slave端的IO线程在接收到master端IO返回的信息后，将接收到的binlog日志内容依次写入到slave端的relaylog文件(mysql-relay-bin.xxxxxx)的最末端，并将读取到的master端的binlog文件名和pos节点位置记录到master-info（该文件存在slave端）文件中，以便在下一次读取的时候能够清楚的告诉master “我需要从哪个binlog文件的哪个pos节点位置开始，请把此节点以后的日志内容发给我”。 slave端的SQL线程在检测到relaylog文件中新增内容后，会马上解析该log文件中的内容。然后还原成在master端真实执行的那些SQL语句，并在自身按顺序依次执行这些SQL语句。这样，实际上就是在master端和slave端执行了同样的SQL语句，所以master端和slave端的数据是完全一样的。 配置流程 在master库执行 12345678910mysql&gt; flush tables with read lock;Query OK, 0 rows affected (0.00 sec)mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000005 | 107 | | |+------------------+----------+--------------+------------------+1 row in set (0.00 sec) 1[root@master ~]# mysqldump -uroot -p rep &gt; /usr/local/mysql/rep.sql 12mysql&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 在slave库执行 1mysql &lt; /usr/local/mysql/rep.sql 123456[root@slave ~]# vim /etc/my.cnf server-id=2 // 设置server-id，不可重复log-bin=mysql-bin // 开启二进制日志，用于链式复制的情况下，即这台服务器如果需要作为其他从服务器的主服务器，则需要开启这个选项read_only = 1 // 设置为只读relay_log = mysql-relay-bin //配置中继日志log_slave_updates = 1 // 表示slave将复制事件写进自己的二进制日志 123456789mysql&gt; change master to -&gt; master_host='192.168.30.110', -&gt; master_user='rep', -&gt; master_password='123456', -&gt; master_log_file='mysql-bin.000005', -&gt; master_log_pos=107;Query OK, 0 rows affected (0.06 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.03 sec) 常用操作帮助命令123456789101112131415mysql&gt; ? index;Many help items for your request exist.To make a more specific request, please type 'help &lt;item&gt;',where &lt;item&gt; is one of the followingtopics: ALTER TABLE CACHE INDEX CREATE INDEX CREATE TABLE DROP INDEX JOIN LOAD INDEX SHOW SHOW INDEX SPATIAL 查看系统时间123456789101112131415mysql&gt; select now();+---------------------+| now() |+---------------------+| 2018-09-21 08:16:48 |+---------------------+1 row in set (0.00 sec)mysql&gt; select current_time;+--------------+| current_time |+--------------+| 08:17:02 |+--------------+1 row in set (0.00 sec) 创建表1234mysql&gt; create table t_table(id int(11), -&gt; name varchar(20), -&gt; grade float );Query OK, 0 rows affected (0.02 sec) 查询表结构123456789mysql&gt; describe t_table;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || name | varchar(20) | YES | | NULL | || grade | float | YES | | NULL | |+-------+-------------+------+-----+---------+-------+3 rows in set (0.02 sec) 查询表创建语句123456789101112131415161718192021222324mysql&gt; show create table t_table;+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| t_table | CREATE TABLE `t_table` ( `id` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, `grade` float DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 |+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; show create table t_table\\G;*************************** 1. row *************************** Table: t_tableCreate Table: CREATE TABLE `t_table` ( `id` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, `grade` float DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin11 row in set (0.00 sec)ERROR:No query specified 插入部分数据12345678910mysql&gt; insert into t_table(id,name) values(1,\"henry\");Query OK, 1 row affected (0.01 sec)mysql&gt; select * from t_table;+------+-------+-------+| id | name | grade |+------+-------+-------+| 1 | henry | NULL |+------+-------+-------+1 row in set (0.00 sec) 插入完整数据1234567891011mysql&gt; insert into t_table values(2,\"cc\", 3.30);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t_table;+------+-------+-------+| id | name | grade |+------+-------+-------+| 1 | henry | NULL || 2 | cc | 3.3 |+------+-------+-------+2 rows in set (0.00 sec) 更新数据123456789101112mysql&gt; update t_table set grade=1.10 where id=1;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from t_table;+------+-------+-------+| id | name | grade |+------+-------+-------+| 1 | henry | 1.1 || 2 | cc | 3.3 |+------+-------+-------+2 rows in set (0.00 sec) 清空表12345678910111213mysql&gt; select * from table2;+------+------+| id | xx |+------+------+| 1 | 323 |+------+------+1 row in set (0.00 sec)mysql&gt; truncate table table2;Query OK, 0 rows affected (0.01 sec)mysql&gt; select * from table2;Empty set (0.00 sec) 查询状态1234567891011121314151617181920212223mysql&gt; status;--------------mysql Ver 14.14 Distrib 5.7.23, for Linux (x86_64) using EditLine wrapperConnection id: 2Current database: testCurrent user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: ''Using delimiter: ;Server version: 5.7.23 MySQL Community Server (GPL)Protocol version: 10Connection: Localhost via UNIX socketServer characterset: latin1Db characterset: latin1Client characterset: latin1Conn. characterset: latin1UNIX socket: /var/run/mysqld/mysqld.sockUptime: 1 hour 36 min 12 secThreads: 1 Questions: 48 Slow queries: 0 Opens: 108 Flush tables: 1 Open tables: 101 Queries per second avg: 0.008-------------- 修改存储引擎123456789101112131415mysql&gt; alter table t_table engine=myisam;Query OK, 2 rows affected (0.01 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; show create table t_table;+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| t_table | CREATE TABLE `t_table` ( `id` int(11) DEFAULT NULL, `name` varchar(20) DEFAULT NULL, `grade` float DEFAULT NULL) ENGINE=MyISAM DEFAULT CHARSET=latin1 |+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 修改慢查询123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; show variables like '%slow%';+---------------------------+--------------------------------------+| Variable_name | Value |+---------------------------+--------------------------------------+| log_slow_admin_statements | OFF || log_slow_slave_statements | OFF || slow_launch_time | 2 || slow_query_log | OFF || slow_query_log_file | /var/lib/mysql/56bd89bc82a8-slow.log |+---------------------------+--------------------------------------+5 rows in set (0.01 sec)mysql&gt; set global slow_query_log=ON;Query OK, 0 rows affected (0.01 sec)mysql&gt; show variablesshow master status; like '%slow_query_log%';+---------------------+--------------------------------------+| Variable_name | Value |+---------------------+--------------------------------------+| slow_query_log | ON || slow_query_log_file | /var/lib/mysql/56bd89bc82a8-slow.log |+---------------------+--------------------------------------+mysql&gt; select sleep(10);+-----------+| sleep(10) |+-----------+| 0 |+-----------+1 row in set (10.00 sec)mysql&gt; exitByeroot@56bd89bc82a8:/# mysqldumpslow /var/lib/mysql/56bd89bc82a8-slow.logReading mysql slow query log from /var/lib/mysql/56bd89bc82a8-slow.logCount: 1 Time=0.00s (0s) Lock=0.00s (0s) Rows=0.0 (0), 0users@0hosts mysqld, Version: N.N.N (MySQL Community Server (GPL)). started with: # Time: N-N-21T13:N:N.266357Z # User@Host: root[root] @ localhost [] Id: N # Query_time: N.N Lock_time: N.N Rows_sent: N Rows_examined: N use test; SET timestamp=N; select sleep(N) 查看进程1234567891011mysql&gt; show full processlist\\G*************************** 1. row *************************** Id: 2 User: root Host: localhost db: testCommand: Query Time: 0 State: starting Info: show full processlist1 row in set (0.00 sec) 性能分析1234567mysql&gt; explain select * from t_table;+----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | t_table | NULL | ALL | NULL | NULL | NULL | NULL | 2 | 100.00 | NULL |+----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.00 sec) explain查看方法： 先看查询类型“type”列，如果为ALL就不用看后面的了，因为是全表扫描； 再看“key”列，看是否使用了索引； 再看“rows”列，表示在SQL执行过程中扫描的行数； 半同步部署 Master 12345 # 安装插件INSTALL PLUGIN rpl_semi_sync_master SONAME# 启用半同步：SET @@GLOBAL.rpl_semi_sync_master_enabled=1;rpl_semi_sync_master_wait_point的值改为：AFTER_COMMIT Slave 12345INSTALL PLUGIN rpl_semi_sync_slave SONAMESET GLOBAL rpl_semi_sync_slave_enabled=1;STOP SLAVE IO THREAD;START SLAVE IO THREAD; 多主复制配置123456CHANGE MASTER TO MASTER_HOST='1.1.1.1',MASTER_USER='user',MASTER_PASSWORD='password',master_auto_position=1 FOR CHANNEL 'm1';START SLAVE FOR CHANNEL 'm1';CHANGE MASTER TO MASTER_HOST='2.2.2.2',MASTER_USER='user',MASTER_PASSWORD='password',master_auto_position=1 FOR CHANNEL 'm2';START SLAVE FOR CHANNEL 'm2'; 安全加固1mysql_secure_installation 主要包含以下操作： 为root用户设置密码 删除匿名账号 取消root用户远程登录 删除test库和对test库的访问权限 刷新授权表使修改生效","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"mysql","slug":"mysql","permalink":"http://ljchen.net/tags/mysql/"}]},{"title":"浅析gRPC","slug":"浅析gRPC","date":"2018-09-20T14:49:44.000Z","updated":"2020-04-06T09:24:01.503Z","comments":true,"path":"2018/09/20/浅析gRPC/","link":"","permalink":"http://ljchen.net/2018/09/20/浅析gRPC/","excerpt":"gRPC.io官网上的一篇Blog大致讲解了gRPC是如何使用HTTP/2的，觉得讲的比较抽象，理解的不够透彻，于是自己找了一些资料。大致总结一下，个人认为：gRPC之所以高效，除了在协议层使用Protobuffer之外，底层使用HTTP/2也是一个非常重要的原因。下面先上一张图，再来看看HTTP/2的一些特征。","text":"gRPC.io官网上的一篇Blog大致讲解了gRPC是如何使用HTTP/2的，觉得讲的比较抽象，理解的不够透彻，于是自己找了一些资料。大致总结一下，个人认为：gRPC之所以高效，除了在协议层使用Protobuffer之外，底层使用HTTP/2也是一个非常重要的原因。下面先上一张图，再来看看HTTP/2的一些特征。 HTTP/2概念 消息(Message)：由一个或多个帧组合而成，例如请求和响应; 流(Stream)：存在于连接中的一个虚拟通道，流可以承载双向消息，每个流都有一个唯一的整数ID； 帧(Frame)：HTTP/2通信的最小单位，每个帧包含帧首部，至少也会标识出当前帧所属的流; 连接(Connection)：与 HTTP/1 相同，都是指对应的 TCP 连接; 特征 多路复用、乱序收发：可以乱序收发数据报文，不用使用单步：发1-&gt;收1 或者流水线：发1-&gt;发2-&gt;收2-&gt;收1 的流程，提高效率； Header压缩：不用花大量篇幅重复发送常用header，采用发送增量的方法，由客户端和服务器端共同维护一个字典； stream优先级：可以在一个连接上，为不同stream设置不同优先级； 服务器推送：提前发送需要的资源； gRPC收发消息流程发送流程 解析地址：client消息发送给gRpc，然后resolver解析域名，并获取到目标服务器地址列表； 负载均衡：客户端基于负载均衡算法，从连接服务器列表中找出一个目标服务器； 连接：如果到目标服务器已有连接，则使用已有连接，访问目标服务器；如果没有可用连接，则创建HTTP/2连接； 编码：对请求消息使用 Protobuf做序列化，通过 HTTP/2 Stream 发送给 gRPC 服务端； 接收流程 编码：接收到服务端响应之后，使用Protobuf 做反序列化； 回调：回调 GrpcFuture 的 set(Response) 方法，唤醒阻塞的客户端调用线程，获取 RPC 响应。 负载均衡简单流程 gRPC支持客户端负载均衡rr和grpclb policy流程: grpc client请求nameserver解析服务名称，返回地址里面会标识该地址是属于lb还是server； 如果返回地址是lb的，客户端将使用grpclb策略，否则使用其他配置的负载均衡策略； 其他负载均衡策略都会建立到所有server的子通道，然后按照策略发送流量； 如果是grpclb策略，将建立到lb server的连接，并通过该连接获取grpc server的地址（grpc server会发送负载情况到lb server以更新状态）； 实现基于gRPC dnsResolver可以实现基于consul的resolver，只需要实现Resolve接口。 12345678910111213func (r *consulResolver) Resolve(target string) (naming.Watcher, error) &#123; config := api.DefaultConfig() config.Address = r.address client, err := api.NewClient(config) if err != nil &#123; return nil, err &#125; return &amp;consulWatcher&#123; client: client, service: r.service, addrs: map[string]struct&#123;&#125;&#123;&#125;, &#125;, nil&#125; 其返回值naming.Watcher需要实现next方法，用于刷新address列表。在使用的时候，还是需要指定对应的target，除此之外，还需要通过lb和resolver配置负载均衡。 123456t.gRpcConn, err = grpc.Dial( t.GRpcURL, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithBalancer(grpc.RoundRobin(grpclb.NewConsulResolver(consulAddr, service))),) 这里必须要指定grpc.WithBlock()，否则不能够生效，应该是从consul获取地址需要时间，如果不指定的话会直接跑过。但是，这也带来了坑，一旦consul上查不到对应的地址列表，会阻塞，且没有任何打印信息。 服务定义service在protobuf中定义的service，最终会被编译为一个类；所有的RPC调用都是这个类的方法。 123service RouteGuide &#123; ...&#125; RPC类型所有的RPC都是service的编译成的类的一个方法，gRPC支持四种不同的方法。其中流式RPC中，使用stream关键字特殊定义了参数类型。 简单RPC 1rpc GetFeature(Point) returns (Feature) &#123;&#125; 服务器端流式RPC 1rpc ListFeatures(Rectangle) returns (stream Feature) &#123;&#125; 客户端流式RPC 1rpc RecordRoute(stream Point) returns (RouteSummary) &#123;&#125; 向流式RPC 1rpc RouteChat(stream RouteNote) returns (stream RouteNote) &#123;&#125; 编译结果1234567891011121314151617181920212223type routeGuideServer struct &#123; ...&#125;...func (s *routeGuideServer) GetFeature(ctx context.Context, point *pb.Point) (*pb.Feature, error) &#123; ...&#125;...func (s *routeGuideServer) ListFeatures(rect *pb.Rectangle, stream pb.RouteGuide_ListFeaturesServer) error &#123; ...&#125;...func (s *routeGuideServer) RecordRoute(stream pb.RouteGuide_RecordRouteServer) error &#123; ...&#125;...func (s *routeGuideServer) RouteChat(stream pb.RouteGuide_RouteChatServer) error &#123; ...&#125; 流式RPC流式RPC是指，传入或者返回的是一个stream（类似于一个套接口）。代码可以对该stream执行send或者recv操作来写入或者读出在RPC定义时指定的数据结构，可循环操作多次，直到读完数据。 使用方法12345678910111213141516171819202122232425262728// 创建gRPC连接conn, err := grpc.Dial(*address, grpc.WithInsecure())if err != nil &#123; log.Fatalf(\"faild to connect: %v\", err)&#125;defer conn.Close()// 初始化客户端代码c := pb.NewGreeterClient(conn)// 调用服务器端流式RPC，请求是一个HelloRequest结构体，返回是streamstream, err := c.SayHello1(context.Background(), &amp;pb.HelloRequest&#123;Name: *name&#125;)if err != nil &#123; log.Fatalf(\"could not greet: %v\", err)&#125;// 循环读取在RPC中定义的数据结构，直到返回io.EOFfor &#123; reply, err := stream.Recv() if err == io.EOF &#123; break &#125; if err != nil &#123; log.Printf(\"failed to recv: %v\", err) &#125; log.Printf(\"Greeting: %s\", reply.Message)&#125; 场景当年不知道gRPC支持流式RPC，为了从日志agent获取日志信息，使用了简单的RPC来逐条读取消息，效率极其低下。现在想来，其实如果使用gRPC的流式方法来获取日志应该也算是一个比较好的方案。比如，当用户在页面上要查看日志的时候，controller层就发起到日志微服务的gGRPC流式请求，一个日志请求发送过去，从该RPC就可以流式的返回所需查看的所有日志；而不用发送一条再请求第二条，再返回。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"},{"name":"gRPC","slug":"gRPC","permalink":"http://ljchen.net/tags/gRPC/"}]},{"title":"Redis知识点整理","slug":"redis知识点整理","date":"2018-09-11T14:08:22.000Z","updated":"2020-04-06T09:24:01.486Z","comments":true,"path":"2018/09/11/redis知识点整理/","link":"","permalink":"http://ljchen.net/2018/09/11/redis知识点整理/","excerpt":"Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，和Memcached类似;它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）；同时，还支持地理位置GEO和Bitmap扩展类型。","text":"Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，和Memcached类似;它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）；同时，还支持地理位置GEO和Bitmap扩展类型。Redis之所以这么受欢迎，得益于它的性能，说到性能又不得不提它的单线程设计。那为什么redis采用单线程还这么高效呢？主要由以下原因： 减少上下文切换时间 纯内存访问 非阻塞IO，使用epoll 数据类型字符串12345678910111213141516171819202122232425262728set key xx ex[seconds] | px [milliseconds] [nx |xx] # 如何实现分布式共享锁setex seconds keysetnx keysetxx keyget keydel keymset key1 value1 [key2 value2 …] # mset 与 set 之间最大的差异和性能改善在哪儿？mget key1 [key2 …]append key valueexists keyexpire key secondsstrlen keygetset key valuesetrange key offset valuegetrange key start endtype keyobject encoding key incr keydecr key 哈希123456789101112131415hset key field valueHget key fieldHdel key field [field …]hmset key field value [field value …]hmget key field [field …]hexists key fieldhkeys keyhvals keyhgetall keyhstrlen key fieldhlen key 列表特征： 列表中元素有序 列表中元素可重复 12345678910111213141516171819lpush key value [value …]rpush key value [value …]lrange start end # lrange key 0 -1linsert key before | after pivot valuelindex key indexllen keylpop keyrpop keyltrim key count value # count&gt;0 从左到右删除最多count个， count&lt;0 从右到左删除最多count个，count=0 删除所有ltrim key start end # 保留start-&gt;endlset key index value blpop key timeoutbrpop key timeout 集合12345678910111213141516sadd key element [element …]srem key element [element …]scard key # 计算元素个数sismember key element # 判断元素是否在集合中srandmemeber key [count] # 随机从集合中返回指定个数值（不会删除）spop key # 从集合中随机弹出元素（随即删除）sinter key [key ...] # 交集suinon key [key ...] # 并集sdiff key [key ...] # 差集sinterstore destination key [key ...] # 交集集合保存suionstore destination key [key ...] # 并集集合保存sdiffstore destination key [key ...] # 差集集合保存 有序集合特征： 保留不可重复特征 给每个元素设置一个score作为排序的依据 1234567891011121314151617181920zadd key score member [score member ...]zcard keyzscore key member # 计算某个成员的分数zrank key member # 从低到高排名zrevrank key member # 从高到低排名zrem key member [member ...] # 删除成员zincrby key increment member # 增加成员分数zrange key start end [withscores] # 返回指定排名范围的成员zrevrange key start end [withscores]zrangebyscore key min max [withscores] [limit offset count] # 返回指定分数范围的成员zrevrangebyscore key max min [withscores] [limit offset count]zcount key min max # 返回指定分数范围的成员个数zremrangebyrank key start end # 删除指定排名内的升序元素zremrangebyscore key min max # 删除指定分数范围的成员 其他 包括用于计算地理位置的GEO 和 BITMAP 两种扩展类型。 键管理12345678910111213141516dbsizerename key newKey randomkey # 随机返回一个keyexpire key secondpersist keyttl keymove key dbdump key # 序列化为RDB格式restore key ttl valueflushdb # 清除当前数据库flushall # 清除所有数据库 使用场景 分布式锁（setnx ex） 消息队列（list） 延时队列（zset） 计数（bitmap） 统计UV（HyperLogLog pfadd/pfcount） 大规模数据集判断是否存在（布隆过滤器 bf.add/bf.exists） 限流（cl.throttle） 计算距离（GeoHash） 管道本质客户端行为，是将多个操作放到一个命令里面发送到服务器端处理。 事务 multi -&gt; exec 是客户端将多个命令发送到服务器端缓存，然后执行exec的时候再统一执行。它不具有原子性，无法保障里面的多个命令都成功或者都失败。 一般管道和事务和结合使用，在multi之后，将多个操作命令压如管道中，一次发送到服务器端执行。 watch watch 会在事务开始之前盯住一个或多个关键变 量，当事务执行时，也就是服务器收到了 exec 指令要顺序执行缓存的事务队列时，Redis 会检查关键变量自 watch 之后 是否被修改了(包括当前事务所在的客户端)。如果关键变量被人动过了， exec 指令就会返回 NULL 回复告知客户端事务执行失败，这个时候客户端一般会选择重试。 Redis 禁止在 multi 和 exec 之间执行 watch 指令，而必须在 multi 之前盯住关键 变量，否则会出错。 日常运维慢查询12345config set slowlog-log-slower-than 20000 # 慢查询标准为慢于20000微秒config set slowlog-max-len 1000 # 日志最大条数slowlog get [n] # 读取慢查询日志slowlog len # 获取慢查询日志长度 事务1234multi xxx xxxexec/discard run idredis的run id在重启之后会发生变化，这样slave就能基于该id来判断需要重新全量同步master的数据，因为重启过程可能发生数据恢复等操作。 如果希望不改变run id的情况下重新加载配置，需要执行 redis-cli debug reload 数据持久化 RDB快照手动触发时，save和bgsave的差异？ AOF以独立日志方式记录每次写操作，重启时重放以恢复数据。 触发方法 手动bgrewriteaof 自动根据auto-aof-rewrite-min-size 和 auto-aof-rewrite-percentage确定自动触发机制 持久化流程命令写入 -&gt; AOF缓冲区 -&gt; AOF文件 -&gt; rewrite -&gt; 重启(load) 执行重写后，为什么AOF文件会变小? 干掉一些中间状态的数据，比如set之后又del的数据； expire已经timeout的数据，不再写入； 多条命令可以合并为一条； 三种AOF缓冲区同步文件策略 always everysec none 复制相关123slave of host portslave of no oneinfo replication 默认情况下， 从节点使用 slave-read-only=yes 配置为只读模式。 哨兵模式redis sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线标识。 如果被标识的是主节点，它还会和其他Sentinel节点进行“协商”，当大多数 Sentinel节点都认为主节点不可达时，它们会选举出一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化实时通知给Redis应用方。 哨兵的流程： 监控并发送报文给各自的节点； 发现主观下线; 询问，并投票，客观下线; 领导者节点选举-&gt;raft（基本为谁最早发现某个redis节点主观下线，就会发起投票，然后的票就会最多，然后成为切换该主redis的领导者）; 故障转移；选出新的老大（在从节点中，过滤不健康的，然后基于优先级-&gt;复制偏移量-&gt;runid 集群方案集群方案在这里有详细介绍。redis cluser采用虚拟槽分区，所有的键根据哈希函数映射到0~16383整数槽内，计算公式： slot = CRC16(key）&amp; 16383 集群限制 不支持多个db Key的批量操作有限，比如mset 节点握手123cluster meet &#123;ip&#125; &#123;port&#125;cluster nodescluster info 分配槽1cluster addslots &#123;0…5555&#125; 从节点设置1cluster replicate &#123;master node id&#125; 缓存穿透解决办法 缓存空对象 布隆过滤器 值得深入思考的问题 redis如何实现消息队列？ 如何使用redis实现每分钟获取验证码的次数不超过5次？ redis的订阅与发布适合用来做消息队列吗？为什么？ 如果一个key设置了expire时间，get操作之后，expire会发生变化吗？ 关系型数据库存储与hash散列存储的差异性在哪儿？ 将数据直接序列化后使用string存到redis和将数据按照hash存到redis两者的优缺点？ 如何通过redis快速计算出具有共同兴趣爱好的一类人？ 如何通过redis计算被点赞数的用户排名？ 当系统中同时存在AOF和RDB文件时，系统重启默认有限加载哪一个？ 哨兵模式下，客户端连接的是redis节点，还是哨兵节点？ 缓存雪崩后，重建缓存可能会被很多程序调用到，这个时候如何采用较好的方法避免出现大家都去重建缓存呢？","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"redis","slug":"redis","permalink":"http://ljchen.net/tags/redis/"}]},{"title":"编译原理","slug":"编译原理","date":"2018-09-02T12:46:58.000Z","updated":"2020-04-06T09:24:01.503Z","comments":true,"path":"2018/09/02/编译原理/","link":"","permalink":"http://ljchen.net/2018/09/02/编译原理/","excerpt":"一般高级语言程序编译的过程：预处理、编译、汇编、链接。 gcc在后台实际上也经历了这几个过程，我们可以通过-v参数查看它的编译细节，如果想看某个具体的编译过程，则可以分别使用-E, -S, -c和 -O，对应的后台工具则分别为cpp,cc1,as,ld。 下面我们将逐步分析这几个过程以及相关的内容，诸如语法检查、代码调试、汇编语言等。","text":"一般高级语言程序编译的过程：预处理、编译、汇编、链接。 gcc在后台实际上也经历了这几个过程，我们可以通过-v参数查看它的编译细节，如果想看某个具体的编译过程，则可以分别使用-E, -S, -c和 -O，对应的后台工具则分别为cpp,cc1,as,ld。 下面我们将逐步分析这几个过程以及相关的内容，诸如语法检查、代码调试、汇编语言等。 预处理预处理是C语言程序从源代码变成可执行程序的第一步，主要是C语言编译器对各种预处理命令进行处理，包括头文件的包含、宏定义的扩展、条件编译的选择等。打印出预处理之后的结果：gcc -E hello.c 或者 cpp hello.c这样我们就可以看到源代码中的各种预处理命令是如何被解释的，从而方便理解和查错。 gcc调用了cpp的(虽然我们通过gcc的-v仅看到cc1)，cpp即The C Preprocessor，主要用来预处理宏定义、文件包含、条件编译等。下面介绍它的一个比较重要的选项-D。在命令行定义宏：gcc –Dmacro=1 hello.c 或者 cpp –Dmacro=1 hello.c等同于在文件的开头定义宏，即#define maco，但是在命令行定义更灵活。例如，在源代码中有这些语句: 123#ifdef DEBUGprintf(\"this code is for debuggingn\");#endif 编译编译之前，C语言编译器会进行词法分析、语法分析(-fsyntax-only)，接着会把源代码翻译成中间语言，即汇编语言。如果想看到这个中间结果，可以用-S选项。 编译程序工作时，先分析，后综合，从而得到目标程序。所谓分析，是指词法分析和语法分析；所谓综合是指代码优化，存储分配和代码生成。为了完成这些分析综合任务，编译程序采用对源程序进行多次扫描的办法，每次扫描集中完成一项或几项任务，也有一项任务分散到几次扫描去完成的。 下面举一个四遍扫描的例子： 第一遍扫描做词法分析； 第二遍扫描做语法分析； 第三遍扫描做代码优化和存储分配； 第四遍扫描做代码生成。 值得一提的是，大多数的编译程序直接产生机器语言的目标代码，形成可执行的目标文件，但也有的编译程序则先产生汇编语言一级的符号代码文件，然后再调用汇编程序进行翻译加工处理，最后产生可执行的机器语言目标文件。 语法检查之后是翻译动作，gcc提供了一个优化选项-O，以便根据不同的运行平台和用户要求产生经过优化的汇编代码。例如， 123456$ gcc -o hello hello.c #采用默认选项，不优化$ gcc -O2 -o hello2 hello.c #优化等次是2$ gcc -Os -o hellos hello.c #优化目标代码的大小$ time ./hello #查看代码运行时间hello, world 根据上面的简单演示，可以看出gcc有很多不同的优化选项，主要看用户的需求了，目标代码的大小和效率之间貌似存在一个纠缠，需要开发人员自己权衡。 下面我们通过-S选项来看看编译出来的中间结果，汇编语言，还是以之前那个hello.c为例。 1234567891011121314151617181920212223242526272829$ gcc -S hello.c #默认输出是hello.s，可自己指定，输出到屏幕-o -，输出到其他文件-o file$ cat hello.scat hello.s .file \"hello.c\" .section .rodata.LC0: .string \"hello, world\" .text.globl main .type main, @functionmain: leal 4(%esp), %ecx andl $-16, %esp pushl -4(%ecx) pushl %ebp movl %esp, %ebp pushl %ecx subl $4, %esp movl $.LC0, (%esp) call puts movl $0, %eax addl $4, %esp popl %ecx popl %ebp leal -4(%ecx), %esp ret .size main, .-main .ident \"GCC: (GNU) 4.1.3 20070929 (prerelease) (Ubuntu 4.1.2-16ubuntu2)\" .section .note.GNU-stack,\"\",@progbits 和intel的汇编语法不太一样，这里用的是AT&amp;T语法格式。这里需要补充的是，在写C语言代码时，如果能够对编译器比较熟悉（工作原理和一些细节）的话，可能会很有帮助。包括这里的优化选项(有些优化选项可能在汇编时采用)和可能的优化措施。 汇编把作为中间结果的汇编代码翻译成了机器代码，即目标代码，不过它还不可以运行。如果要产生这一中间结果，可用gcc的-c选项，当然，也可通过as命令汇编语言源文件来产生。 12345678910$ file hello.shello.s: ASCII assembler program text$ gcc -c hello.s #用gcc把汇编语言编译成目标代码$ file hello.o #file命令可以用来查看文件的类型hello.o: ELF 32-bit LSB relocatable, Intel 80386, version 1 (SYSV), not stripped$ as -o hello.o hello.s #用as把汇编语言编译成目标代码$ file hello.ohello.o: ELF 32-bit LSB relocatable, Intel 80386, version 1 (SYSV), not stripped gcc和as默认产生的目标代码都是ELF格式的，因此这里主要讨论ELF格式的目标代码。目标代码不再是普通的文本格式，无法直接通过文本编辑器浏览，需要一些专门的工具。 binutils(GNU Binary Utilities)的很多工具都采用这个库来操作目标文件，这类工具有objdump, objcopy, nm, strip等，不过另外一款非常优秀的分析工具readelf并不是基于这个库，所以你也应该可以直接用elf.h头文件中定义的相关结构来操作ELF文件。 ELF文件的结构： ELF Header (ELF文件头)说明了文件的类型，大小，运行平台，节区数目等。 Porgram Headers Table (程序头表，实际上叫段表好一些，用于描述可执行文件和可共享库)Section 1Section 2… Section Headers Table(节区头部表，用于链接可重定位文件成可执行文件或共享库) 可以分别通过 readelf文件的-h，-l和-S参数查看ELF文件头(ELF Header)、程序头部表（Program Headers Table，段表）和节区表(Section Headers Table)。 下面通过这几段代码来演示通过readelf -h参数查看ELF的不同类型。期间将演示如何创建动态连接库(即可共享文件)、静态连接库，并比较它们的异同。 123456789101112131415161718192021$ gcc -c myprintf.c test.c #编译产生两个目标文件myprintf.o和test.o，它们都是可重定位文件(REL)$ readelf -h test.o | grep Type Type: REL (Relocatable file)$ readelf -h myprintf.o | grep Type Type: REL (Relocatable file)$ gcc -o test myprintf.o test.o #根据目标代码连接产生可执行文件，这里的文件类型是可执行的(EXEC)$ readelf -h test | grep Type Type: EXEC (Executable file)$ ar rcsv libmyprintf.a myprintf.o #用ar命令创建一个静态连接库$ readelf -h libmyprintf.a | grep Type #因此，使用静态连接库和可重定位文件一样，它们之间唯一不同是前者可以是多个可重定位文件的“集合”。 Type: REL (Relocatable file)$ gcc -o test test.o -llib -L./ #可以直接连接进去，也可以使用-l参数，-L指定库的搜索路径$ gcc -Wall myprintf.o -shared -Wl,-soname,libmyprintf.so.0 -o libmyprintf.so.0.0 #编译产生动态链接库，并支持major和minor版本号，动态链接库类型为DYN$ ln -sf libmyprintf.so.0.0 libmyprintf.so.0$ ln -sf libmyprintf.so.0 libmyprintf.so$ readelf -h libmyprintf.so | grep Type Type: DYN (Shared object file)$ gcc -o test test.o -llib -L./ #编译时和静态连接库类似，但是执行时需要指定动态连接库的搜索路径$ LD_LIBRARY_PATH=./ ./test #LD_LIBRARY_PATH为动态链接库的搜索路径$ gcc -static -o test test.o -llib -L./ #在不指定static时会优先使用动态链接库，指定时则阻止使用动态连接库这个时候会把所有静态连接库文件加入到可执行文件中. 可重定位文件本身不可以运行，仅仅是作为可执行文件、静态连接库（也是可重定位文件）、动态连接库的 “组件”。 下面来看看ELF文件的主体内容，节区（Section)。ELF文件具有很大的灵活性，它通过文件头组织整个文件的总体结构，通过节区表 (Section Headers Table)和程序头（Program Headers Table或者叫段表)来分别描述可重定位文件和可执行文件。在可重定位文件中，节区表描述的就是各种节区本身；而在可执行文件中，程序头描述的是由各个节区组成的段（Segment），以便程序运行时动态装载器知道如何对它们进行内存映像，从而方便程序加载和运行。 可以通过readelf的-S参数查看ELF的节区。先来看看可重定位文件的节区信息，通过节区表来查看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687$ gcc -c myprintf.c #默认编译好myprintf.c，将产生一个可重定位的文件myprintf.o$ readelf -S myprintf.o #通过查看myprintf.o的节区表查看节区信息There are 11 section headers, starting at offset 0xc0:Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al [ 0] NULL 00000000 000000 000000 00 0 0 0 [ 1] .text PROGBITS 00000000 000034 000018 00 AX 0 0 4 [ 2] .rel.text REL 00000000 000334 000010 08 9 1 4 [ 3] .data PROGBITS 00000000 00004c 000000 00 WA 0 0 4 [ 4] .bss NOBITS 00000000 00004c 000000 00 WA 0 0 4 [ 5] .rodata PROGBITS 00000000 00004c 00000e 00 A 0 0 1 [ 6] .comment PROGBITS 00000000 00005a 000012 00 0 0 1 [ 7] .note.GNU-stack PROGBITS 00000000 00006c 000000 00 0 0 1 [ 8] .shstrtab STRTAB 00000000 00006c 000051 00 0 0 1 [ 9] .symtab SYMTAB 00000000 000278 0000a0 10 10 8 4 [10] .strtab STRTAB 00000000 000318 00001a 00 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings) I (info), L (link order), G (group), x (unknown) O (extra OS processing required) o (OS specific), p (processor specific)$ objdump -d -j .text myprintf.o #这里是程序指令部分，用objdump的-d选项可以看到反编译的结果， #-j指定需要查看的节区myprintf.o: file format elf32-i386Disassembly of section .text:00000000 &lt;myprintf&gt;: 0: 55 push %ebp 1: 89 e5 mov %esp,%ebp 3: 83 ec 08 sub $0x8,%esp 6: 83 ec 0c sub $0xc,%esp 9: 68 00 00 00 00 push $0x0 e: e8 fc ff ff ff call f &lt;myprintf+0xf&gt; 13: 83 c4 10 add $0x10,%esp 16: c9 leave 17: c3 ret$ readelf -r myprintf.o #用-r选项可以看到有关重定位的信息，这里有两部分需要重定位Relocetion section '.rel.text' at offset 0x334 contains 2 entries: Offset Info Type Sym.Value Sym. Name0000000a 00000501 R_386_32 00000000 .rodata0000000f 00000902 R_386_PC32 00000000 puts$ readelf -x .rodata myprintf.o #.rodata节区包含只读数据，即我们要打印的hello, world!.Hex dump of section '.rodata': 0x00000000 68656c6c 6f2c2077 6f726c64 2100 hello, world!.$ readelf -x .data myprintf.o #没有这个节区,.data应该包含一些初始化的数据Section '.data' has no data to dump.$ readelf -x .bss myprintf.o #也没有这个节区，.bss应该包含一些未初始化的数据，程序默认初始为0Section '.bss' has no data to dump.$ readelf -x .comment myprintf.o #是一些注释，可以看到是是GCC的版本信息Hex dump of section '.comment': 0x00000000 00474343 3a202847 4e552920 342e312e .GCC: (GNU) 4.1. 0x00000010 3200 2.$ readelf -x .note.GNU-stack myprintf.o #这个也没有内容Section '.note.GNU-stack' has no data to dump.$ readelf -x .shstrtab myprintf.o #包括所有节区的名字Hex dump of section '.shstrtab': 0x00000000 002e7379 6d746162 002e7374 72746162 ..symtab..strtab 0x00000010 002e7368 73747274 6162002e 72656c2e ..shstrtab..rel. 0x00000020 74657874 002e6461 7461002e 62737300 text..data..bss. 0x00000030 2e726f64 61746100 2e636f6d 6d656e74 .rodata..comment 0x00000040 002e6e6f 74652e47 4e552d73 7461636b ..note.GNU-stack 0x00000050 00 .$ readelf –x .symtab myprintf.o #符号表，包括所有用到的相关符号信息，如函数名、变量名Symbol table '.symtab' contains 10 entries: Num: Value Size Type Bind Vis Ndx Name 0: 00000000 0 NOTYPE LOCAL DEFAULT UND 1: 00000000 0 FILE LOCAL DEFAULT ABS myprintf.c 2: 00000000 0 SECTION LOCAL DEFAULT 1 3: 00000000 0 SECTION LOCAL DEFAULT 3 4: 00000000 0 SECTION LOCAL DEFAULT 4 5: 00000000 0 SECTION LOCAL DEFAULT 5 6: 00000000 0 SECTION LOCAL DEFAULT 7 7: 00000000 0 SECTION LOCAL DEFAULT 6 8: 00000000 24 FUNC GLOBAL DEFAULT 1 myprintf 9: 00000000 0 NOTYPE GLOBAL DEFAULT UND puts$ readelf -x .strtab myprintf.o #字符串表，用到的字符串，包括文件名、函数名、变量名等。Hex dump of section '.strtab': 0x00000000 006d7970 72696e74 662e6300 6d797072 .myprintf.c.mypr 0x00000010 696e7466 00707574 7300 intf.puts. 从上表可以看出，对于可重定位文件，会包含这些基本节区.text, .rel.text, .data, .bss, .rodata, .comment, .note.GNU-stack, .shstrtab, .symtab和.strtab。 看一看 myprintf.c 产生的汇编代码。 12345678910111213141516171819202122$ gcc -S myprintf.c$ cat myprintf.s .file \"myprintf.c\" .section .rodata.LC0: .string \"hello, world!\" .text.globl myprintf .type myprintf, @functionmyprintf: pushl %ebp movl %esp, %ebp subl $8, %esp subl $12, %esp pushl $.LC0 call puts addl $16, %esp leave ret .size myprintf, .-myprintf .ident \"GCC: (GNU) 4.1.2\" .section .note.GNU-stack,\"\",@progbits 链接链接是处理可重定位文件，把它们的各种符号引用和符号定义转换为可执行文件中的合适信息(一般是虚拟内存地址)的过程。链接又分为静态链接和动态链接，前者是程序开发阶段程序员用ld(gcc实际上在后台调用了ld)静态链接器手动链接的过程，而动态链接则是程序运行期间系统调用动态链接器(ld-linux.so)自动链接的过程。比如，如果链接到可执行文件中的是静态连接库libmyprintf.a，那么.rodata节区在链接后需要被重定位到一个绝对的虚拟内存地址，以便程序运行时能够正确访问该节区中的字符串信息。而对于puts，因为它是动态连接库libc.so中定义的函数，所以会在程序运行时通过动态符号链接找出puts函数在内存中的地址，以便程序调用该函数。 静态链接过程主要是把可重定位文件依次读入，分析各个文件的文件头，进而依次读入各个文件的节区，并计算各个节区的虚拟内存位置，对一些需要重定位的符号进行处理，设定它们的虚拟内存地址等，并最终产生一个可执行文件或者是动态链接库。这个链接过程是通过ld来完成的，ld在链接时使用了一个链接脚本（linker scripq），该链接脚本处理链接的具体细节。这里主要介绍可重定位文件中的节区（节区表描述的）和可执行文件中段（程序头描述的）的对应关系以及gcc编译时采用的一些默认链接选项。 下面先来看看可执行文件的节区信息，通过程序头（段表）来查看： 1234567891011121314151617181920212223242526272829303132333435363738394041424344$ readelf -S test.o #为了比较，先把test.o的节区表也列出There are 10 section headers, starting at offset 0xb4:Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al [ 0] NULL 00000000 000000 000000 00 0 0 0 [ 1] .text PROGBITS 00000000 000034 000024 00 AX 0 0 4 [ 2] .rel.text REL 00000000 0002ec 000008 08 8 1 4 [ 3] .data PROGBITS 00000000 000058 000000 00 WA 0 0 4 [ 4] .bss NOBITS 00000000 000058 000000 00 WA 0 0 4 [ 5] .comment PROGBITS 00000000 000058 000012 00 0 0 1 [ 6] .note.GNU-stack PROGBITS 00000000 00006a 000000 00 0 0 1 [ 7] .shstrtab STRTAB 00000000 00006a 000049 00 0 0 1 [ 8] .symtab SYMTAB 00000000 000244 000090 10 9 7 4 [ 9] .strtab STRTAB 00000000 0002d4 000016 00 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings) I (info), L (link order), G (group), x (unknown) O (extra OS processing required) o (OS specific), p (processor specific)$ gcc -o test test.o libmyprintf.o$ readelf -l test #我们发现，test和test.o,libmyprintf.o相比，多了很多节区，如.interp和.init等Elf file type is EXEC (Executable file)Entry point 0x80482b0There are 7 program headers, starting at offset 52Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align PHDR 0x000034 0x08048034 0x08048034 0x000e0 0x000e0 R E 0x4 INTERP 0x000114 0x08048114 0x08048114 0x00013 0x00013 R 0x1 [Requesting program interpreter: /lib/ld-linux.so.2] LOAD 0x000000 0x08048000 0x08048000 0x0047c 0x0047c R E 0x1000 LOAD 0x00047c 0x0804947c 0x0804947c 0x00104 0x00108 RW 0x1000 DYNAMIC 0x000490 0x08049490 0x08049490 0x000c8 0x000c8 RW 0x4 NOTE 0x000128 0x08048128 0x08048128 0x00020 0x00020 R 0x4 GNU_STACK 0x000000 0x00000000 0x00000000 0x00000 0x00000 RW 0x4 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .interp .note.ABI-tag .hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .rodata .eh_frame 03 .ctors .dtors .jcr .dynamic .got .got.plt .data .bss 04 .dynamic 05 .note.ABI-tag 06 上表给出了可执行文件的如下几个段(segment)， PHDR: 给出了程序表自身的大小和位置，不能出现一次以上。 INTERP: 因为程序中调用了puts（在动态链接库中定义），使用了动态连接库，因此需要动态装载器／链接器(ld-linux.so) LOAD: 包括程序的指令，.text等节区都映射在该段，只读(R) LOAD: 包括程序的数据，.data, .bss等节区都映射在该段，可读写(RW) DYNAMIC: 动态链接相关的信息，比如包含有引用的动态连接库名字等信息 NOTE: 给出一些附加信息的位置和大小 GNU_STACK: 这里为空，应该是和GNU相关的一些信息 这里的段可能包括之前的一个或者多个节区，也就是说经过链接之后原来的节区被重排了，并映射到了不同的段，这些段将告诉系统应该如何把它加载到内存中。这些新的节区来自哪里？它们的作用是什么呢？先来通过gcc的-v参数看看它的后台链接过程。 1234567$ gcc -v -o test test.o myprintf.o #把可重定位文件链接成可执行文件Reading specs from /usr/lib/gcc/i486-slackware-linux/4.1.2/specsTarget: i486-slackware-linuxConfigured with: ../gcc-4.1.2/configure --prefix=/usr --enable-shared --enable-languages=ada,c,c++,fortran,java,objc --enable-threads=posix --enable-__cxa_atexit --disable-checking --with-gnu-ld --verbose --with-arch=i486 --target=i486-slackware-linux --host=i486-slackware-linuxThread model: posixgcc version 4.1.2 /usr/libexec/gcc/i486-slackware-linux/4.1.2/collect2 --eh-frame-hdr -m elf_i386 -dynamic-linker /lib/ld-linux.so.2 -o test /usr/lib/gcc/i486-slackware-linux/4.1.2/../../../crt1.o /usr/lib/gcc/i486-slackware-linux/4.1.2/../../../crti.o /usr/lib/gcc/i486-slackware-linux/4.1.2/crtbegin.o -L/usr/lib/gcc/i486-slackware-linux/4.1.2 -L/usr/lib/gcc/i486-slackware-linux/4.1.2 -L/usr/lib/gcc/i486-slackware-linux/4.1.2/../../../../i486-slackware-linux/lib -L/usr/lib/gcc/i486-slackware-linux/4.1.2/../../.. test.o myprintf.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/i486-slackware-linux/4.1.2/crtend.o /usr/lib/gcc/i486-slackware-linux/4.1.2/../../../crtn.o","categories":[{"name":"programming-language","slug":"programming-language","permalink":"http://ljchen.net/categories/programming-language/"}],"tags":[{"name":"c-language","slug":"c-language","permalink":"http://ljchen.net/tags/c-language/"}]},{"title":"Kafka原理总结","slug":"kafka原理总结","date":"2018-08-25T04:37:40.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2018/08/25/kafka原理总结/","link":"","permalink":"http://ljchen.net/2018/08/25/kafka原理总结/","excerpt":"研究了rocket-mq之后，一直想知道为什么kafka会这么火, 终于有时间学习了一下这块的内容, 简单总结以备以后查阅。","text":"研究了rocket-mq之后，一直想知道为什么kafka会这么火, 终于有时间学习了一下这块的内容, 简单总结以备以后查阅。 部署架构 其实这个架构和rocket-mq比较像，差异是rocket-mq使用了nameserver，而kafka使用zookeeper来做配置协调中心。 组成原理主要涉及kafka从broker启动、生产者发送消息、broker分发消息到消费者消费消息的流程。 服务注册 所有的broker启动后都注册到zookeeper，写到/brokers/ids目录下； Broker组件订阅这个路径，当有borker加入、删除就能够立即知道其broker的信息了； 健康检查机制，Broker长时间丢失连接会被自动回收； 基于此，producer在配置的时候，并不需要填写所有broker的地址，只需要填写几个broker的地址就可以通过他们去发现自己需要写topic的broker了。 Controller的作用 Broker起来后选举出一个controller，controller是使用锁的方式来实现的。即在zookeeper的/controller目录，谁先写入数据，谁就是controller； 新的controller会获得一个递增的epoch，如果其他broker收到老controller发出来的消息，消息体重会带着旧的epoch，就直接忽略消息； 其他broker会一直watch /controller的变化；并等待当前controller挂了之后抢占controller的地位。 controller的职责是管控broker与topic中partition主备映射关系； 当broker挂掉、新加broker时，controller通过watch可以感知到变化；它会通过分析挂掉的broker中的partition的副本来重新选举出partition的leader，并将变化下发到相关的broker； 生产者 Producer先序列化数据，按照topic分类（下面还有key/value），然后按照负载均衡算法push消息到topic的不同partition； Producer发送消息的模式有三种: oneway(结果未知)， sync(阻塞)， async(有回调函数)； Producer可以设置Acks方式，可以设置为: 1. 发送后立即返回(0); 2. 等待主partition保存后返回(1); 3.等待副本patirion复制完成后返回(-1); Topic与Partition Topic可以有多个partition，可以基于各种算法来将消息分类到不同partition，拥有同样key的消息会放到同一个partition； partition是物理实现，可以指定partition的副本数； Topic下的partition数量可以递增，但是不能递减； Kafka的设计也是源自生活，好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了。 分区再分配 新增的broker是不会自动地分担己有topic的负载的，它只会对增加broker后新创建的topic生效； 如果要让新增 broker为己有的 topic服务，用户必须手动地调整己有topic的分区分布，将一部分分区搬移到新增broker上。这就是分区重分配(partition reassignment)； 可以基于自带的脚本kafka-reassign-partitions.sh来实现，该脚本分为generate和execute阶段，generate阶段可以看到系统推荐分配的方案； 使用脚本kafka-reassign-partitions.sh，同样可以为topic增加副本数。 Parition主备复制 kafka的高可用是通过topic的每一个partition多个副本来实现的； partition的副本负责从leader拷贝数据，数据中包含了offset等信息； kafka的leader负责处理外部的读写操作，follower不对外服务，只有备份的作用； partition的副本会发送回复信息给leader，leader基于此来判断副本是否保持同步； 只有同步的分区副本才能在leader挂后被选为新的leader； 消费者只能看到已经被复制到ISR（in-sync replica）的消息，分区副本从leader复制消息之前，理论上leader是不允许该消息被consumer消费的，因为这样的消息不安全。 因为各种各样的原因，一小部分replica开始落后于leader replica的进度。当滞后到一定程度(replica.lag.time.max.ms)时，Kafka会将这些replica “踢”出 ISR。当这些replica重新“追上”了leader的进度时，那么Kafka会将它们加回到ISR中。 kafka几个关键数据: HW（high water）代表follower已经同步的offset；LEO（log end offset）代表leader/follower的数据结尾offset。LAG 代表consumer的偏移，（LAG = HW - Consumer offset）。 消费者组再均衡 这里的再均衡是针对consumer group中的consumer来讲！ Topic如果增加partition，或者consumer group中的consumer有变动，均需要重新分配分区（再均衡）； Consumer的变动是通过定期向broker协调者发送心跳报文来实现的，心跳报文中包含了群属关系以及分区所有权关系； 在再均衡期间，消费者无法读取消息； Consumer读取消息，并将offset发送到一个__consumer_offset的特殊topic中，该消息包含了每个分区的偏移量； 一旦再均衡后，新的consumer可以继续按照之前的offset工作； __consumer_offset 可以把它想象成一个KV格式的消息，key就是一个三元组: group.id+topic+分区号，而value就是offset的值。 每次consumer读取了消息就往该topic发送消息，kafka负责执行压缩算法，只保留最新的offset数据。 该topic默认有50个partition Rebalance协议 JoinGroup请求 consumer请求加入组 (当收集全JoinGroup 请求后， coordinator从中选择一个consumer担任group的 leader，并把所有成员信息以及它们的订阅信息发送给leader)。 SyncGroup请求 group leader把分配方案同步更新到组内所有成员中(一旦分配完成，leader会把这个分配方案封装进 SyncGroup请求并发送给coordinator。组内所有成员都会发送SyncGroup请求，不过只有leader发送的SyncGroup请求中包含了分配方案。coordinator接收到分配方案后把属于每个consumer的方案consumer group分配方案是在consumer端执行的单独抽取出来作为syncGroup请求的response返还给各自的 consumer)。 Heartbeat请求 consumer定期向coordinator汇报心跳表明自己依然存活。 LeaveGroup请求 consumer主动通知coordinator该consumer即将离组。 DescribeGroup请求 查看组的所有信息，包括成员信息、协议信息、分配方案以及订阅信息等 。该请求类型主要供管理员使用 。 coordinator不使用该请求执行 rebalance。 消息存储 Broker的消息保存策略为： 保存指定时间; 保存达到某一Size; Log文件创建topic时，Kafka为该topic的每个分区在文件系统中创建了一个对应的子目录，名字是: &lt;topic&gt;-&lt;分区号&gt;, 如operation-product-sync-4。 1234567891011121314[root@centos operation-product-sync-4]# ll -htotal 5.9G-rw-r--r-- 1 root root 105K Oct 4 08:15 00000000000002227127.index-rw-r--r-- 1 root root 1.0G Oct 4 08:15 00000000000002227127.log-rw-r--r-- 1 root root 151K Oct 4 08:15 00000000000002227127.timeindex-rw-r--r-- 1 root root 293K Oct 4 13:10 00000000000002247767.index-rw-r--r-- 1 root root 1.0G Oct 4 13:10 00000000000002247767.log-rw-r--r-- 1 root root 10 Oct 4 08:15 00000000000002247767.snapshot-rw-r--r-- 1 root root 428K Oct 4 13:10 00000000000002247767.timeindex-rw-r--r-- 1 root root 10M Oct 4 19:03 00000000000002455169.index-rw-r--r-- 1 root root 409M Oct 4 19:03 00000000000002455169.log-rw-r--r-- 1 root root 10 Oct 4 13:10 00000000000002455169.snapshot-rw-r--r-- 1 root root 10M Oct 4 19:03 00000000000002455169.timeindex-rw-r--r-- 1 root root 15 Oct 4 13:13 leader-epoch-checkpoint .log文件是日志段文件，保存着真实的Kafka记录，Kafka使用该文件第一条记录对应的offset来命名此文件（默认最大1G）； .index和.timeindex文件都是与日志段对应的位移索引文件和时间戳索引文件；每写入（log.index.interval.bytes，默认4K）数据才更新一次索引文件。 位移索引保存的是与索引文件起始位移的差值。索引文件文件名中的位移就是该索引文件的起始位移； 时间戳索引项保存的是时间戳与位移的映射关系，给定时间戳之后根据此索引文件只能找到不大于该时间戳的最大位移，稍后还需要拿着返回的位移再去位移索引文件中定位真实的物理文件位置。 log compaction 只会根据某种策略有选择性地移除log中的消息，而不会变更消息的offset值。kafka通过cleaner组件来压实log；该配置是topic级别的，必须指定key。 消费者 Kafka支持Consumer group，它可以保障每一个group中只有一个consumer消费某条消息； 如果要广播消息，需要创建多个consumer group； Consumer的数量不要超过分区数量，否者某些consumer会闲置。 consumer group 一个consumer group可能有若干个consumer实例(一个group只有一个实例也是允许的): 对于同一个group而言，topic的每条消息只能被发送到group下的一个consumer实例上: topic消息可以被发送到多个group中。 调优吞吐量broker 端 适当增加 num.replica.fetchers，但不超过CPU核数。 调优 GC 避免经常性的Full GC。 producer 端 适当增加 batch.size，比如 100~512 KB 适当增加 linger.ms，比如 10~100 毫秒 设置 compression均pe=lz4 acks=0 或 1 retries = 0 若多线程共享 producer或分区数很多，增加 buffer.memory consumer 端 采用多 consumer实例。 增加 fetch.min.bytes，比如 100000。 时延broker 端 适度增加 num.replica.fetchers。 避免创建过多topic分区。 producer 端 设置 linger.ms=0 设置 compression.type=none 设置 acks=1 或 0 consumer 端 设置 fetch.min.bytes=1。 持久性broker 端 设置 auto.create.topics.enable=false 设置 replication.factor= 3, min.insync.replicas = replication.factor - 1 设置 default.replication.factor=3 设置 broker.rack属性分散分区数据到不同机架 设置 log.flush.interval.message 和 log.flush.interval.ms为一个较小的值 producer 端 设置 acks=all 设置retries为一个较大的值，比如 10~300 设置 max.in.flight.requests.per.connection=1 设置 enable.idempotence=true 启用幂等性 consumer 端 设置 auto.commit.enable=false 消息消费成功后调用 commitSync 提交位移； 可用性broker 端 避免创建过多分区 设置 unclean.leader.election.enable=true 设置 min.insync.replicas=1 设置 num.recovery.threads.per.data.dir=broker端参数log.dirs中设置的目录数 producer 端 设置 acks=1，若一定要设置为all，则遵循上面broker端的min.insyn.replicas配置 consumer 端 设置 session.timeout.ms为较低的值，比如 10000 设置 max.poll.interval.ms为比消息平均处理时间稍大的值 实践规划CPU: 使用多核（ &gt; 8core），但是对单核的主频要求不高；内存: 主要用于page/cache，jvm使用的heap内存并不多(&lt; 6G)；磁盘: 顺序读，HDD盘基本够用，没必要上SSD； 典型配置： CPU 24核。 内存 32GB。 磁盘 1TB 7200转 SAS盘两块。 带宽 1Gb/s。 ulimit -n 1000000 (永久生效：vim /etc/security/limits.conf)。 Socket Buffer至少 64KB 一一 适用于跨机房网络传输。 kafka在linux服务器上，下载并启动kafka服务。 1234567891011# 下载cd /opt/wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.3.0/kafka_2.12-2.3.0.tgztar -xzf kafka_2.12-2.3.0.tgzcd kafka_2.12-2.3.0# 启动zookeepernohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp;# 启动kafkanohup bin/kafka-server-start.sh config/server.properties &amp; 验证消息的发送和消费。 123456789101112# 创建topicbin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic test# 查看topic列表bin/kafka-topics.sh --list --bootstrap-server localhost:9092# 发送消息到topicbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test~~~~~~~ # 你要发送的内容# 消费topic中的消息bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning kafka-manager虽然我们能够通过脚本的方式来操作kafka消费消息，但是没有一个理想的portal还是感觉这货不过直观，接下来介绍的就是一个差不多算是最好的kafka的dashboard，kafka-manager，由yahoo开发。 我们通过docker的方式来启动，镜像的具体信息参考这里 12345docker run -d \\ -p 9000:9000 \\ -e ZK_HOSTS=\"&lt;zookeeper-ip&gt;:2181\" \\ hlebalbau/kafka-manager:stable \\ -Dpidfile.path=/dev/null 在浏览器中访问 http://{ip}:9000，可以看到界面。通过添加cluster，其实就是指定一下kafka对应zookeeper的地址；好了之后可以查看kafka集群的broker，topic，以及partition，consumer等信息。其界面如下图所示： zookeeper UI前面已经说到kafka没有UI感觉很懵逼，那zookeeper其实也是如此。我们知道zookeeper一方面是做分布式协调，另一方面也是作为DB来存储了kafka和kafka-manager的配置信息。这里我们也介绍一个工具来查看zookeeper中都保存了些什么东东，有助于我们了解kafka的原理。 这里要介绍的项目为zkui, 有兴趣可以去github了解一下。 1docker run -d --name zkui -p 9090:9090 -e ZK_SERVER=&lt;zookeeper-ip&gt;:2181 rootww/zkui 启动之后，在浏览器访问http://{ip}:9090 就可以看到如下的界面（页面上有登录的账号信息，默认是admin/manager）。里面可以看到关于kafka和kafka-manager的配置信息。 点击页面上的export菜单，还可以直接导出所有的zookeeper配置，这样便于我们直接查看各个key的value。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#App Config Dashboard (ACD) dump created on :Sun Aug 11 11:21:04 CST 2019/admin=delete_topics=/brokers/ids=0=&#123;\"listener_security_protocol_map\":&#123;\"PLAINTEXT\":\"PLAINTEXT\"&#125;,\"endpoints\":[\"PLAINTEXT://master:9092\"],\"jmx_port\":-1,\"host\":\"master\",\"timestamp\":\"1565487511830\",\"port\":9092,\"version\":4&#125;/brokers/topics/__consumer_offsets/partitions/0=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/10=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/11=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/12=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/13=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/14=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/15=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/16=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/17=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/18=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/19=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/1=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/20=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/21=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/22=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/23=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/24=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/25=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/26=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/27=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/28=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/29=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/2=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/30=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/31=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/32=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/33=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/34=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/35=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/36=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/37=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/38=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/39=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/3=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/40=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/41=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/42=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/43=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/44=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/45=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/46=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/47=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/48=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/49=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/4=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/5=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/6=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/7=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/8=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/__consumer_offsets/partitions/9=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/test/partitions/0=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/topic-1/partitions/0=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/topic-1/partitions/1=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers/topics/topic-1/partitions/2=state=&#123;\"controller_epoch\":1,\"leader\":0,\"version\":1,\"leader_epoch\":0,\"isr\":[0]&#125;/brokers=seqid=/cluster=id=&#123;\"version\":\"1\",\"id\":\"YA5L4mM-S-aHeYeatKtRKw\"&#125;/config/topics=__consumer_offsets=&#123;\"version\":1,\"config\":&#123;\"segment.bytes\":\"104857600\",\"compression.type\":\"producer\",\"cleanup.policy\":\"compact\"&#125;&#125;/config/topics=test=&#123;\"version\":1,\"config\":&#123;&#125;&#125;/config/topics=topic-1=&#123;\"version\":1,\"config\":&#123;&#125;&#125;/config=brokers=/config=changes=/config=clients=/config=users=/=consumers=/=controller=&#123;\"version\":1,\"brokerid\":0,\"timestamp\":\"1565487511918\"&#125;/=controller_epoch=1/=isr_change_notification=/kafka-manager/clusters/haha=topics=172.17.0.4/kafka-manager/configs=haha=&#123;\"name\":\"haha\",\"curatorConfig\":&#123;\"zkConnect\":\"10.200.204.67:2181\",\"zkMaxRetry\":100,\"baseSleepTimeMs\":100,\"maxSleepTimeMs\":1000&#125;,\"enabled\":true,\"kafkaVersion\":\"2.2.0\",\"jmxEnabled\":true,\"jmxUser\":null,\"jmxPass\":null,\"jmxSsl\":false,\"pollConsumers\":true,\"filterConsumers\":false,\"logkafkaEnabled\":false,\"activeOffsetCacheEnabled\":false,\"displaySizeEnabled\":false,\"tuning\":&#123;\"brokerViewUpdatePeriodSeconds\":30,\"clusterManagerThreadPoolSize\":2,\"clusterManagerThreadPoolQueueSize\":100,\"kafkaCommandThreadPoolSize\":2,\"kafkaCommandThreadPoolQueueSize\":100,\"logkafkaCommandThreadPoolSize\":2,\"logkafkaCommandThreadPoolQueueSize\":100,\"logkafkaUpdatePeriodSeconds\":30,\"partitionOffsetCacheTimeoutSecs\":5,\"brokerViewThreadPoolSize\":8,\"brokerViewThreadPoolQueueSize\":1000,\"offsetCacheThreadPoolSize\":8,\"offsetCacheThreadPoolQueueSize\":1000,\"kafkaAdminClientThreadPoolSize\":8,\"kafkaAdminClientThreadPoolQueueSize\":1000,\"kafkaManagedOffsetMetadataCheckMillis\":30000,\"kafkaManagedOffsetGroupCacheSize\":1000000,\"kafkaManagedOffsetGroupExpireDays\":7&#125;,\"securityProtocol\":\"PLAINTEXT\",\"saslMechanism\":null,\"jaasConfig\":null&#125;/kafka-manager/mutex=leases=/kafka-manager/mutex=locks=/kafka-manager=deleteClusters=172.17.0.4/=latest_producer_id_block=&#123;\"version\":1,\"broker\":0,\"block_start\":\"0\",\"block_end\":\"999\"&#125;/=log_dir_event_notification=","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"kafka","slug":"kafka","permalink":"http://ljchen.net/tags/kafka/"}]},{"title":"Docker核心原理","slug":"docker核心原理","date":"2018-08-12T08:45:52.000Z","updated":"2020-04-06T09:24:01.480Z","comments":true,"path":"2018/08/12/docker核心原理/","link":"","permalink":"http://ljchen.net/2018/08/12/docker核心原理/","excerpt":"每次看docker相关的资料，都是零零碎碎的，没有系统性的去把所有知识融汇起来。最近花时间好好学习了一下，基本算是能够把docker所用的linux相关的知识给串通起来；这里简单记录一下，希望便于后续进一步学习或者对后来人有一定的帮助。","text":"每次看docker相关的资料，都是零零碎碎的，没有系统性的去把所有知识融汇起来。最近花时间好好学习了一下，基本算是能够把docker所用的linux相关的知识给串通起来；这里简单记录一下，希望便于后续进一步学习或者对后来人有一定的帮助。 核心技术docker是一个牛X的产品，它所用到的核心技术，在linux中已经有较老的历史了，有很多人早已看到linux的这些特性，也不乏有类似docker这样使用这些技术的公司。但是Solomon Hykes和他年青的小伙伴儿们看到这这个商机，成立Dotcloud公司，把创建“大规模的创新工具”的想法付诸现实；最终发展成为被大众接纳，甚至渐渐爱不释手的docker。 那么，到底docker的幕后都用到了哪些更牛X的技术呢？我们就来聊下那些幕后英雄们！ NameSpace Cgroup AUFS NameSpaceNamespace为docker的进程（容器）作了逻辑隔离。 接触较多，也比较好理解的可能莫属linux network namespace了。我们可以在linux上手动执行命令ip net add net-1, 就可以在创建一个名为net-1的网络命名空间，这个命名空间就相当于是位于linux系统中另一个逻辑网络区域，它和系统当前的命名空间相互隔离。 有了上面形象的理解后，我们再来看linux下docker使用到的其他命名空间（以下各个参数，都可以在/proc/{pid}/ns/*目录下查询到）： UTSUTS Namespace 主要用来隔离nodename和domainname两个系统标识。在UTSNamespace里, 每个Namespace都允许有自己的hostname，docker就是靠它来保障容器的hostname与宿主机不一样的。 PIDPID Namespace 用来隔离进程ID。同一个进程在不同的PID Namespace里可以拥有不同的PID，比如每一个container中的1号进程，其实在宿主机上都有自己的进程ID，我们如果要在外部操作它，就需要通过docker inspect等命令来找到它在宿主机上的ID号，并对其进行操作。 在C语言中，创建新的PID命名空间，需要调用clone()系统函数，并且传入CLONE_NEWPID参数。指定该参数后，子进程无法获取到parent pid的信息，认为自己没有父进程（已经被隔离）。 IPCIPC Namespace 用来隔离System V IPC和POSIX message queues。每一个IPC Namespace都有自己的System V IPC和POSIX message queue。这一点通过在容器内外分别执行ipcs -a可以验证。 MOUNTMount Namespace 用来隔离各个进程看到的挂载点视图。在不同Namespace的进程中，看到的文件系统层次是不一样的。在Mount Namespace中调用 mount()和 umount()仅仅只影响当前Namespace的文件系统，而对全局的文件系统是没有影响的。 在C语言中，需要在clone()函数中传入CLONE_NEWNS参数。 NET这块在开篇已经讲到，主要是将新启的进程放到自己的network namespace中，保障与外部网络的隔离性。 根命名空间和子命名空间中的通信，可以通过使用veth来实现，veth创建后默认在根命名空间中，只需要将一个veth的另一端放入对应的子NS中即可。 USER用来隔离用户和用户组，一个进程的user id和group id在namespace内部与在宿主机上是不同的。比如宿主机上非root用户，在容器内部可以是root用户。 实践在golang的实现中，我们应该熟悉以下代码: 123456789cmd := exec.Command(initCmd, \"init\")cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNET | syscall.CLONE_NEWIPC,&#125;// ...省略...if err := cmd.Start(); err != nil &#123; log.Error(err) &#125; 上面代码在创建子进程的时候，就分别指定了各个命名空间的参数，这样创建出的子进程就拥有对应的子命名空间。 Cgroup如果说namespace主要为docker作了进程间的隔离，那Cgroup就真正为这层隔离加上物理的资源限制。它提供了对一组进程及将来子进程的资源限制、控制和统计的能力，这些资源包括CPU、内存、存储、网络等。 Cgroup主要包含三大组件： hierarchy 子系统 进程组 hierarchyhierarchy 是把一组cgroup串成一个树状结构，一个这样的树便是一个hierarchy，通过这种树状结构，Cgroups可以做到继承。我们甚至可以将其理解为cgroup文件系统，在挂载了该文件系统的目录下，创建子目录就会有对应的cgroup相关文件生成；而这种目录结构是可以层层继承的，下面hierarchy对应子系统的能力也随着这样的继承关系而层层继承。 子系统 cpu – 使用调度程序提供对 CPU 的 cgroup 任务访问。 cpuacct – 自动生成 cgroup 中任务所使用的 CPU 报告。 cpuset – 为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。 devices – 可允许或者拒绝 cgroup 中的任务访问设备。 memory – 设定 cgroup 中任务使用的内存限制，并自动生成由那些任务使用的内存资源报告。 blkio – 为块设备设定输入/输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）。 net_cls – 使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。 ns – 名称空间子系统。 freezer – 挂起或者恢复 cgroup 中的任务。 在linux上，这些子系统都对应已经挂载到了对应的目录。假设docker要为某一个容器指定memory的限制，在ubuntu下，docker会直接在/sys/fs/cgrop/memory/docker/下创建目录（名称为容器ID），并在目录下创建对应的限制。 1234567891011121314151617181920root@vpn:/sys/fs/cgroup# ls /sys/fs/cgroup/ # &lt;=这里是系统所有的子系统blkio cpu cpuacct cpu,cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd root@vpn:/sys/fs/cgroup#root@vpn:/sys/fs/cgroup# mount -l | grep cgroup # &lt;=查看其挂在路径tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)root@vpn:/sys/fs/cgroup#root@vpn:/sys/fs/cgroup# ls ./cpu/docker/ # &lt;=查看cpu下docker的目录，可以看到有一个容器正在被cpu subsystem控制7bdb229c1974123cfa6f408a142bcf496c46c04381087f0f2878c5c61763fe44 cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.shares notify_on_releasecgroup.clone_children cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.stat tasks 进程组cgroup文件系统中对应每一个子目录，都会自动创建tasks文件，而受该subsystem限制的进程号就逐一列到该task文件中，如下： 123456root@vpn:/sys/fs/cgroup/cpu/docker/7bdb229c1974123cfa6f408a142bcf496c46c04381087f0f2878c5c61763fe44# lscgroup.clone_children cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.stat taskscgroup.procs cpuacct.usage cpu.cfs_period_us cpu.shares notify_on_releaseroot@vpn:/sys/fs/cgroup/cpu/docker/7bdb229c1974123cfa6f408a142bcf496c46c04381087f0f2878c5c61763fe44#root@vpn:/sys/fs/cgroup/cpu/docker/7bdb229c1974123cfa6f408a142bcf496c46c04381087f0f2878c5c61763fe44# cat tasks32186 AUFS这里不讲AUFS的概念，直接讲这个流程： 首先，docker启动容器需要有一个镜像，但是这个镜像解压缩后不允许被修改，是readOnly的； 接下来，你启动容器后，要修改容器里面的文件，docker就为container创建了另外一个目录，你对镜像做的任何操作的内容都位于这个叠加的文件上； 对于container来讲，它看不到后面那些文件是readOnly，哪些是readWrite的，它就像用户一样，其实都在使用这个虚拟的联合文件系统，这就是AUFS。 为了提高效率，AUFS使用了写时复制技术： 写时复制（ copy-on-write），也叫隐式共享， 是一种对可修改资源实现高效复制的资源管理技术。它的思想是，如果一个资源是重复的，但没任何修改，这时并不需要立即创建一个新的资源，这个资源可以被新旧实例共享。创建新资源发生在第一次写操作，也就是对资源进行修改的时候。通过这种资源共享的方式，可以显著地减少 未修改资源复制带来的消耗，但是也会在进行资源修改时增加小部分的开销。 具体到docker上，aufs的实现体现在 /var/lib/docker/aufs/ 目录下的三个文件夹:12root@vpn:/var/lib/docker/aufs# lsdiff layers mnt 容器最终mount的是 mnt 目录下的文件，而readWrite的层都位于 diff 目录中。所有，我们要实现一个commit命令，本质就是将容器位于 mnt 中的文件打包出来。 实践东拉西扯的讲了一堆核心原理，突然觉得其他也没啥好讲的了，就来看看docker的创建流程吧。 docker run 主进程(也就是runc) 这里的创建父进程，其实就是container进程，只是首先运行的命令是docker init（该进程启动之后会阻塞，等待从管道读取信息）； 然后docker run 在这个阶段需要为container的1号进程准备各种设置，也就是前面提到的各种namespace，cgroup、aufs等； 同样，网络和存储，涉及外部需要准备的资源都需要docker run进程来处理； container进程(容器1号进程) 主要等待主进程ready后，从管道中读取各种配置信息，比如环境变量等； 设置根目录，挂载自己的 /proc 和 /tmpfs等，这样才能在内部查询到进程号； 执行启动容器中指定的entrypoint（虽然进程号一样，但是之前执行的是docker in 各种架构DockerD - Containerd - OCI 其实就是我们有一个oci的config文件，然后结合runc就可以运行起来一个容器了。至于containd就是访问runc的一层rpc，它同时实现对image的一些管理功能（比如pull镜像）。 从下图中可以看到，dockerd 还管理了docker-proxy为容器配置网络；而containerd-shim就是runc的实现，每一个容器都有一个container-shim与之一一对应，这样就算dockerd挂了，也不影响container。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@qcloud-vm system]# docker -vDocker version 18.09.2, build 6247962[root@qcloud-vm system]#[root@qcloud-vm system]# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2019-02-20 16:19:55 CST; 18h ago Docs: https://docs.docker.com Main PID: 20233 (dockerd) Tasks: 10 Memory: 106.5M CGroup: /system.slice/docker.service └─20233 /usr/bin/dockerd -H fd://[root@qcloud-vm system]#[root@qcloud-vm system]# systemctl status containerd● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2019-02-20 15:53:52 CST; 18h ago Docs: https://containerd.io Main PID: 14753 (containerd) Tasks: 30 Memory: 85.7M CGroup: /system.slice/containerd.service ├─14753 /usr/bin/containerd ├─19935 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/58622134b6ce3184b11b91ae1bd5b357bbb247d... └─22097 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/48fa22b740d033bfe0754c887c891d3aad8fb5f...[root@qcloud-vm system]#[root@qcloud-vm system]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES48fa22b740d0 nginx \"nginx -g 'daemon of…\" 2 seconds ago Up 1 second 0.0.0.0:80-&gt;80/tcp nginx-158622134b6ce nginx \"nginx -g 'daemon of…\" 20 minutes ago Up 20 minutes 80/tcp nginx[root@qcloud-vm system]#[root@qcloud-vm system]# ps -ef | grep containerdroot 14753 1 0 Feb20 ? 00:00:23 /usr/bin/containerdroot 19935 14753 0 10:24 ? 00:00:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/58622134b6ce3184b11b91ae1bd5b357bbb247d6e8e3abeb022f2b878a733b75 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runcroot 22097 14753 0 10:45 ? 00:00:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/48fa22b740d033bfe0754c887c891d3aad8fb5f6494f28c38be33f18972ba22c -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc[root@qcloud-vm system]#[root@qcloud-vm system]# pstree -a | grep containerd |-containerd | |-containerd-shim -namespace moby -workdir... | | `-9*[&#123;containerd-shim&#125;] | |-containerd-shim -namespace moby -workdir... | | `-9*[&#123;containerd-shim&#125;] | `-10*[&#123;containerd&#125;] | |-grep --color=auto containerd[root@qcloud-vm system]#[root@qcloud-vm system]# ps -ef | grep docker-proxyroot 22092 20233 0 10:45 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.3 -container-port 80root 22262 19774 0 10:46 pts/0 00:00:00 grep --color=auto docker-proxy[root@qcloud-vm system]#[root@qcloud-vm system]# ps -ef | grep docker-proxyroot 22092 20233 0 10:45 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.3 -container-port 80root 22262 19774 0 10:46 pts/0 00:00:00 grep --color=auto docker-proxy[root@qcloud-vm system]# ps -ef | grep 20233root 20233 1 0 Feb20 ? 00:00:17 /usr/bin/dockerd -H fd://root 22092 20233 0 10:45 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.3 -container-port 80root 22305 19774 0 10:46 pts/0 00:00:00 grep --color=auto 20233 通过上面的关系实践，我们基本了解了整个docker中进程的关系： 系统启动的时候启动了docker daemon和containerd两个守护进程； 用户docker daemon响应docker client的请求，并下发请求到containerd； 如果container指定了端口映射，docker daemon还会启动docker-proxy来做端口映射； containerd是通过containerd-shim来运行runc，并真正创建容器的。 CRI-O k8s代码对pod做了很多抽象，要将pod映射到container需要一层转换（比如现在是运行了dockerService，在里面实现containerManager，Network等的管理），而K8S试图取消中间经过containerd的管理，直接到达CRI，当前已有crictl之类的工具。","categories":[{"name":"container","slug":"container","permalink":"http://ljchen.net/categories/container/"}],"tags":[{"name":"container","slug":"container","permalink":"http://ljchen.net/tags/container/"}]},{"title":"微服务网关Kong实践","slug":"微服务网关Kong实践","date":"2018-08-11T09:44:39.000Z","updated":"2020-04-06T09:24:01.501Z","comments":true,"path":"2018/08/11/微服务网关Kong实践/","link":"","permalink":"http://ljchen.net/2018/08/11/微服务网关Kong实践/","excerpt":"之前做PaaS平台使用过新浪基于OpenResty实现蓝绿部署和灰度发布的方案。这几天倒腾了一下大名鼎鼎的Kong，对其新版本中的概念抽象，以及环境搭建、plugin编程规则等内容大概记录一下。 Kong介绍 API网关是所有的客户端和消费端统一的接入点，它对消费者屏蔽了后端的微服务，并处理非业务功能。通常，网关也提供REST/HTTP的访问API，服务端通过API-GW注册和管理服务。 Kong是一个可扩展的开放源码的API中间件, 它在任何RESTful API的前面运行，通过插件扩展，它提供了超越核心平台的额外功能和服务。Kong是基于OpenResty（不要问我Openresty是不是基于Nginx），在其基础上做了一系列的抽象概念、提供了相应的Lua脚本编程框架和库；同时，它也提供了集群方案，是一个满足生产级的API网关解决方案。","text":"之前做PaaS平台使用过新浪基于OpenResty实现蓝绿部署和灰度发布的方案。这几天倒腾了一下大名鼎鼎的Kong，对其新版本中的概念抽象，以及环境搭建、plugin编程规则等内容大概记录一下。 Kong介绍 API网关是所有的客户端和消费端统一的接入点，它对消费者屏蔽了后端的微服务，并处理非业务功能。通常，网关也提供REST/HTTP的访问API，服务端通过API-GW注册和管理服务。 Kong是一个可扩展的开放源码的API中间件, 它在任何RESTful API的前面运行，通过插件扩展，它提供了超越核心平台的额外功能和服务。Kong是基于OpenResty（不要问我Openresty是不是基于Nginx），在其基础上做了一系列的抽象概念、提供了相应的Lua脚本编程框架和库；同时，它也提供了集群方案，是一个满足生产级的API网关解决方案。 部署实践由于安装Openresty需要各种依赖，在MacBook上安装了很多次最后都放弃了；最后还是采用docker的方式来部署（不得不又膜拜一下docker，技术都是linux的老技术，重点是如何把这些技术组合成完美的产品。） docker-network1docker network create kong-net 安装数据库这里我们采用postgres数据库方案： 123456docker run -d --name kong-database \\ --network=kong-net \\ -p 5432:5432 \\ -e \"POSTGRES_USER=kong\" \\ -e \"POSTGRES_DB=kong\" \\ postgres:9.6 然后需要初始化数据库表结构： 123456docker run --rm \\ --network=kong-net \\ -e \"KONG_DATABASE=postgres\" \\ -e \"KONG_PG_HOST=kong-database\" \\ -e \"KONG_CASSANDRA_CONTACT_POINTS=kong-database\" \\ kong:latest kong migrations bootstrap 安装Kong运行Kong需要注意，指定proxy的端口号 KONG_PROXY_LISTEN 并暴露容器的端口出去。如果要使用自定义的lua plugin，还需要使用环境变量KONG_CUSTOM_PLUGINS指定插件的名称。 1234567891011121314151617181920docker run -d --name kong \\ --network=kong-net \\ -e \"KONG_DATABASE=postgres\" \\ -e \"KONG_PG_HOST=kong-database\" \\ -e \"KONG_CASSANDRA_CONTACT_POINTS=kong-database\" \\ -e \"KONG_PROXY_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_ADMIN_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_PROXY_ERROR_LOG=/dev/stderr\" \\ -e \"KONG_ADMIN_ERROR_LOG=/dev/stderr\" \\ -e \"KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\" \\ -p 8000:8000 \\ -p 8443:8443 \\ -p 8001:8001 \\ -p 8444:8444 \\ kong:latest # 如果使用自定义plugin-xxx，需要指定 -e \"KONG_CUSTOM_PLUGINS=plugin-xxx\" \\ # 如果指定使用DNS服务器，可以用consul做dns服务器 -e \"KONG_DNS_RESOLVER=x.x.x.x:8600\" \\ 至此，在本地访问管理端API: http://localhost:8001 应该能够返回数据了，说明kong已经开始工作了。 安装图形化管理界面这里使用konga来管理，界面比较美观，只是暂时还没有中文版。 12345docker run -d -p 1337:1337 \\ --network=kong-net \\ --name konga \\ -e \"NODE_ENV=development\" \\ pantsel/konga 安装好后，我们登录上去，默认登录用户名密码为： Admin login: adminPassword: adminadminadmin 将管理的kong地址设置为: http://kong:8001 即可开启Kong之旅了。 操作顺序依次为： upstream upstream’s target service service’s router 抽象概念下面来讲讲kong里面抽象的各种概念，与Nginx中有一些差异。 抽象 upstream: 同nginx的upstream，kong会对upstream下的各个target做健康检查，基于健康检查结果流量负载均衡；因此，其调度的最小单元为upstream。 target: 就是一个具体的主机或者微服务实例，需要指定Ip和服务Port； service：我的理解是这里的service是为了对应于某一个微服务，现在的用法都是一个service对应一个upstream； router：类似于kubernetes上ingress的概念，分流的粒度也是可以达到某一个域名下的sub-path上；在konga上，router只能够在service创建之后为该service指定其外部ingress的规则。 理想方案Kong为了适配微服务架构，虽然抽象出了service的概念，但是其upstream的增加还是无法做到自动化。如果能够集成consul之类的服务注册发现工具，将是另一片美好的蓝天。但是… 我们后面会在讲到lua脚本hook其实并没有办法处理这些类似于外部配置的问题。因此如果希望达成这一理想，还需要在外部开发一个程序，该程序实现watch consul上passing的服务，并及时的调用kong admin的api来实现动态更新upstream的target。 PluginKong的强大就在于它提供了众多的Plugins，进到kong的docker容器，在lua的路径下(/usr/local/share/lua/5.1/kong/plugins/*)可以查看到系统自带的所有lua plugin。 1234567/usr/local/share/lua/5.1/kong/plugins # lsacl correlation-id http-log loggly request-size-limiting syslogaws-lambda cors ip-restriction oauth2 request-termination tcp-logazure-functions datadog jwt post-function request-transformer udp-logbase_plugin.lua file-log key-auth pre-function response-ratelimiting zipkinbasic-auth galileo ldap-auth prometheus response-transformerbot-detection hmac-auth log-serializers rate-limiting statsd 授人以鱼不如授人以渔，我们今天不为Kong自带的Plugin打广告，我们讲讲如何实现一个自己的Plugin： 原理上面拍拍贷的一幅图，详细的阐述了Openrestry在nginx转发的过程中设置hook的地方（可以参考kong-nginx.conf配置文件），如果对于该将自己的操作放到哪个函数中实现还不清楚的可以参考下面。 kong源码导读 &lt;- 这篇文章值得看看 init_by_lua 发生在master进程启动阶段。这里会对数据访问层进行初始化，加载插件的代码，构造路由规则表。 init_worker_by_lua 发生在worker进程启动阶段。这里会开启数据同步机制，执行每个插件的init_worker方法。 set_by_lua 处理请求第一个执行阶段。这里可以做一些流程分支处理判断变量初始化。kong没有使用该阶段。 rewrite_by_lua 这里可以对请求做一些修改。kong在这里会把处理代理给插件的rewrite方法。 access_by_lua kong在这里对请求进行路由匹配，找到后端的upstream服务的节点。 balancer_by_lua kong在这里会把上一阶段找到的服务节点设置给nginx的load balancer。如果设置了重试次数，此阶段可能会被执行多次。 header_filter_by_lua 这里可以对响应头做一些处理。kong在这里会把处理代理给插件的header_filter方法。 body_filter_by_lua 这里可以对响应体做一些处理。kong在这里会把处理代理给插件的body_filter方法。 log_by_lua kong在这里会通过插件异步记录日志和一些metrics数据。 实现流程创建Plugin需要定义在目录下创建两个文件： schema.lua 指定配置文件 handler.lua 处理逻辑 schema.lua中需要return出对应的fields（fields里面包含了所有的配置），比如： 12345678910return &#123; no_consumer = true, fields = &#123; RedisHost = &#123; type = \"string\", required = false, default = \"127.0.0.1\" &#125;, &#125;&#125; 然后在handler.lua中实现各种业务逻辑，这里需要注意的是：所有的plugin都需要集成Kong已经定义好的BasePlugin类，而该类中具体的方法见上文。 12345678910111213141516171819202122local BasePlugin = require \"kong.plugins.base_plugin\"local access = require \"kong.plugins.auth.access\"local rewrite = require \"kong.plugins.auth.rewrite\"local AuthHandler = BasePlugin:extend()AuthHandler.PRIORITY = 1001AuthHandler.VERSION = \"0.1.0\"function AuthHandler:new() AuthHandler.super.new(self, \"auth\")endfunction AuthHandler:access(conf) AuthHandler.super.access(self) access.execute(conf)endfunction AuthHandler:rewrite(conf) AuthHandler.super.rewrite(self) rewrite.execute(conf)endreturn AuthHandler 对应access和rewrite的实现就可以写到auth目录下的access.lua 和rewrite.lua中，这里只需要在reqire引入auth目录下的access和rewrite就行了。kong的Plugin是有优先级的，每一个plugin都需要设置一个优先级: AuthHandler.PRIORITY 貌似数值越大越优先。 另外，这里卖个关子。kong里面有众多操作db或者IO的行为，这都应该是一个较耗时的操作，它是如何做到不影响nginx转发性能的呢？有时间大家可以详细研究研究kong自带的lua插件的源码。 Proxy-Cache之前大数据的数据服务平台提供的API频繁被外部业务调用，并发量非常高，导致elasticsearch堆内存不足。但是很多数据和之前的调用都是一样的结果，完全没有必要重复到ES中去做查询（因为ES是多个节点，一旦查询请求落到的节点没有缓存之前的结果，就会导致重复查询）。我们希望通过对某些特定的请求做一定时间的缓存，这样可以支持更高的并发，同是也环节了后端ES的压力。 从v1.2.x开始，kong支持proxy-cache插件，该插件就可以满足我们的需求。只是当前官方释放出来的是只基于memory的，而基于外部redis集群的方案并没有开放出来；我们暂且了解一下基于memory的proxy-cache的功能。 首先，proxy-cache支持在几个粒度创建缓存： consumer service route 这三个粒度就不再过多的解释，在实现中，它首先通过使用nginx的 $request_uri 来作为缓存的key。这就除了包含url路径之外，还包括了后面的query参数。同时，用户可以指定需http method和http response，以及http header中的文本格式等。所有这些信息会作为key，当新的请求的这些信息重新hash后的key在缓存中找到对应的value时，直接从cache中返回数据，而不再转发到upstream。当然，用户可以指定缓存的TTL时间。 当使用memory模式的时候，相当于软件模拟了一个内存字典，在字典中缓存了对应的数据。但是由于内存是单机的，没法做多个kong的全局缓存；而使用redis可以做到这一点。但是，对于一般的业务来讲，如果kong的数量不是特别多，只是做了一个主备的话，其实使用memory也问题不大，不外乎后端多承受一部分压力，但是在高并发的场景中，该插件是有实际收益的。","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"},{"name":"kong","slug":"kong","permalink":"http://ljchen.net/tags/kong/"},{"name":"api-gateway","slug":"api-gateway","permalink":"http://ljchen.net/tags/api-gateway/"}]},{"title":"Rocket-MQ总结","slug":"Rocket-MQ总结","date":"2018-08-04T10:46:09.000Z","updated":"2020-04-06T09:24:01.468Z","comments":true,"path":"2018/08/04/Rocket-MQ总结/","link":"","permalink":"http://ljchen.net/2018/08/04/Rocket-MQ总结/","excerpt":"之前一直在使用rabbitmq，对Rocket MQ研究的比较少；最近由于另外一个项目已经使用了RMQ，为了不至于引入更多的中间件类型，在基础架构中也直接选型RMQ。然后大致找了一些资料，和书籍了解了一下RMQ的原理；趁周末，大概总结一下。 MQ作用既然都聊到消息队列了，就势必再提一下它在分布式中的作用。对我来讲，比较有深入体会的，大致包含一下三个方面。","text":"之前一直在使用rabbitmq，对Rocket MQ研究的比较少；最近由于另外一个项目已经使用了RMQ，为了不至于引入更多的中间件类型，在基础架构中也直接选型RMQ。然后大致找了一些资料，和书籍了解了一下RMQ的原理；趁周末，大概总结一下。 MQ作用既然都聊到消息队列了，就势必再提一下它在分布式中的作用。对我来讲，比较有深入体会的，大致包含一下三个方面。 架构解耦 从解耦的角度来讲，再各个微服务架构设计中还是比较常见的。比如，使用消息对接，可以将一个既处理API操作，又实现业务逻辑的单体架构，拆解为两个微服务;其中，API controller只负责接受消息，记录到数据库，并发送处理消息到MQ积压。然后专门的operator来去订阅MQ中对应操作的消息并处理业务。 流量消峰 在上一个例子中，当API controller瞬间接受到大流量时，只需要记录到数据库，并发送消息到MQ；也许operator并没有那么强的应对高峰请求的处理能力，这个时候就只能够时间换空间，逐步的从MQ中取出消息去消费。这种情况下，MQ就起到了很重要的流量消峰的作用。 消息分发 消息分发应该属于MQ都共有的一个特性，一般都支持不同的topic，或者key映射到不同的队列，然后消费者根据自己的喜好去订阅不同类型的消息。说到这一点，感觉rabbitmq比rocket mq在这块的特征要更加突出，因为它维护了一个消费者端的队列，而不是像RMQ一样，逻辑在consumer侧。 架构Rocket MQ采用较简化的架构来设计，主要由下面四个部分组成： NameServer: 存储broker与topic之间的映射关系，类似于一个metadata服务器； Broker: 真正处理消息积压和分发的地方； Producer: 消息的生产者，主要是以SDK或者库的形势运行在业务端程序上； Consumer: 消息的消费者，同Producer，也是以SDK或者库的形势运行在业务端程序上。 NameServer NameServer是采用内存存储topic与broker的映射关系的，没有集中的存储或者缓存中间件； 所有的其他组件在启动前，都会指定NameServer地址；Broker会与NameServer建立长连接，定期向NamerServer发送心跳； Broker向NameServer发心跳，会带上当前自己所负责的所有Topic信息，相当于每一个心跳TTL周期都在为NameServer做全同步； 由于无状态，NameServer支持多实例部署；当动态添加一个NameServer的时候，需要所有其他组件都感知到，Broker支持动态修改配置； 当查询完topic到broker的map后，Produce与Broker，Broker与Consumer之间都会建立长连接，NameServer全部挂掉后，也不会影响旧业务。 Broker主备 支持一个主和多个备部署，拥有同样name的broker为一组；在这组中，id为0的为master，其他为slave； master支持producer往上面写消息并支持从上面读消息，slave只支持从上面读消息； broker支持指定采用同步还是异步模式；在同步模式，当producer写消息到master后，master必须将消息写到slave才会返回状态； broker暂不支持主备切换，在同一个组中，如果master挂了，consumer只能够从slaves上消费还未被消费的消息，但producer无法再写入； 创建Topic时，把Topic的多个队列创建在多个Broker组上，这样一个Broker组的Master挂后，其他组的Master仍然可用，Producer仍然可以发送消息。 消息消费模式 Clustering模式， 同一个ConsumerGroup（GroupName相同）只会有一个consumer抢到消息并消费；多个ConsumerGroup的consumer可以订阅同一个topic，这样每个ConsumerGroup中都会有一个consumer能消费到消息（即：同一个消息支持被多个ConsumerGroup重复消费）。 Broadcasting模式， 同一个ConsumerGroup里的每个Consumer都能消费到所订阅Topic的全部消息，也就是一个消息会被多次分发，被多个Consumer消费。 注意：不要混淆了ConsumerGroup和ProducerGroup。 Broker队列 在创建topic的时候，可以指定其存放的队列个数；该队列数为在单台broker上的队列数（当有N个slave，总队列数就对应乘以N倍）； 默认情况下，producer会将消息轮询分发到master的所有队列上； 消息存储 消息存储是由ConsumeQueue和CommitLog配合完成的，消息真正的物理存储文件是CommitLog，ConsumeQueue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址。每个Topic下的每个Message Queue都有一个对应的ConsumeQueue文件； broker支持指定同步落盘还是异步落盘；同步落盘会保障消息被写到磁盘才返回，异步是先记录到内存就会返回状态，当内存消息达到一定量后再写入物理存储； 消息顺序 如果需要支持全局顺序消息，就必须为topic只指定一个队列，否则多个队列无法保障消息的顺序； 需要只需要保障部分顺序消息，需要使用MessageQueue-Selector来将消息发送到指定队列上，消费端也需要使用MessageListenerOrderly保障不会并发读取。 Producer发送消息 支持发送延迟消息，当指定了消息类型为延迟消息后，该消息发送到broker，需要等待延迟时间到后，才能够生效，被consumer消费(支持指定时刻和指定延迟间隔)； 一个topic有多个队列，默认配置下，producer会轮流往这些队列发送该topic的消息，从而实现负载均衡； 可以设置Message-QueueSelector来自定义消息放到哪些queue里面； 事务消息采用两阶段提交的方式实现事务消息； Producer先发送一个待确定消息到broker，此时该消息无法被consumer看到，但是该操作会返回消息在队列中的地址等信息，作为索引； Producer在本地处理完操作之后，基于第一阶段broker返回的消息索引，发送第二个确认消息到broker（可能为commit或者rollback）； broker如果收到commit消息，将让consumer去消费该消息；如果收到rollback，将消息处理掉即可。 如果producer在发送第二个确认消息到broker的途中异常，消息没有送达broker；broker会定期扫描队列中未被确认的消息，并回调producer的处理函数处理； 消息去重 保障消息消费逻辑的幂等性； 维护消息消费的记录，每次消费的时候去查询是否被消费过（麻烦）。 Consumer两种方式 push方式，其实是通过长轮询来实现的，在该类型下由consumer维护了maptree，保存所有接收到的消息的列表，并实现类似于时间窗口的算法来做流控；PushConsumer会判断获取但还未处理的消息个数、消息总大小、Offset的跨度，任何一个值超过设定的大小就隔一段时间再拉取消息，从而达到流量控制的目的。此外 ProcessQueue还可以辅助实现顺序消费的逻辑。push方式当退出consume时，必须显示调用shutdown()告知broker记录offset等信息； pull方式，需要自己维护从broker的队列上读取消息的offset，并存储到本地，从而保障当consumer挂掉重启之后能够重新接着源地址消费消息； 从原理来讲，pull方式应该是不支持brodcast消息方式的，除非broker通过session获取到连接到其上的所有consumer，并为未消费的consumer保留消息等待消费。 负载均衡 consumer自己获取到全局queue信息后，并在客户端实现负载均衡； 消费失败 Consumer只负责消费消息，当消费消息失败的时候，会将消息写回broker，并设置其topic为SCHEDULE_TOPIC_XXXX（同样会做落盘处理）; scheduler会轮询读取队列中的消息，查看是否到达消费时间； 当时间到后，将其放入retry队列，开始重新消费。 这篇文章对该流程有消息的介绍","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"rocket-mq","slug":"rocket-mq","permalink":"http://ljchen.net/tags/rocket-mq/"}]},{"title":"分布式系统Leader-Election实践","slug":"distributed-system leader-election实践","date":"2018-07-29T07:56:12.000Z","updated":"2020-04-06T09:24:01.470Z","comments":true,"path":"2018/07/29/distributed-system leader-election实践/","link":"","permalink":"http://ljchen.net/2018/07/29/distributed-system leader-election实践/","excerpt":"最近在实现基于golang的基础代码框架，正好实现了一遍leader选举的代码；研究了各种方式选主的实现，简单记录一下。 两种思路刷存在感、论资排辈以前搞网络协议的时候，绝大多数的选主流程都是依靠协议的节点相互之间发送：“请求-&gt;应答-&gt;确认”等报文来通告朋友圈自己的信息，耍存在感；协议定义了朋友圈儿中论资排辈的规则，比如先看MAC地址、再判断用户配置的优先级等等。","text":"最近在实现基于golang的基础代码框架，正好实现了一遍leader选举的代码；研究了各种方式选主的实现，简单记录一下。 两种思路刷存在感、论资排辈以前搞网络协议的时候，绝大多数的选主流程都是依靠协议的节点相互之间发送：“请求-&gt;应答-&gt;确认”等报文来通告朋友圈自己的信息，耍存在感；协议定义了朋友圈儿中论资排辈的规则，比如先看MAC地址、再判断用户配置的优先级等等。最终所有的节点信息达成同步，按照老祖宗定下的规矩，老大必须是xx，大家就安心干活，总有一天老大挂了，媳妇儿才能熬成婆。通过定期刷朋友圈儿（心跳报文）来保障当老大挂了之后能够立刻召开大会，进行下一轮选举、快速倒换业务。电信级的OAM甚至对业务的倒换时间都有毫秒级的要求。 所以，一般协议的RFC都包含了一套选主、论资排辈以及倒换业务的流程，只需要按照该流程实现就行了。 但是，这样的实现一般需要一套组播发现机制来保障所有参与选举的节点都能够加入进来；或者通过手动配置的方式来实现。这样的实现，不依赖第三发，开发成本和复杂度都相对较大。在分布式中，常见的如raft协议等。 马仔抢帽子互联网的业务可以依赖一些外部中间件，而中间件的高可用平台已提供了保障。因此，基于中间件来实现选主是一个更加低成本和高效的选择。借助于中间件，选主本质就变为一个抢锁的过程。 简单整理一下思路： 我们将leader角色看作写着老大字样的帽子，所有参与选主的节点就是参与抢帽子的马仔们。 一开始所有人都是马仔，然后开始抢帽子，谁先抢到锁（老大的帽子），谁就是这一个时间片的老大； 抢到帽子的老大拥有一定的特权，在这一个时间片结束前，老大如果还想继续扮演老大的角色，就自个儿去为自己续职； 没抢到老大帽子的马仔们，会不甘心当一辈子马仔，所以一旦有老大掉了老大帽子的消息就飞速的去争抢。 通过这样一个简单的抢帽子比赛，组织内部就维持了某一时间点只有一个老大的状态。这个简单的比喻，我们大概知道了选主的流程。也知道了这种选主方式与先前讲的多个成员定期耍朋友圈，刷存在感，再论资排辈之间在架构上的差异是引入了中间件。 那么哪些中间件才能用来做这个事情呢？这里又涉及到两种情况。 马仔们高度自觉，发现帽子已经被人拥有后，不会起歹念；这种情况中间件只用来存放时间状态。 马仔们素质较低，一有机会就试图抢走帽子；这种情况下中间件必须将帽子放入笼子，保障在定时器未到期时，帽子没法被马仔们抢走。 第一种情况就是k8s对leader election的实现方式。而我基于consul实现的是对第二种方式。 实现分析K8s选主K8s的controller-manger和scheduler是有状态的，它的HA是A+P模式。通过选主，只在主节点上运行业务，其他从节点处于待命状态。抢到锁的节点会将自己的标记（目前是hostname）设为锁的持有者，其他人则需要通过对比锁的更新时间和持有者来判断自己是否能成为新的 leader ，而 leader 则可以通过更新 RenewTime 来确保持续保有该锁。 k8s并没有使用中间件，但是本质上是使用了etcd来存放我们之前比喻中的帽子。只是这个帽子通过API-Server演化成了k8s的object；go-client中有两种实现，一种是configmap，另外一种是endpoint。而锁的属性（更新时间、持有者）通过k8s object的annotation字段存放起来；该 annotation 的 key 为 control-plane.alpha.kubernetes.io/leader（如下）。 12345678910111213[root@ljchen ~]# kcs get endpoints kube-controller-manager -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;\"holderIdentity\":\"dev-7\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-05-22T09:54:30Z\",\"renewTime\":\"2018-07-29T07:06:17Z\",\"leaderTransitions\":0&#125;' creationTimestamp: 2018-05-22T09:54:56Z name: kube-controller-manager namespace: kube-system resourceVersion: \"22700552\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: 2e879038-5da6-11e8-9dac-00163e0f7d95subsets: null 基于consul的选主consul 是一个伟大的产品，大家都熟知它被用来实现服务的注册与发现、配置中心，统一下发配置等。 除了这些大家都熟知的功能，consul还有一个类似于redis超时时间的功能。即consul中的key可以绑定一个session，这个session有TTL，当session的持有者在TTL超时之前不去续命，consul就会自动释放session持有者对key的独占权。另外，consul基于long polling的实时watch机制也决定了它天然适合用来实现锁。 基本原理如下： 客户端创建session，并试着去获取锁； 如果获取锁成功，按照TTL/2的周期去续命； 如果获取锁失败，开始watch锁； 整个过程，任何一个环节报错，就重新开始一遍流程；当失败次数大于指定的次数，退出选举（可能consul无法连接或其他外部异常）。 代码实现大致如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899const ( LeaderElectionPathFmt = \"lock/%s/leader\")type LeaderElection struct &#123; Consul *Consul TTL time.Duration Callback func(leader bool) key string&#125;func (l *LeaderElection) Run() &#123; var ( identity = Utils&#123;&#125;.GetMyIPAddr() retryPeriod = time.Second * 5 maxAttempt = 30 attempt = 0 kv = l.Consul.client.KV() session = l.Consul.client.Session() ) l.key = fmt.Sprintf(LeaderElectionPathFmt, Utils&#123;&#125;.GetAppName()) callback := func(leader bool) &#123; GetRoleInst().SetRole(leader) l.Callback(leader) &#125; se := &amp;consul.SessionEntry&#123; Name: identity, TTL: \"10s\", LockDelay: time.Nanosecond, &#125; for &#123; // check retry times if attempt &gt; maxAttempt &#123; panic(\"Run retry times reach Max failed count.\") &#125; // create new session sessionId, _, err := session.CreateNoChecks(se, nil) if err != nil &#123; logs.Info(\"create session err: %v, retry after 5 second.\", err) time.Sleep(retryPeriod) attempt ++ continue &#125; logs.Info(\"session sessionId:\", sessionId) p := &amp;consul.KVPair&#123; Key: l.key, Value: []byte(identity), Session: sessionId, &#125; // try to acquire lock locked, _, err := kv.Acquire(p, nil) if err != nil &#123; logs.Info(\"acquire err: %v, retry after 5 second.\", err) time.Sleep(retryPeriod) attempt ++ continue &#125; // unlocked if !locked &#123; callback(false) respChan := l.Consul.WatchKey(l.key, nil) select &#123; case ret := &lt;-respChan: if ret.Error != nil &#123; logs.Info(\"watch key err: %v, retry after 5 second.\", err) time.Sleep(retryPeriod) &#125; else &#123; logs.Info(\"leader released, it's time to election lock!\") &#125; &#125; // locked &#125; else &#123; callback(true) // RenewPeriodic 是一个阻塞函数 err := session.RenewPeriodic(se.TTL, sessionId, nil, nil) utils.Display(\"err:\", err) callback(false) &#125; attempt = 0 &#125;&#125;","categories":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/categories/distributed-system/"}],"tags":[{"name":"distributed-system","slug":"distributed-system","permalink":"http://ljchen.net/tags/distributed-system/"},{"name":"leader-election","slug":"leader-election","permalink":"http://ljchen.net/tags/leader-election/"},{"name":"consul","slug":"consul","permalink":"http://ljchen.net/tags/consul/"}]},{"title":"Docker Cpu资源隔离","slug":"docker-cpu资源隔离","date":"2018-06-24T10:29:25.000Z","updated":"2020-04-06T09:24:01.470Z","comments":true,"path":"2018/06/24/docker-cpu资源隔离/","link":"","permalink":"http://ljchen.net/2018/06/24/docker-cpu资源隔离/","excerpt":"概述docker提供对CPU资源的限制，主要基于cgroup的cpuset来实现；到v1.13版本后，主要演化成以下几个参数。 –cpus 指定容器所需的cpu核数；其实现方式只是指定的对应cpu核心数相对的资源总量，容器可能会在多个核心上运行，可能涉及到上下文切换的消耗； –cpu-shares 指定多个容器抢占有限的资源时能够分配到cpu资源的权重；需要搭配着其他两个命令一起使用，单纯使用该命令，或在没有资源抢占的情况下使用该命令没有意义； –cpuset-cpus 指定容器绑定到具体哪些cpu核上运行；资源隔离相对较好，但也可能存在多个容器同时被绑定到同一个核上的情况。","text":"概述docker提供对CPU资源的限制，主要基于cgroup的cpuset来实现；到v1.13版本后，主要演化成以下几个参数。 –cpus 指定容器所需的cpu核数；其实现方式只是指定的对应cpu核心数相对的资源总量，容器可能会在多个核心上运行，可能涉及到上下文切换的消耗； –cpu-shares 指定多个容器抢占有限的资源时能够分配到cpu资源的权重；需要搭配着其他两个命令一起使用，单纯使用该命令，或在没有资源抢占的情况下使用该命令没有意义； –cpuset-cpus 指定容器绑定到具体哪些cpu核上运行；资源隔离相对较好，但也可能存在多个容器同时被绑定到同一个核上的情况。 宿主机上有4core： 12# cat /proc/cpuinfo | grep processor | wc -l4 –cpu-shares参数该选项用来设置CPU权重，它的默认值为1024。我们可以把它设置为1表示很低的权重，但是设置为0将表示使用默认值1024。它提供对抢占资源时，cpu的分配份额。 在只有一个进程使用资源时，cpu-shares没有意义，资源直接被容器消耗完。 1234567891011# docker run --rm --cpu-shares=512 -it progrium/stress -c 4# toptop - 19:23:51 up 2:25, 7 users, load average: 1.60, 1.46, 0.95Tasks: 181 total, 6 running, 175 sleeping, 0 stopped, 0 zombie%Cpu0 : 98.0/1.7 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu1 : 97.7/2.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu2 : 95.3/4.0 99[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]%Cpu3 : 98.0/1.4 99[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]KiB Mem : 8009496 total, 4529720 free, 777820 used, 2701956 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6832780 avail Mem –cpus参数指定容器使用的cpu数，在资源够分配时，它对资源使用总量的限制较准确；当资源不够出现超卖时，其无法保障能够获取到对应的CPU资源数。 资源足够时1234567891011# docker run --rm --cpus=1 -it progrium/stress -c 4# toptop - 19:15:52 up 2:17, 7 users, load average: 0.08, 0.18, 0.45Tasks: 179 total, 5 running, 174 sleeping, 0 stopped, 0 zombie%Cpu0 : 24.0/1.7 26[|||||||||||||||||||||||||| ]%Cpu1 : 23.9/2.0 26[|||||||||||||||||||||||||| ]%Cpu2 : 24.3/1.7 26[|||||||||||||||||||||||||| ]%Cpu3 : 23.4/1.4 25[|||||||||||||||||||||||| ]KiB Mem : 8009496 total, 4532328 free, 776500 used, 2700668 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6834536 avail Mem 结果显示，此时指定cpus的结果，相当于分配了1个core给stress运行。 超卖资源时无–cpu-shares以下命令相当于指定了6 core CPU，此时超过了我们宿主机的cpu core总数4。 1234567891011121314151617181920212223# docker run --rm --cpus=4 -it progrium/stress -c 4# docker run --rm --cpus=2 -it progrium/stress -c 4# toptop - 19:31:26 up 2:33, 7 users, load average: 1.70, 1.42, 1.14Tasks: 188 total, 9 running, 179 sleeping, 0 stopped, 0 zombie%Cpu0 : 98.3/1.3 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]%Cpu1 : 97.3/2.0 99[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]%Cpu2 : 96.6/3.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu3 : 98.6/1.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]KiB Mem : 8009496 total, 4515312 free, 790772 used, 2703412 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6819260 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 7615 root 20 0 7304 92 0 R 63.5 0.0 0:03.34 stress 7613 root 20 0 7304 92 0 R 51.5 0.0 0:02.55 stress 7616 root 20 0 7304 92 0 R 48.8 0.0 0:02.48 stress7614 root 20 0 7304 92 0 R 42.2 0.0 0:02.14 stress 7558 root 20 0 7304 100 0 R 48.5 0.0 0:03.32 stress 7557 root 20 0 7304 100 0 R 47.5 0.0 0:03.26 stress 7560 root 20 0 7304 100 0 R 43.2 0.0 0:03.01 stress 7559 root 20 0 7304 100 0 R 42.5 0.0 0:03.02 stress 有–cpu-shares（貌似效果也不好）1234567891011121314151617181920212223# docker run --rm --cpus=2 --cpu-shares=512 -it progrium/stress -c 4# docker run --rm --cpus=4 --cpu-shares=1024 -it progrium/stress -c 4# toptop - 19:34:12 up 2:35, 7 users, load average: 2.81, 1.66, 1.27Tasks: 185 total, 9 running, 176 sleeping, 0 stopped, 0 zombie%Cpu0 : 97.3/2.3 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]%Cpu1 : 97.7/2.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu2 : 96.3/3.7 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu3 : 98.3/1.3 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ]KiB Mem : 8009496 total, 4518760 free, 787184 used, 2703552 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6822896 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 8689 root 20 0 7304 96 0 R 84.4 0.0 0:08.85 stress 8687 root 20 0 7304 96 0 R 70.4 0.0 0:08.37 stress 8686 root 20 0 7304 96 0 R 57.5 0.0 0:08.04 stress 8688 root 20 0 7304 96 0 R 51.5 0.0 0:07.77 stress 8744 root 20 0 7304 100 0 R 42.2 0.0 0:03.20 stress 8745 root 20 0 7304 100 0 R 29.6 0.0 0:02.49 stress 8747 root 20 0 7304 100 0 R 27.9 0.0 0:02.54 stress 8746 root 20 0 7304 100 0 R 26.2 0.0 0:02.35 stress –cpuset-cpus参数该参数用来绑定容器使用的CPU core，对资源的限制较精确，但是需要外部维护绑定关系。当有多个容器都绑定到同一个core上时，需要借助–cpu-shares来分配份额。 资源抢占时当不指定份额时，会平均分配资源： 12345678910111213141516# docker run --rm --cpuset-cpus=0 -it progrium/stress -c 1# docker run --rm --cpuset-cpus=0 -it progrium/stress -c 1# toptop - 19:40:01 up 2:41, 6 users, load average: 1.86, 1.67, 1.37Tasks: 181 total, 3 running, 178 sleeping, 0 stopped, 0 zombie%Cpu0 : 100.0/0.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu1 : 1.0/1.4 2[|| ]%Cpu2 : 1.4/1.7 3[||| ]%Cpu3 : 0.3/1.0 1[| ]KiB Mem : 8009496 total, 4516292 free, 788620 used, 2704584 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6821116 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND11210 root 20 0 7304 92 0 R 49.8 0.0 0:12.24 stress11329 root 20 0 7304 96 0 R 49.8 0.0 0:05.64 stress 有–cpu-shares12345678910111213141516# docker run --rm --cpuset-cpus=0 --cpu-shares=512 -it progrium/stress -c 1# docker run --rm --cpuset-cpus=0 --cpu-shares=1024 -it progrium/stress -c 1# toptop - 19:38:25 up 2:40, 7 users, load average: 2.89, 1.69, 1.34Tasks: 182 total, 3 running, 179 sleeping, 0 stopped, 0 zombie%Cpu0 : 99.5/0.0 100[||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||]%Cpu1 : 0.7/1.7 2[||| ]%Cpu2 : 1.0/1.4 2[|| ]%Cpu3 : 0.7/1.4 2[|| ]KiB Mem : 8009496 total, 4519984 free, 784588 used, 2704924 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 6824732 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND10555 root 20 0 7304 96 0 R 66.8 0.0 0:06.90 stress10618 root 20 0 7304 96 0 R 33.2 0.0 0:02.32 stress –cpu-period &amp; –cpu-quotadocker提供–cpu-period、–cpu-quota两个参数控制容器可以分配到的CPU时钟周期。 --cpu-period 用来指定容器对CPU的使用要在多长时间内做一次重新分配; --cpu-quota 用来指定在这个周期内，最多可以有多少时间用来跑这个容器。跟–cpu-shares不同的是这种配置是指定一个绝对值，而且没有弹性在里面，容器对CPU资源的使用绝对不会超过配置的值。 –cpu-period和–cpu-quota的单位为微秒（μs）。–cpu-period的最小值为1000微秒，最大值为1秒（10^6 μs），默认值为0.1秒（100000 μs）。–cpu-quota的值默认为-1，表示不做控制。 举个例子，如果容器进程需要每1秒使用单个CPU的0.2秒时间，可以将–cpu-period设置为1000000（即1秒），–cpu-quota设置为200000（0.2秒）。当然，在多核情况下，如果允许容器进程需要完全占用两个CPU，则可以将–cpu-period设置为100000（即0.1秒），–cpu-quota设置为200000（0.2秒）。 K8S 代码实现request 被转化为 —cpu-share 参数 如果request=0 &amp;&amp; limit !=0, —cpu-shares=limit; 如果 request != 0, —cpu-shares=request. limit 被转化为 –cpu-quota 参数 –cpu-preiod被强制设置为100毫秒； –cpu-quota = Limit * 100毫秒，但是最小为1毫秒. 在kubelet创建container的时候，kuberuntime_container.go中的类kubeGenericRuntimeManager会调用generateContainerConfig来生成创建container的配置文件（从k8s yaml中指定对resources的配置转化而来）。 绝大多数的参数在不同操作系统之间都是一样的，但是对资源的限制这块，Linux和Windows有自己不同的参数。因此该类会继续调用 applyPlatformSpecificContainerConfig方法, 最终到 generateLinuxContainerConfig来为linux系统转化参数，具体如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// applyPlatformSpecificContainerConfig applies platform specific configurations to runtimeapi.ContainerConfig.func (m *kubeGenericRuntimeManager) applyPlatformSpecificContainerConfig(config *runtimeapi.ContainerConfig, container *v1.Container, pod *v1.Pod, uid *int64, username string) error &#123; config.Linux = m.generateLinuxContainerConfig(container, pod, uid, username) return nil&#125;// generateLinuxContainerConfig generates linux container config for kubelet runtime v1.func (m *kubeGenericRuntimeManager) generateLinuxContainerConfig(container *v1.Container, pod *v1.Pod, uid *int64, username string) *runtimeapi.LinuxContainerConfig &#123; lc := &amp;runtimeapi.LinuxContainerConfig&#123; Resources: &amp;runtimeapi.LinuxContainerResources&#123;&#125;, SecurityContext: m.determineEffectiveSecurityContext(pod, container, uid, username), &#125; // set linux container resources var cpuShares int64 cpuRequest := container.Resources.Requests.Cpu() cpuLimit := container.Resources.Limits.Cpu() memoryLimit := container.Resources.Limits.Memory().Value() oomScoreAdj := int64(qos.GetContainerOOMScoreAdjust(pod, container, int64(m.machineInfo.MemoryCapacity))) // If request is not specified, but limit is, we want request to default to limit. // API server does this for new containers, but we repeat this logic in Kubelet // for containers running on existing Kubernetes clusters. if cpuRequest.IsZero() &amp;&amp; !cpuLimit.IsZero() &#123; cpuShares = milliCPUToShares(cpuLimit.MilliValue()) &#125; else &#123; // if cpuRequest.Amount is nil, then milliCPUToShares will return the minimal number // of CPU shares. cpuShares = milliCPUToShares(cpuRequest.MilliValue()) &#125; lc.Resources.CpuShares = cpuShares if memoryLimit != 0 &#123; lc.Resources.MemoryLimitInBytes = memoryLimit &#125; // Set OOM score of the container based on qos policy. Processes in lower-priority pods should // be killed first if the system runs out of memory. lc.Resources.OomScoreAdj = oomScoreAdj if m.cpuCFSQuota &#123; // if cpuLimit.Amount is nil, then the appropriate default value is returned // to allow full usage of cpu resource. cpuQuota, cpuPeriod := milliCPUToQuota(cpuLimit.MilliValue()) lc.Resources.CpuQuota = cpuQuota lc.Resources.CpuPeriod = cpuPeriod &#125; return lc&#125;","categories":[{"name":"container","slug":"container","permalink":"http://ljchen.net/categories/container/"}],"tags":[{"name":"container","slug":"container","permalink":"http://ljchen.net/tags/container/"}]},{"title":"Macvlan与ipvlan解析","slug":"macvlan与ipvlan解析","date":"2018-06-24T07:02:20.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2018/06/24/macvlan与ipvlan解析/","link":"","permalink":"http://ljchen.net/2018/06/24/macvlan与ipvlan解析/","excerpt":"本文主要就macvlan和ipvlan的工作模式以及差异做简要介绍；同时，为便于形象的理解，还会涉及到一些实际操作命令。 macvlan这里的macvlan是linux kernel提供的一种network driver类型，它有别于传统交换机上提供的mac based vlan功能。可以在linux命令行执行lsmod | grep macvlan 查看当前内核是否加载了该driver；如果没有查看到，可以通过modprobe macvlan来载入，然后重新查看。","text":"本文主要就macvlan和ipvlan的工作模式以及差异做简要介绍；同时，为便于形象的理解，还会涉及到一些实际操作命令。 macvlan这里的macvlan是linux kernel提供的一种network driver类型，它有别于传统交换机上提供的mac based vlan功能。可以在linux命令行执行lsmod | grep macvlan 查看当前内核是否加载了该driver；如果没有查看到，可以通过modprobe macvlan来载入，然后重新查看。如果要查看内核源码，可以到以下路径：/drivers/net/macvlan.c 工作模式 Bridge：属于同一个parent接口的macvlan接口之间挂到同一个bridge上，可以二层互通（经过测试，发现这些macvlan接口都无法与parent 接口互通）。 VPEA：所有接口的流量都需要到外部switch才能够到达其他接口。 Private：接口只接受发送给自己MAC地址的报文。 三种模式之间的差异，可以通过下图形象的理解： 实验时间下面的实验创建了两个macvlan接口，分别放到两个netns中；然后验证这两个macvlan口之间客户互通。 先使用bridge mode创建两个macvlan接口，其parent接口都是enp0s8。 12# ip link add link enp0s8 name macv1 type macvlan mode bridge# ip link add link enp0s8 name macv2 type macvlan mode bridge 查看创建的结果（注意每个接口都有自己的mac地址）： 1234567# ip link 3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:31:1a:5d brd ff:ff:ff:ff:ff:ff7: macv1@enp0s8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1 link/ether 06:95:37:0c:83:36 brd ff:ff:ff:ff:ff:ff8: macv2@enp0s8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1 link/ether a2:e1:f1:e9:95:18 brd ff:ff:ff:ff:ff:ff 创建网络命名空间，并将接口加入进来，并通过dhcp server来分配IP地址： 12345# ip link set macv1 netns net-1# ip link set macv2 netns net-2# ip net exec net-1 dhclient macv1# ip net exec net-2 dhclient macv2 发现macv1 ping不通宿主机接口enp0s8的地址： 12345# ip net exec net-1 ping 10.0.2.15PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.^C--- 10.0.2.15 ping statistics ---4 packets transmitted, 0 received, 100% packet loss, time 3008ms 但是可以ping通macv2的地址： 12345# ip net exec net-1 ping 10.0.2.17PING 10.0.2.17 (10.0.2.17) 56(84) bytes of data.64 bytes from 10.0.2.17: icmp_seq=1 ttl=64 time=0.043 ms64 bytes from 10.0.2.17: icmp_seq=2 ttl=64 time=0.052 ms64 bytes from 10.0.2.17: icmp_seq=3 ttl=64 time=0.052 ms ipvlan下面介绍ipvlan接口，它也是linux kernel的一个network driver，具体代码见内核目录：/drivers/net/ipvlan/。与macvlan不同的是，macvlan是通过MAC查找macvlan设备，而ipvlan是通过IP查找ipvlan设备。 可以思考一下，ipvlan接口和直接在eth0上添加多个ip address的效果有什么区别？ 工作模式应用kernel-docs的一句话来介绍这两种模式的差异性： l2: the slaves will RX/TX multicast and broadcast (if applicable)as well. l3: the slaveswill not receive nor can send multicast / broadcast traffic. 形象的理解见下图： 实验时间以下实验分别创建了两个ipvlan接口，并放到两个netns中；ipvlan可以配置到不同的网段，它们彼此之间通过内部路由能够互访。 另外，L2 mode时，ipvlan接口能够接收到二层广播和组播报文；而L3 mode时，ipvlan接口不再处理所有二层报文。实验通过tcpdump抓ARP包的方式来验证该特性差异，通过实验，我们对ipvlan的两种模式应该有比较形象的理解。 L3 mode实验创建两个ipvlan接口，都是使用l3mode；查看其mac地址都等同于其parent接口的mac地址： 1234567891011121314# ip link add link enp0s3 ipvlan1 type ipvlan mode l3# ip link add link enp0s3 ipvlan2 type ipvlan mode l3# ip addr2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:d7:d6:a9 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic enp0s3 valid_lft 86193sec preferred_lft 86193sec inet6 fe80::c6bb:4c9:37e8:7ac2/64 scope link noprefixroute valid_lft forever preferred_lft forever7: ipvlan1@enp0s3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 08:00:27:d7:d6:a9 brd ff:ff:ff:ff:ff:ff8: ipvlan2@enp0s3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 08:00:27:d7:d6:a9 brd ff:ff:ff:ff:ff:ff 创建网络命名空间，并将接口放进命名空间： 12345# ip net add net-1# ip net add net-2# ip link set ipvlan1 netns net-1# ip link set ipvlan2 netns net-2 由于MAC地址一样，因此无法通过dhclient来分配IP地址，只能手动配置: 12# ip net exec net-1 ip addr add 10.0.2.18/24 dev ipvlan1# ip net exec net-2 ip addr add 10.0.3.19/24 dev ipvlan2 添加默认路由 12# ip net exec net-1 route add default dev ipvlan1# ip net exec net-2 route add default dev ipvlan2 可以ping通另一个namespace中的ipvlan接口（它们位于不同的网段中） 12345678# ip net exec net-2 ping 10.0.2.18PING 10.0.2.18 (10.0.2.18) 56(84) bytes of data.64 bytes from 10.0.2.18: icmp_seq=1 ttl=64 time=0.072 ms64 bytes from 10.0.2.18: icmp_seq=2 ttl=64 time=0.048 ms^C--- 10.0.2.18 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.048/0.060/0.072/0.012 ms 此时，在对端接口上抓不到ARP报文，说明二层广播和组播都不处理，工作在L3 1234# ip net exec net-1 tcpdump -ni ipvlan1 -p arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ipvlan1, link-type EN10MB (Ethernet), capture size 262144 bytes 无法ping通父接口enp0s3的IP 12345# ip net exec net-1 ping 10.0.2.15PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.^C--- 10.0.2.15 ping statistics ---97 packets transmitted, 0 received, 100% packet loss, time 96106ms L2 mode实验通L3mode类似，实验流程不再过多介绍： 1234567891011121314151617181920212223# ip link add link enp0s3 ipvlan1 type ipvlan mode l2# ip link add link enp0s3 ipvlan2 type ipvlan mode l2# ip net add net-1# ip net add net-2# ip link set ipvlan1 netns net-1# ip link set ipvlan2 netns net-2# ip net exec net-1 ip link set ipvlan1 up# ip net exec net-2 ip link set ipvlan2 up# ip net exec net-1 ip addr add 10.0.2.18/24 dev ipvlan1# ip net exec net-2 ip addr add 10.0.3.18/24 dev ipvlan2# ip net exec net-1 route add default dev ipvlan1# ip net exec net-2 route add default dev ipvlan2# ip net exec net-1 ip link set ipvlan1 up# ip net exec net-2 ip link set ipvlan2 up# ip net exec net-2 ip link set lo up# ip net exec net-1 ip link set lo up 与L3 mode不通的是，发现可以抓取到ARP报文（二层广播） 1234567# ip net exec net-1 tcpdump -ni ipvlan1 -p arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ipvlan1, link-type EN10MB (Ethernet), capture size 262144 bytes21:06:27.461824 ARP, Request who-has 10.0.3.18 tell 10.0.2.18, length 2821:06:27.461818 ARP, Request who-has 10.0.2.18 tell 10.0.3.18, length 2821:06:27.461842 ARP, Reply 10.0.2.18 is-at 08:00:27:d7:d6:a9, length 2821:06:27.461845 ARP, Reply 10.0.3.18 is-at 08:00:27:d7:d6:a9, length 28","categories":[{"name":"sdn","slug":"sdn","permalink":"http://ljchen.net/categories/sdn/"}],"tags":[{"name":"container","slug":"container","permalink":"http://ljchen.net/tags/container/"},{"name":"network","slug":"network","permalink":"http://ljchen.net/tags/network/"},{"name":"linux","slug":"linux","permalink":"http://ljchen.net/tags/linux/"}]},{"title":"高负荷应对方案","slug":"high-load-solution","date":"2018-06-01T09:22:58.000Z","updated":"2020-04-06T09:24:01.481Z","comments":true,"path":"2018/06/01/high-load-solution/","link":"","permalink":"http://ljchen.net/2018/06/01/high-load-solution/","excerpt":"前提假设我们为每个实例分配的CPU/MEMORY/NET是固定的。","text":"前提假设我们为每个实例分配的CPU/MEMORY/NET是固定的。 三种类型业务的高负荷 第一种独立的服务最容易伸缩，它可以进行线性地伸缩。如果用户增长了两倍，我们就运行两倍的服务实例, 就万事大吉了。 第二种服务依赖了外部的资源，比如那些使用了数据库的服务。数据库有它自己的容量上限，这个一定要注意。你还要知道，如果系统性能出现衰退，就不应该再增加更多的实例，而且你要知道这种情况会在什么时候发生。 第三种服务受到外部系统的牵制。例如，外部的账单系统。 就算运行了 100 个服务实例，它也没办法处理超过 500 个请求。我们要考 虑到这些限制。在确定了服务类型并设置了相应的标记之后，是时候看看 它们是如何通过我们的构建管道的。 应对方案 如果是第一种类型的服务我们使用一个实例，并在这个环境里运行它，给它最大的负载。在运行了几轮之后，我们取其中的最小值，将它存入InfluxDB，将它作为该服务的负载上限。 如果是第二种类型的服务我们逐渐加大负载，直到出现了性能衰退。我们对这个过程进行评估，如果我们知道该系统的负载，那么就比较当前负载是否已经足够，否则，我们就会设置告警，不会把这个服务发布到生产环境。我们会告诉开发人员: “你们需要分离出一些东西，或者加进去另一个工具，让这个服务可以更好地伸缩。” 因为我们知道第三种类型服务的上限所以我们只运行一个实例。我们也会给它一些负载，看看它可以服务多少个用户。如果我们知道账单系统的上限是1000个请求，并且每个服务实例可以处理200个请求，那么就需要5个实例。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"}]},{"title":"微服务改造","slug":"microservice-transform","date":"2018-05-30T09:58:40.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2018/05/30/microservice-transform/","link":"","permalink":"http://ljchen.net/2018/05/30/microservice-transform/","excerpt":"清理应用程序。确保应用程序具有良好的自动化测试套件，并使用了最新版本的软件包、框架和编程语言。 重构应用程序，把它拆分成多个模块，为模块定义清晰的API。不要让外部代码直接触及模块内部，所有的交互都应该通过模块提 供的API来进行。","text":"清理应用程序。确保应用程序具有良好的自动化测试套件，并使用了最新版本的软件包、框架和编程语言。 重构应用程序，把它拆分成多个模块，为模块定义清晰的API。不要让外部代码直接触及模块内部，所有的交互都应该通过模块提 供的API来进行。 从应用程序中选择一个模块，并把它拆分成独立的应用程序，部署在相同的主机上。你可以从中获得一些好处，而不会带来太多 的运维麻烦。不过，你仍然需要解决这两个应用之间的交互问 题，虽然它们都部署在同一个主机上。不过你可以无视微服务架 构里固有的网络分区问题和分布式系统的可用性问题。 把独立出来的模块移动到不同的主机上。现在，你需要处理跨网络交互问题，不过这样可以让这两个系统之间的耦合降得更低。 如果有可能，可以重构数据存储系统，让另一个主机上的模块负责自己的数据存储。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"}]},{"title":"微服务架构的先决条件","slug":"microservice-precondiction","date":"2018-05-30T09:56:48.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2018/05/30/microservice-precondiction/","link":"","permalink":"http://ljchen.net/2018/05/30/microservice-precondiction/","excerpt":"下面是我总结的内容，感觉这样会更好理解： 敏捷基础设施 公共基础服务（中间件） 故障反馈机制（监控告警） 流程与自动化工具链（DevOps流水线） 微服务框架（SpringCloud/Dubbo）","text":"下面是我总结的内容，感觉这样会更好理解： 敏捷基础设施 公共基础服务（中间件） 故障反馈机制（监控告警） 流程与自动化工具链（DevOps流水线） 微服务框架（SpringCloud/Dubbo） 快速配置如果你的开发团队里只有少数几个人可以配置新服务、虚拟环境或其他配套设施，那说明你们还没有为微服务做好准备。你的每个团队里都应该要有几个这样的人，他们具备了配置基础设施和部署服务的能力，而且不需要求助于外部。要注意，光是有一个 DevOps 团队并不意味着你在实施DevOps。开发人员应该参与管理与应用程序相关的组件，包括基础设施。 类似的，如果你没有灵活的基础设施(易于伸缩并且可以由团队里的不同人员来管理)来支撑当前的架构，那么在迁移到微服务前必须先解决这个问题。你当然可以在裸机上运行微服务，以更低的成本获得出众的性能，但在服务的运维和部署方面也必须具备灵活性。 基本的监控如果你不曾对你的单体应用进行过性能监控，那么在迁移到微服务时，你的日子会很难过。你需要熟悉系统级别的度量指标(比如CPU和内存)、应用级别的度量指标(比如端点的请求延迟或端点的错误)和业务级别的度量指标(比如每秒事务数或每秒收益)，这样才可以更好地理解系统的性能。 在性能方面，微服务生态系统比单体系统要复杂得多，就更不用提诊断问题的复杂性了。你可以搭建一个监控系统(如 Prometheus)，在将单体应用拆分成微服务之前对应用做一些增强，以便进行监控。 快速部署如果你的单体系统没有一个很好的持续集成流程和部署系统，那么要集成和部署好你的微服务几乎是件不可能的事。 想象一下这样的场景: 10个团队和100个服务，它们都需要进行手动测试和部署，然后再将这些工作与测试和部署一个单体所需要的工作进行对比。100个服务会出现多少种问题? 而单体系统呢? 这些先决条件很好地说明了微服务的复杂性。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"microservice","slug":"microservice","permalink":"http://ljchen.net/tags/microservice/"}]},{"title":"Cloud Native 12 Factor","slug":"cloud-native-12-factor","date":"2018-05-30T09:43:27.000Z","updated":"2020-04-06T09:24:01.469Z","comments":true,"path":"2018/05/30/cloud-native-12-factor/","link":"","permalink":"http://ljchen.net/2018/05/30/cloud-native-12-factor/","excerpt":"基准代码从一个代码库部署到多个环境。 一个代码库，包括生产环境软件包，确保了单一的信任源，从而保证了更少的配置错误和更强的容错和复原能力。 依赖依赖管理是声明式的。 云平台根据这些声明管理依赖，确保云应用所需的库和服务。","text":"基准代码从一个代码库部署到多个环境。 一个代码库，包括生产环境软件包，确保了单一的信任源，从而保证了更少的配置错误和更强的容错和复原能力。 依赖依赖管理是声明式的。 云平台根据这些声明管理依赖，确保云应用所需的库和服务。 配置配置信息保存在环境中。 环境变量是一种清楚、容易理解和标准化的配置方法，特别适合用不同编程语言编写的无状态应用的使用。 后端服务将后台服务视为附加的资源。 将每一种资源都视为一种远程的资源，应用因此具有容错和复原能力，因为它一方面要求编码时就要考虑资源不可用的情况，另外一方面也增强微服务方法的好处。 构建、发布、运行区分构建、发布和运行阶段。 Cloud Native应用的构建流程把大部分发布配置挪到开发阶段，包括实际的代码构建和运行应用所需的生产环境配置。 进程作为无状态进程运行。 尽量保持应用栈每一层的轻量级，保证Cloud Native基础设施的速度和效率。 端口绑定通过端口绑定对外暴露服务。 Cloud Native应用的服务接口优先选择 HTTP API 作为通用的集成框架。 并发通过添加无状态进程实现横向扩展。 强调无状态、无共享的设计，这意味着依赖底层平台就能实现横向扩展，不需要技术难度高的多线程编码。 易处理快速地启动，优雅地关停。 假设任何进程随时都能启动和关停。 开发环境和线上环境等价开发、预发布和生产环境运行同样的应用和依赖配置。 由于强调自动化和在每个阶段使用同一个云平台，如果每个人用同样的服务器配置，那么“应用在我这里是可以的”就意味着在其他人或者环境那里也是可以的。 日志日志输出到标准输出，方便日志聚合和事件响应。 当日志是由云平台而不是应用包含的库处理时，日志处理机制必须保持简单。 管理进程零时任务作为短时进程运行。 在Cloud Native中，管理任务也是一个进程，而不是特别的工具；同样重要的是，管理任务的进程不应使用秘密的 API 或者内部机制。","categories":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/categories/cloud-native/"}],"tags":[{"name":"cloud-native","slug":"cloud-native","permalink":"http://ljchen.net/tags/cloud-native/"}]},{"title":"Telepresence","slug":"Telepresence","date":"2018-05-24T15:35:36.000Z","updated":"2020-04-06T09:24:01.469Z","comments":true,"path":"2018/05/24/Telepresence/","link":"","permalink":"http://ljchen.net/2018/05/24/Telepresence/","excerpt":"介绍Telepresence是Apache 2.0 下的一个开源工具，它允许开发者在本地运行单个服务，并将这个服务连接到远端的k8s集群中。这使得那些致力于多个服务应用程序开发的团队，通过使用双向网络代理的方式，在本地运行调试程序来取代在K8S集群中运行的pod，从而实现Kubernetes微服务的快速本地开发。","text":"介绍Telepresence是Apache 2.0 下的一个开源工具，它允许开发者在本地运行单个服务，并将这个服务连接到远端的k8s集群中。这使得那些致力于多个服务应用程序开发的团队，通过使用双向网络代理的方式，在本地运行调试程序来取代在K8S集群中运行的pod，从而实现Kubernetes微服务的快速本地开发。 实践K8S中访问本地服务(替换deployment)该实践的场景是我们已经在k8s环境上运行了某个服务，比如以下是一个名叫hello-world的deployment。为了对这个服务做一些debug或者功能的更新，我们在本地已经修改好了代码，并编译成了可执行文件。 按照原来的开发流程，我们需要借助CI流水线打包镜像，然后部署到K8S环境，滚动升级之前的deployment；而该流程较繁琐、耗时（哪怕使用了流水线，由于网络的一些原因，镜像上传下载是一个耗时的操作）。 而通过Telepresence，我们只需要在本地执行几条命令，就可以让K8S段的其他服务正常访问本地运行的测试程序。对于其他服务来，目标服务运行在本地这个行为是无感知的。这就相当于使用本地运行的服务，替换掉了K8S中运行的deployment。 先在k8s上部署一个名叫hello-world的deployment： 12$ kubectl run hello-world --image=datawire/hello-world --port=8000$ kubectl expose deployment hello-world --type=LoadBalancer --name=hello-world 然后在本地电脑上执行： 1234mkdir /tmp/telepresence-testcd /tmp/telepresence-testecho \"hello from your laptop\" &gt; file.txttelepresence --swap-deployment hello-world --expose 8000 --run python3 -m http.server 8000 &amp; 该操作后，telepresence内部的过程包括： 启动一个类似VPN的进程，将查询发送到适当的DNS和IP范围到群集。 --swap-deployment 告知Telepresence将现有的hello-world pod替换为一个Telepresence代理pod。当本地的Telepresence退出时，之前的pod将恢复运行。 --run 告知Telepresence运行本地Web服务，并将其连接到网络代理（Telepresence代理pod）。 执行之后，系统会将k8s中旧的hello-world停掉，同时启动了新的pod（其实运行的应该是telepresence proxy镜像） 123$ kubectl get pod | grep hello-worldhello-world-2169952455-874dd 1/1 Running 0 1mhello-world-3842688117-0bzzv 1/1 Terminating 0 4m 当结束调试的时候，只需要从本地的后台唤醒进程 1fg 然后执行ctrl+C来退出。可以看到，k8s之前的pod又恢复了运行。所有流程可以通过执行访问服务的URL来验证。 K8S中访问本地服务(新建deployment)我们将如下python程序保存到本地helloworld.py 1234567891011121314#!/usr/bin/env python3from http.server import BaseHTTPRequestHandler, HTTPServerclass RequestHandler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header('Content-type', 'text/plain') self.end_headers() self.wfile.write(b\"Hello, world!\\n\") returnhttpd = HTTPServer(('', 8080), RequestHandler)httpd.serve_forever() 在本地执行telepresence，这次使用的是--new-deployment 并在本地运行程序来通过8080端口，对外提供服务： 12$ telepresence --new-deployment hello-world --expose 8080$ python3 helloworld.py 该命令对应在K8S上创建deployment和service hello-world；接下来我们可以在K8S上可以直接访问该服务的 cluster-ip:8080 来验证。 如果要在本地debug程序，只需要修改代码并重新执行： 123456python3 helloworld.py^Clocalhost$ sed s/Hello/Goodbye/g -i helloworld.pylocalhost$ grep Goodbye helloworld.py self.wfile.write(b\"Goodbye, world!\\n\")localhost$ python3 helloworld.py k8s上的服务对外提供的API的返回内容立刻生效。 本地访问K8S服务先在K8s上创建deployment并暴露service端口，如下： 1234$ kubectl run myservice --image=datawire/hello-world --port=8000 --expose$ kubectl get service myserviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEmyservice 10.0.0.12 &lt;none&gt; 8000/TCP 1m 在本地执行telepresence命令： 12$ telepresence --run curl http://myservice:8000/Hello, world! 该流程中，telepresence在k8s中启动了一个 proxy pod，用于代理到k8s pod网络内部，然后访问dns并请求目标服务。当curl结束的时候，该pod会自动退出。 1234root@ubuntu:~# kubectl get podNAME READY STATUS RESTARTS AGEhello-world-74556688f-gd78c 1/1 Running 0 46mtelepresence-1527136988-31514-26798-7489f99c9d-pvvj5 1/1 Terminating 0 24s","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"telepresence","slug":"telepresence","permalink":"http://ljchen.net/tags/telepresence/"}]},{"title":"Escalator","slug":"Escalator","date":"2018-05-22T03:26:00.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2018/05/22/Escalator/","link":"","permalink":"http://ljchen.net/2018/05/22/Escalator/","excerpt":"介绍Escalator是atlassian开源的自动弹性项目。该项目通过对k8s集群上workload的资源需求总量和Node Group中Node能提供的可用资源总量来计算使用率和扩缩容增量，并最终向Cloud Provider下发弹性伸缩的行为。另外，项目还提供了适用于Prometheus的metrics端点，方便从外部采集监控数据。需要强调的是，该项目针对的扩缩容目标是Node Group中的Node，而不是K8s层面的replicas数量。","text":"介绍Escalator是atlassian开源的自动弹性项目。该项目通过对k8s集群上workload的资源需求总量和Node Group中Node能提供的可用资源总量来计算使用率和扩缩容增量，并最终向Cloud Provider下发弹性伸缩的行为。另外，项目还提供了适用于Prometheus的metrics端点，方便从外部采集监控数据。需要强调的是，该项目针对的扩缩容目标是Node Group中的Node，而不是K8s层面的replicas数量。为了对自动弹性伸缩行为做优化, Escalator对Node节点，除了做常规的up/down 之外，还是用了中间态的taint。同时，还在算法中指定了cool down时间等，从而保障算法的高效。 下文是对Escalator的扩容流程和核心算法的简单介绍。 扩容过程 获取节点组中的所有容器信息 获取节点组中的所有节点信息 将节点过滤为三类： tainted untaint cordoned 计算Pods的资源需求总量 计算untaints节点的可分配资源总量 使用资源需求总量与可分配资源总量计算使用率 如果存在扩容锁，等待扩容锁已被释放 确定CPU或内存利用率的较大者 确定扩缩容的行为：需要扩容，缩容或什么都不做？ 如果需要扩容，计算需要增加的节点数量 按需要的节点数量扩容节点组 首先untaint节点 如果我们仍然需要更多节点，向云提供商发出请求以增加节点组 如果我们要求扩展云提供商，锁定扩容锁 算法与计算假设为了计算出资源需求，当前实例数，利用率和扩容的增量，Escalator作出了以下假设： 节点组中的所有节点拥有相同的可分配资源 节点组中的所有Pods的容器均指定了资源需求量 如果Escalator不能达成以上假设，比如，容器没有指定资源需求量，或者多个节点拥有不同的可分配资源，扩缩容行为可能产生意想不到的副作用 资源需求，容量和利用率为了计算出节点组的扩容量，Escalator会计算节点组中的当前资源需求总量，并将其与节点组的当前实例数进行比较。 为此，节点组中Pods的所有容器的资源需求都被汇总起来。所有节点的可分配资源（容量）也汇总到一起。 然后比较这些资源请求总量与节点的可分配资源总量，并为CPU和内存生成百分比利用率。然后Escalator采用两者中的较高者（CPU和内存）并用于后续计算。 例如: 我们有 10 个分别只包含一个容器的pods, 每个容器指定了需要使用 500m CPU 和 100mb 内存.计算出来的资源需求总量就是(5000m) CPU 和 1000mb 内存. 10 * 500m = 5000m 10 * 100mb = 1000mb 我们有两个Node节点，其可分配的资源量均为 1 (1000m) CPU 和 4000mb 内存.计算出来的可分配资源总量为 2 (2000m) CPU 和 8000mb 内存. 2 * 1000m = 2000m 2 * 4000mb = 8000mb 于是，可以计算出资源使用率如下: CPU: 5000m / 2000m * 100 = 250% 内存: 1000mb / 8000mb * 100 = 12.5% 然后，我们采用更高的百分比利用率，在这种情况下， CPU: 250%. 根据这个数字，我们可以扩容、缩容或者什么都不做。这取决于配置的阈值。 扩容增量的计算当确定Escalator需要扩容节点组后，需要计算并告知云提供商扩容节点组的数量。 扩容增量是通过百分比减少公式来计算的。我们需要通过计算将利用率降低到scale_up_threshold_percent选项所需的节点数，这个节点数就是增加节点组的数量。 例如: scale_up_threshold_percent 是 70 CPU利用率是 250 (250 - 70) / 70 = 2.57142857143 2.57142857143 * 2 nodes = 5.14285714286 扩容总量为: ceil(5.14285714286) = 6 个节点 通过向云提供商请求将节点组再扩展6个节点后，新的总节点数将变为8个。当新节点数变为8个时，节点组利用率如下： 使用率将按如下计算: CPU: 5000m / 8000m * 100 = 62.5% 内存: 1000mb / 32000mb * 100 = 3.125% DaemonsetsDaemonsets运行在所有Node节点上，它有意被排除在Escalator执行的利用率计算之外的。主要是基于以下原因： 只关注具有node selector或node affinity的Pods，可以简化利用率的计算。 无法简单的选择节点组中运行的Daemonsets，因为它们没有node selector或node affinity。 无法简单的计算出在新扩容节点上Daemonsets的利用率。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"autoscaling","slug":"autoscaling","permalink":"http://ljchen.net/tags/autoscaling/"}]},{"title":"基于Openstack的Rancher扁平网络","slug":"基于Openstack的Rancher扁平网络","date":"2017-10-12T13:01:31.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2017/10/12/基于Openstack的Rancher扁平网络/","link":"","permalink":"http://ljchen.net/2017/10/12/基于Openstack的Rancher扁平网络/","excerpt":"多层嵌套的网络方案一方面浪费了大量计算资源来处理网络流量，同时又导致一些旧的网络监控设备无法透明的监控网络流量。","text":"多层嵌套的网络方案一方面浪费了大量计算资源来处理网络流量，同时又导致一些旧的网络监控设备无法透明的监控网络流量。 需求背景网络转发效率从两个方面分别来分析： Rancher的原生网络是使用基于IPSec的overlay VPN技术，由于跨主机的容器之间的流量都需要经过IPSec解封包，因此性能方面有待改善。 Openstack的网络性能受到所使用的网络解决方案的影响。如果使用基于SDN的硬件解决方案来offload流量到外部设备做报文的解封包，这样尚且能够忍受内部容器网络的overlay方案。一旦使用普通的openVswitch，使用软件来解封包，浙江浪费很多的CPU资源来处理报文。 流量监控的需求金融客户大部分都有自己网络监控工具，这些工具一般能够解封常规的报文，较难处理IPSec加密的报文。为了保障网络对于安全监控的透明性，网络内部严禁使用Overlay VPN的解决方案。因此，如何在基于openstack的IaaS上层运行Rancher管理平台，就成为亟待解决的问题。 解决方案要实现容器在openstack网络中的扁平化，也就是需要将运行与虚拟机中的容器的端口放到虚拟的层面。网上有一些现成的方案可以达到这一目的，比如MACVLAN， 或者VLAN Trunk等。但是基于Rancher的Metadata server位于其所在宿主机169.254.169.250的这一特殊性，我们最终选用了Bridge的方式来实现。具体如下图： 为了阻止租户修改VM的IP或者MAC地址来搞破坏，openstack网络内部一般不允许虚拟机内部使用其他IP地址来通信。这就会导致容器无法使用自己的IP来与外部通信，为了解决这个问题，我们必须修改虚拟机port的“allow address pair”属性，并配置上容器所使用的IP地址。同时，在容器迁移或者是销毁的时候，对应虚拟机port的该属性又需要被更新。在我们的方案中，通过容器宿主机上的CNI Plugin来触发对应的操作。 另外，为了实现容器和虚拟机之间的通信，我们在每个VM上绑定两张网卡。其中一张用来管理VM，另一张用于容器的通信。容器网络和虚拟机网络最终在租户的虚拟路由器上汇集，实现三层可达。同时，虚拟路由器同样连接外部网络，通过设置特定的防火墙规则，开放特定IP地址对Openstack管理网络的访问。从而保障运行于Rancher内部的wise2c-neutron-controller可以访问neutron server。 除此之外，在VM内部，为了访问到绑定到docker0上的metadata server，我们创建了veth-pair来连接br-cni0和docker0。但是这样的连接必须在数据链路层做严格的流量控制，除了允许容器网络侧访问169.254.169.250的流量外，其他流量均不允许通行。 核心模块介绍整个方案中，最核心的是图上深蓝色的两个组件。 CNI Plugin主要包含如下功能 创建br-cni0网桥（首次启动检测到该主机上不存在该网桥时） 建立br-cni0到docker0的连接，并写入ebtable规则，保障只有metadata流量可以通信，其他流量禁止 挂载container-port到br-cni0网桥 从Rancher IPAM获取容器的IP和MAC地址，并配置到容器的eth0 使用容器所在主机信息、容器的IP和MAC地址这些参数向Wise2C-Neutron-Controller请求neutron-port 配置容器的路由，并写入访问Metadata地址的路由下一跳 Neutron Controller 启动后从openstack Neutron读取所有port，并过滤出容器端口作为缓存 监听容器port新增和删除的请求 当收到请求或，将请求转化为对openstack Neutron的具体操作，创建/删除port以及更新parent port的“allow address pair”属性","categories":[{"name":"sdn","slug":"sdn","permalink":"http://ljchen.net/categories/sdn/"}],"tags":[{"name":"CNI","slug":"CNI","permalink":"http://ljchen.net/tags/CNI/"},{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"},{"name":"flatnetwork","slug":"flatnetwork","permalink":"http://ljchen.net/tags/flatnetwork/"},{"name":"openstack","slug":"openstack","permalink":"http://ljchen.net/tags/openstack/"},{"name":"neutron","slug":"neutron","permalink":"http://ljchen.net/tags/neutron/"}]},{"title":"K8S External-NFS-Storage 简析","slug":"K8S-External-NFS-Storage-简析","date":"2017-08-03T07:32:10.000Z","updated":"2020-04-06T09:24:01.466Z","comments":true,"path":"2017/08/03/K8S-External-NFS-Storage-简析/","link":"","permalink":"http://ljchen.net/2017/08/03/K8S-External-NFS-Storage-简析/","excerpt":"","text":"工作原理K8S的外部NFS驱动，可以按照其工作方式（是作为NFS server还是NFS client）分为两类： nfs-client也就是我们接下来演示的这一类，它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。当用户创建对应的PVC来申请PV时，该provider就将PVC的要求与自身的属性比较，一旦满足就在本地挂载好的NFS目录中创建PV所属的子目录，为Pod提供动态的存储服务。 nfs与nfs-client不同，该驱动并不使用k8s的NFS驱动来挂载远端的NFS到本地再分配，而是直接将本地文件映射到容器内部，然后在容器内使用ganesha.nfsd来对外提供NFS服务；在每次创建PV的时候，直接在本地的NFS根目录中创建对应文件夹，并export出该子目录。 接下来我们来操作一个nfs-client驱动的例子，先对其有个直观的认识！ 部署实例这里，我们将nfs-client驱动做一个deployment部署到K8S集群中，然后对外提供存储服务。 部署nfs-client-provisioner环境变量的PROVISIONER_NAME、NFS服务器地址、NFS对外提供服务的路径信息等需要设置好；部署所使用的yaml文件关键代码如下所示： 12345678910111213141516171819202122kind: Deployment... spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/wise2c/nfs-client-provisioner:devel volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: wise2c.com/nfs - name: NFS_SERVER value: &lt;Masked-NFS-Server-IP&gt; - name: NFS_PATH value: /var/nfsshare volumes: - name: nfs-client-root nfs: server: &lt;Masked-NFS-Server-IP&gt; path: /var/nfsshare 创建Storage Classstorage class的定义，需要注意的是：provisioner属性要等于驱动所传入的环境变量PROVISIONER_NAME的值。否则，驱动不知道知道如何绑定storage class。 yaml文件如下所示： 12345apiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata: name: wise2c-nfs-storageprovisioner: wise2c.com/nfs 完成之后，在k8s里面就能够看到storage class的信息了： 1234[root@dev-6 henry]# kubectl get scNAME TYPEstandard kubernetes.io/aws-ebswise2c-nfs-storage wise2c.com/nfs 创建PVC这里指定了其对应的storage-class的名字为wise2c-nfs-storage，如下： 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: henry-claim annotations: volume.beta.kubernetes.io/storage-class: \"wise2c-nfs-storage\"spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 可以看到PVC已经绑定了volume，如下所示（volume的名字是namespace，PVC name以及uuid的组合）： 123[root@dev-6 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGEhenry-claim Bound default-henry-claim-pvc-c265cbd9-74d1-11e7-9990-00163e122a49 1Mi RWX wise2c-nfs-storage 3h 然后，我们进入到NFS的export目录，可以看到对应该volume name的目录已经创建出来了： 123[root@dev-5 nfsshare]# lsdefault-henry-claim-pvc-c265cbd9-74d1-11e7-9990-00163e122a49 创建pod指定该pod使用我们刚刚创建的PVC：henry-claim：完成之后，如果attach到pod中执行一些文件的读写操作，就可以确定pod的/mnt已经使用了NFS的存储服务了。那么，NFS-client又是如何从代码侧面来实现这些服务的呢？我们接下来就来分析其代码实现。 实现方式K8S 的external NFS storage驱动源码可以在这里找到：https://github.com/kubernetes-incubator/external-storage。 猜测在分析代码之前，我们先来猜测一下storage provider是如何实现的。通过刚才的实践，我们已经大体了解了NFS provider的功能和用法，基于这些用法我们可以设想，应该至少可以有两种实现方式： 所有的provider启动之后都注册到K8S，K8S提供了一种调度算法，用于分析PVC指名的要求，然后调度到具体的provider上为其分配PV； K8S没有这样一个集中的调度算法，在每一个provider内部内部实现了一种类似于实时监测的机制，用于分析每一个PVC的需求，当自己条件符合PVC的要求时，为其创建对应的PV。 源码分析 Main函数 nfs-client的代码非常简洁，只有一个文件。其main函数的实现里面，从环境变量读取了NFS_SERVER，NFS_PATH以及PROVISIONER_NAME这些参数之后，又收集了K8S inCluster的配置信息。然后把这些信息用于生成了驱动中最重要的结构ProvisionController。然后就开始调用由库函数提供的ProvisionController的run函数了。 接口实现 在provisioner.go中，除main函数外，还为nfsProvisioner实现了两个方法： Provision: 按照PVC的namespace、PVC name以及PVC指定的PV name，来在远端NFS上创建PV所属的目录，并返回所创建好的PV的数据结构。 Delete: 其功能恰好与Provision方法相反，其将原来为PVC创建的目录重命名为archived-的目录。之所以不直接删除，应该也是为了便于以后能够手动找回数据吧。 ProvisionController库 至此，我想大家都开始很关心，到底是驱动是如何调用这两个方法的呢？ 那么，整个驱动的关键就在这里了，controller库中有这样一个函数：NewProvisionController，我们截取一小段代码来看看它都做了什么。 看过kubernetes实现的人一定对这种结构非常熟悉。我们看到这里watch了所有namespace的PVC， 然后下面的handlerFuncs里面映射了三种行为的处理函数： 除了PVC资源意外，采用这种结构，NewProvisionController同样为PV和storage class的各种行为指定了对应的处理函数。添加PVC的流程图如下： 总结 因此，现在我们头脑里面就应该能够联系起所有的流程了： external-storage驱动库提供了一些方法，内部实现了对K8S存储资源，如PVC / PV / Storage Class 的实时watch。同时为这些资源对应的各种操作关联了相应的handler函数； 当k8s上对应存储资源发生变化的时候，external-storage驱动中对应的handler函数就会被调用，从而操作存储资源； external-storage驱动库为各个不同的存储后端提供可扩展的接口，要实现一种是有的存储驱动，就只需实现其Provision和Delete方法即可。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/tags/kubernetes/"},{"name":"nfs","slug":"nfs","permalink":"http://ljchen.net/tags/nfs/"},{"name":"storage","slug":"storage","permalink":"http://ljchen.net/tags/storage/"}]},{"title":"Kubernetes Flannel网络常见问题","slug":"kubernetes-flannel网络常见问题","date":"2017-06-09T14:50:53.000Z","updated":"2020-04-06T09:24:01.485Z","comments":true,"path":"2017/06/09/kubernetes-flannel网络常见问题/","link":"","permalink":"http://ljchen.net/2017/06/09/kubernetes-flannel网络常见问题/","excerpt":"Flannel是将多个不同子网（基于主机Node）通过被Flannel维护的Overlay网络拼接成为一张大网来实现互联的，通过官方的一张网络拓扑图我们可以对其基本原理一目了然：","text":"Flannel是将多个不同子网（基于主机Node）通过被Flannel维护的Overlay网络拼接成为一张大网来实现互联的，通过官方的一张网络拓扑图我们可以对其基本原理一目了然： 值得探讨的是，flannel的这个overlay网络支持多种后端实现，除上图中的UDP，还有VXLAN和host-gw等。此外，flannel支持通过两种模式来维护隧道端点上FDB的信息，其中一种是通过连接Etcd来实现，另外一种是直接对接K8S，通过K8S添加删除Node来触发更新。 部署常见问题Node状态显示为“NotReady”我的K8S环境使用kubeadm来容器化运行K8S的各个组件（除kubelet直接运行在裸机上外），当我使用kubeadm join命令加入新的Minion Node到K8S集群中后，通过kubectl get node会发现所有的node都还是not ready状态，这是因为还没有配置好flannel网络。 使用kube-flannel.yml无法创建DaemonSet我使用的是K8S的1.6.4的版本，然后按照官方的说明，使用kube-flannel.yml来创建flannel deamon set，结果始终报错。正确的姿势是先使用kube-flannel-rbac.yml来创建flannel的ClusterRole并授权。该yml的主要作用是创建名叫flannel的ClusterRole，然后将该ClusterRole与ServiceAccount(flannel)绑定。接下来，当我们使用kube-flannel.yml来创建flannel daemon set的时候，该daemon set明确指定其Pod的ServiceAccount为flannel，于是通过它启动起来的Pod就具有了flannel ClusterRole中指定的权限。 flannel Pod状态为Running，网络不通我之前在我的Mac Pro上跑了三个VM，为了能够访问公网拉取镜像，我为每个VM分配了一张网卡使用NAT模式，其分配到的IP地址可能重启后发生变化。另外，为了我本机方便管理，我为每台VM又分配了一张网卡使用Host-Only网络模式，每个网卡都有一个固定的IP地址方便SSH。然后，奇怪的事情就这样发生了… 原因在与在kube-flannel.yml中，kube-flannel容器的command被指定为： 1command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\"] 可见，其并没有指定使用哪一张网卡作为flanneld对外通信的物理网卡，于是，可能由于机器上面路由配置的差异，导致三台机器并没有一致通过Host-Only网络模式的网卡来打通Overlay网络。遇到这种情况，如果几台机器的配置一致，可以手动修改kube-flannel.yml文件中kube-flannel的command的值，添加参数–iface=ethX,这里的ethX就为我们希望flanneld通信使用的网卡名称。 flannel启动异常，显示install-cni错误这个现象比较坑，遇到这种情况的第一反应就是去查看install-cni容器到底做了什么。我们打开kube-flannel.yml可以看到，该容器的command字段只有简单的一行Shell： 1command: [ \"/bin/sh\", \"-c\", \"set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done\" ] 也就是将镜像中做好的cni-conf.json拷贝到cni的netconf目录下。由于容器的/etc/cni/net.d是挂载主机的对应的目录，所以该操作主要目的是为CNI准备flannel环境，便于启动容器的时候正确从netconf目录中加载到flannel，从而使用flannel网络。 我发现进入主机的netconf目录中能够看到10-flannel.conf： 1234#ls /etc/cni/net.d/10-flannel.conf/etc/cni/net.d/10-flannel.conf# cat /etc/cni/net.d/10-flannel.confcat: /etc/cni/net.d/10-flannel.conf: No such file or directory 无法打出其内容，而且文件显示为红色，说明其内容并没有正确从容器中拷贝过来。 之前我怀疑该异常是因为kubelet所带的文件系统参数为systemd，而docker的文件系统参数为cgroupfs所致，结果发现并非如此。当前能够绕开该异常的workaround为手动进入到主机的netconf目录，创建10-flannel.conf目录，并写入以下数据： 1234567&#123;\"name\": \"cbr0\",\"type\": \"flannel\",\"delegate\": &#123; \"isDefaultGateway\": true &#125;&#125; flannel网络启动正常，能够创建pod，但是网络不通出现该现象一般会想到debug，而debug的思路当然是基于官方的那张网络拓扑图。比如在我的机器上，通过参看网卡IP地址有如下信息： 1234567891011126: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 22:a0:ce:3c:bf:1f brd ff:ff:ff:ff:ff:ff inet 10.244.1.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet 10.244.2.0/32 scope global flannel.1 valid_lft forever preferred_lft forever7:cni0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1450 qdisc noqueue state DOWN qlen 1000 link/ether 0a:58:0a:f4:01:01 brd ff:ff:ff:ff:ff:ff inet 10.244.1.1/24 scope global cni0 valid_lft forever preferred_lft forever inet6 fe80::4cb6:7fff:fedb:2106/64 scope link valid_lft forever preferred_lft forever 很明显，cni0为10.244.1.1/24网段，说明该主机应该位于10.244.1.0/16子网内；但是我们看flannel.1网卡，确有两个IP地址，分别为于两个不同的”/16”子网。所以，可以肯定的是，我们一定是在该太主机上部署了多次kubeadm，而kubeadm reset并不会清理flannel创建的flannel.1和cni0接口，这就导致环境上遗留下了上一次部署分配到的IP地址。这些IP地址会导致IP地址冲突，子网混乱，网络通信异常。 解决的方法就是每次执行kubeadm reset的时候，手动执行以下命令来清楚对应的残余网卡信息： 12ip link del cni0ip link del flannel.1 K8S如何使用Flannel网络Flannel打通Overlay网络当使用kubeadm来部署k8s，网络组件(如flannel)是通过Add-ons的方式来部署的。我们使用这个yml文件来部署的flannel网络。我截除了关键的一段内容(containers spec): 12345678910111213141516171819202122232425262728containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.7.1-amd64 command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ] securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: install-cni image: quay.io/coreos/flannel:v0.7.1-amd64 command: [ \"/bin/sh\", \"-c\", \"set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done\" ] volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ 通过分析该yaml文件可以知道：该pod内有两个container，分别为install-cni和 kube-flannel。 install-cni 主要负责将config-map中适用于该flannel的CNI netconf配置拷贝到主机的CNI netconf 路径下，从而使得K8S在创建pod的时候可以遵照标准的CNI流程挂载网卡。 kube-flannel 主要启动flanneld，该command中为flanneld的启动指定了两个参数: –ip-masq, –kube-subnet-mgr。第一个参数–ip-masq代表需要为其配置SNAT；第二个参数–kube-subnet-mgr代表其使用kube类型的subnet-manager。该类型有别于使用etcd的local-subnet-mgr类型，使用kube类型后，flannel上各Node的IP子网分配均基于K8S Node的spec.podCIDR属性。可以参考下图的方式来查看（该示例中，k8s为node-1节点分配的podCIDR为:10.244.8.0/24） 123456789101112#kubectl edit node node-1apiVersion: v1kind: Node...name: dev-1resourceVersion: \"2452057\"selfLink: /api/v1/nodesdev-1uid: 31f6e4c3-57b6-11e7-a0a5-00163e122a49spec:externalID: dev-1podCIDR: 10.244.8.0/24 另外，flannel的subnet-manager通过监测K8S的node变化来维护了一张路由表，这张表里面描述了要到达某个Pod子网需要先到达哪个EndPoint。 CNI挂载容器到隧道端点如果说flannel为Pod打通了一张跨node的大网，那么CNI就是将各个终端Pod挂载到这张大网上的安装工人。在刚部署好flannel网络并未在该Node上启动任何Pod时，通过ip link我们只能够看到flannel.1这张网卡，却无法看到cni0。难道是flannel网络运行异常吗？我们接下来就来分析flannel的CNI实现原理，就知道答案了。通过传统方式来部署flannel都需要通过脚本来修改dockerd的参数，从而使得通过docker创建的容器能够挂载到指定的网桥上。但是flannel的CNI实现并没有采用这种方式。通过分析CNI代码，我们可以了解flannel CNI的流程： 读取netconf配置文件，并加载/run/flannel/subnet.env环境变量信息。基于加载的信息，生成适用于其delegate的ipam和CNI bridge的netconf文件；其中指定ipam使用host-local，CNI bridge type为bridge。调用deletgate（CNI bridge type）对应的二进制文件来挂载容器到网桥上。 这里的环境变量文件/run/flannel/subnet.env是由flanneld生成的，里面包含了该主机所能够使用的IP子网网段，具体内容如下： 12345# cat /run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.8.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true 这些数据生成ipam的配置文件的依据，另外，flannel CNI插件的代码中，默认指定delegate为bridge： 123if !hasKey(n.Delegate, \"type\") &#123; n.Delegate[\"type\"] = \"bridge\"&#125; 所以，当flannel CNI插件调用delegate，本质上就是调用bridge CNI插件来将容器挂载到网桥上。分析bridge CNI 插件的过程我们可以看到其指定了默认网桥名称为cni0： 123456789const defaultBrName = \"cni0\"func loadNetConf(bytes []byte) (*NetConf, error) &#123; n := &amp;NetConf&#123; BrName: defaultBrName, &#125; ... return n, nil&#125; 因此，现在我们可以将整个流程连起来了：flannel CNI插件首先读取netconf配置和subnet.env信息，生成适用于bridge CNI插件的netconf文件和ipam（host-local）配置，并设置其delegate为bridge CNI插件。然后调用走bridge CNI插件挂载容器到bridge的流程。由于各个Pod的IP地址分配是基于host-local的Ipam，因此整个流程完全是分布式的，不会对API-Server造成太大的负担。 至此，flannel分析就大致结束了，希望对您有帮助！","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/tags/kubernetes/"},{"name":"flannel","slug":"flannel","permalink":"http://ljchen.net/tags/flannel/"},{"name":"cni","slug":"cni","permalink":"http://ljchen.net/tags/cni/"}]},{"title":"某股份制商业银行定制化PaaS介绍","slug":"某股份制商业银行定制化PaaS介绍","date":"2017-05-04T12:11:34.000Z","updated":"2020-04-06T09:24:01.502Z","comments":true,"path":"2017/05/04/某股份制商业银行定制化PaaS介绍/","link":"","permalink":"http://ljchen.net/2017/05/04/某股份制商业银行定制化PaaS介绍/","excerpt":"某股份制商业银行的PaaS平台是由Wise2C与Rancher合作，基于Rancher做的定制化开发。基于业务场景和银行业的特殊需求，同时为了兼顾能够实现对以后Rancher版本的平滑升级，我们在Rancher之上做了一层逻辑抽象。","text":"某股份制商业银行的PaaS平台是由Wise2C与Rancher合作，基于Rancher做的定制化开发。基于业务场景和银行业的特殊需求，同时为了兼顾能够实现对以后Rancher版本的平滑升级，我们在Rancher之上做了一层逻辑抽象。 软件架构与部署方案整体软件架构如下图所示 顶层的DCOS作为统一的管理平台，可以通过PaaS以及IaaS提供的API实现对云平台的集中管控。左侧蓝色部分是原生Rancher，DCOS与红色定制化部分通过API来访问Rancher。由于未对Rancher做任何改动，可以做到对Rancher版本大于1.2的平滑升级。 红色部分为定制化逻辑抽象部分，归纳起来可以按照功能职责大致分为以下微服务（后面会详细介绍）： 鉴权认证 资源管理 应用编排 弹性伸缩 日志收集 监控告警 镜像仓库 这些微服务在部署时按照Rancher将infrastructure stack部署到环境中的思路，使用一个独立的Rancher环境来部署这些微服务，部署拓扑结构如下图所示： 图中每一个虚线框对应Rancher中的一个环境；“扩展ENV”这个环境直接使用Rancher server的主机作为Agent，上面运行定制化的微服务。其他环境分别对应到某个租户的特定网络，单个网络内部流量不使用Rancher原生的overlay，而采用Wise2C实现的扁平化网络，网络之间流量由外部防火墙控制。 角色与权限模型PaaS平台的角色与权限模型沿用了Rancher的一部分概念，又引入了自己的内容。主要差异在于两方面： PaaS平台引入了对镜像仓库的管理，这在Rancher中是没有的；即角色的权限，除包含操作Rancher外，还能够操作镜像仓库。镜像仓库与PaaS的权限模型是一致的； 另外，客户引入了租户的概念，这点与Rancher中不同，租户是可以跨越多个Rancher的环境的； Rancher权限模型 平台管理员 拥有整个Rancher平台的所有权限； 环境用户 Owner 拥有环境的所有权限； Member 拥有除对环境内部用户授权外的所有权限； Restricted User 拥有环境内部除对用户授权以及操作基础资源外的所有权限； Read Only 拥有环境内部资源的只读权限； PaaS平台权限模型 平台管理员 等同于Rancher的平台管理员权限再加上对镜像仓库管理的所有权限； 租户内部角色 租户管理员 拥有管理租户资源以及对租户内部用户进行授权的所有权限； 再加上对镜像仓库管理的所有权限 高级成员 在PaaS平台内拥有对租户内用户授权以及操作基础资源外的所有权限； 在镜像仓库内，拥有对镜像仓库设置镜像同步规则、创建、删除镜像仓库Namespace、改变镜像状态等权限； 受限成员 在PaaS平台内拥有对租户内用户授权以及操作基础资源外的所有权限； 在镜像仓库所属Namespace内，拥有上传、下载镜像的权限； Read Only 在PaaS平台内，拥有查看租户类资源的权限； 在镜像仓库所属Namespace内，拥有查看镜像仓库资源的权限； 具体映射关系如下图所示： 鉴权部分的软件设计如下所示: 所有对PaaS访问的API请求均经由API proxy做鉴权控制之后代理到系统内部具体的微服务上。PaaS不直接参与租户的增删查改，API proxy通过与PaaS外部的Keystone通信来获取用户角色以及租户信息。 资源管理网络部分 由于金融行业对网络安全性方面的要求比较苛刻，而Rancher所能够提供的均是基于某个环境内部的overlay网络。Overlay必然会导致很多报文无法被安全设备透明的过滤，这是行业内无法接受的；因此，必须采用扁平网络。 处于安全的考虑，会出现同一个stack内部的多个service需要分别部署到不同的网络分区的需求，采用当前Rancher的managed网络显然无法满足需求；因此，必须支持多网络。 对于扁平网络的支持，我在之前的文章 在Rancher 1.2中实现基于CNI的扁平网络 中有详细的介绍，主要是使用ebtable直接在linux bridge上对流量做控制，从而避免使用overlay；这样，外部安全设备能够透明的看到各个容器之间的流量。 对于多网络的支持，我们是通过在Rancher之上实现一层抽象逻辑来实现的。整个模型演变为一个网络映射为Rancher的一个环境（环境内部运行一个扁平网络）。这部分主要涉及对平台中所有网络的管理，以及维护租户与网络之间的映射关系。 下面举一个例子来描述该流程： 平台管理员在PaaS上创建一个网络，指定网络的参数（子网掩码、网关、所属安全域、所属隔离域等），这些数据会保存到数据库； 平台管理员根据需要为租户分配第一个网络；此时，抽象层需要真正在Rancher上创建出网络所对应的环境；以及创建监控、日志、以及定制化系统所需的system级别的应用堆栈； 当平台管理员为租户分配第二个以上的网络时，抽象层还需要将该Rancher环境与租户其他网络对应的Rancher环境之间建立env link关系，否则跨Rancher环境的应用堆栈各service之间无法使用Rancher DNS进行互访。 存储部分客户PaaS在存储部分最终选定NFS作为其存储方案，前期也有讨论过使用ceph等，这部分我在之前的文章探讨容器中使用块存储 中也有专门分析过为什么不选用那种方案。 由于单个租户可以拥有多个网络（也就是多个Rancher环境），而在Rancher中Rancher-NFS driver所创建volume是基于环境层面的。为了能够将该volume映射到租户层面，我们在抽象层中做了这层映射操作。 具体流程如下： 平台管理员在PaaS中指定参数创建出一个NFS server；同网络一样，此时只是将数据保存到数据库； 平台管理员为租户分配NFS server，此时抽象层真正操作租户网络所对应的多个Rancher环境，在逐个环境内添加用于提供Rancher-NFS driver的system stack； 假设租户内用户创建、删除、更新volume；同上，抽象层需要在逐个租户网络对应的Rancher环境内操作volume； 之所以要这样抽象的原因在于客户存在跨网络部署应用栈的需求，因此，存储必须基于租户的粒度，实现跨Rancher环境共享。除此之外，对NFS server的管理方面，客户方面也有自己特殊的要求： 物理存储是按照性能分等级的，同一个租户应该可以同时拥有金牌、银牌、铜牌的NFS server。基于业务的级别，可以为不同级别的微服务指定使用不同等级的NFS server。 因此，与当前Rancher对存储的使用方式不同体现在：同一个租户可以关联多个NFS server，租户内用户在创建volume的时候，可以指定NFS server。 应用编排在应用编排方面，基于金融行业的特殊安全需求，客户要求应用堆栈能够基于微服务的安全等级来跨网络部署同一个应用堆栈，即应用堆栈中的多个微服务可能跨域多个不同网络。 为了实现跨网络部署微服务，除了对基础资源（网络和存储）模型进行抽象之外，整个应用堆栈的部署流程也需要做相应的调整； 另外，对应用堆栈的描述不再使用rancher的catalog，而是基于一套开源的Tosca模板标准；这样做的目的是方便与OpenStack以及其他平台贯通，方面以后使用同一个模板来描述整个IaaS和PaaS的编排情况； 对应用堆栈以及内部微服务的更新，要求提供统一的接口，均通过下发新的Tosca模板更新应用栈的方式来实现。 在解决应用堆栈的跨网络（Rancher环境）部署以及基于Tosca的编排方面，我们在抽象层中操作流程如下： 接受用户输入的Tosca模板，然后交由translator模块做模板语法的check以及翻译，最终输出能够分别部署到各个Rancher环境的rancher-compose文件以及其他附加信息； orchestration模块需要对translator的返回信息进行资源层面的检查，比如是否该租户拥有应用堆栈部署所需的网络（Rancher环境）等； 基于translator的返回信息，按照各个网络之间微服务的依赖关系，决定各个rancher-compose的部署先后顺序，然后开始往网络中（Rancher环境中）部署没有存在依赖的rancher-compose； 基于Rancher环境中应用堆栈的部署情况，按照依赖顺序，逐个部署后续的rancher-compose； 在确保当前应用堆栈在所有Rancher环境中的rancher-compose都部署完成后，将该应用栈的弹性伸缩规则下发到弹性伸缩模块。 该部分的主要难点在于translator按照Tosca语法对于应用栈的定义，以及对存在依赖关系的多个子栈部署顺序的控制。下图是基于Tosca标准建模的一个应用栈模板，该应用栈中有三个微服务：frontend、app和backend。 弹性伸缩自动弹性伸缩是客户基于其业务场景而定制化的需求，大致如下： 首先弹性伸缩的策略是基于时间段的，即按照一天为周期，可以设置在一天的某个时间段内采用哪一种弹性伸缩策略； 弹性伸缩的策略包括三种： a) 基于微服务下所有容器的CPU利用率的平均值； b) 基于微服务下所有容器的内存使用率的平均值； c) 基于时间段，只要进入该时间区间，直接将容器数量伸或缩为某个最大或者最小值；在从该时间区间离去时，恢复容器数； 支持对某个微服务的弹性伸缩策略使能和去使能； 在对CPU和内存的监测时，又有如下规则： 可设置监控指标的上下阈值； 可设定时长，持续超过指定时长，容器数量增加或减少； 可设定触发伸缩行为时，单次容器数量增减值； 可设定弹性伸缩可调节的容器数最大值和最小值； 可配置弹性伸缩动作之后再触发的时间间隔； 对弹性伸缩功能的实现根据策略的类型不同大致分为两种： 基于时间的策略，该策略主要是对当前时间与策略时间区间做匹配，一旦发现进入到基于时间的策略的时间区间就基于微服务的索引，找到并更改目标微服务的容器数量； 基于内存和CPU利用率的策略本身并不监测CPU和内存信息，而是依赖于监控模块。在应用编排侧添加或更新了某个微服务的弹性伸缩策略后，弹性伸缩模块会将对这个微服务的弹性伸缩策略转换为监控告警策略下发到监控模块；同时，监听来自监控告警模块的告警信息。当收到告警时，弹性伸缩就从自己维护的映射表中找到是具体触发该告警的微服务，然后基于一系列规则来决定是否伸缩微服务的容器数量以及一次调整多少个。 由于弹性伸缩策略被设定于各个时间区间内，必然需要维护众多的定时器。一旦规则被设定后，就相当于为微服务定义好了一个周期为24小时，基于时间的状态机。当微服务数量较多时，如何保障既管理好这些状态机、定时器，又能不消耗掉太多的系统资源是软件设计的难点。 另外，由于各个运行实例都运行着独立的状态机，如何做好弹性伸缩的高可用（冗余）又能够保障冗余部分的数据同步，也是值得深入思考的问题。 日志收集客户PaaS对日志的收集主要按照日志的来源可分为三种类型： 主机日志收集； 容器日志收集； 应用日志收集； 对于主机和容器的日志收集相对比较简单，主要通过对指定目录的文件内容进行收集，然后将收集到的日志信息进行格式化后统一发送到kafka集群； 对于应用的日志收集相对较复杂，既要不对业务容器产生侵入又要保障能够收集到及时的日志。我们是通过在Tosca模板中定义某个微服务的log_files来指定应用日志在容器中的路径以及扩展名的。 当容器被调度到某台主机上时，该主机上的日志收集模块就会基于容器标签得知该容器内的应用日志目录，通过分析容器的详情可以获取到该容器内日志目录所映射到主机上的路径，从而将对容器内应用日志的收集，转换为对主机上特定文件内容的收集。具体的收集方式是采用logstash，通过程序自动修改logstach的配置文件来添加日志来源。 将所有日志收集到kafka之后，客户再采用第三方的日志分析工具来对日志做特定的过滤、分析、搜索和多维度的展现。 监控告警客户的监控需求大致如下： 租户宿主机集群的资源使用情况和运行状况，具体包括： a) 租户集群的容器宿主机数量和总体资源使用情况 b) 租户集群中不同网络区域、等保区域等细分范围的容器宿主机数量和资源使用情况 c) 集群总体容器数量、容器在集群各容器宿主机节点的运行和分布情况 d) 每个容器宿主机节点的资源使用情况、运行容器列表 应用（包含Stack和Service）监控数据，监控数据包括应用容器列表（容器IP、所在宿主机）、应用运行情况（健康情况、资源占用）等 每个容器所使用CPU、内存、网络、存储、标签、端口号等信息进行监控，提供Restful API 事件等信息写入事件审计数据库；同时支持配置事件告警规则，当激活事件告警功能后，根据事先设定的告警规则，从事件审计数据库中读取和过滤信息，转换成syslog格式，再将告警信息通过消息队列发送到PaaS平台外部。 该部分的实现主要使用bosun平台, 容器方面从cAdvisor中采集监控数据，主机方面是直接读取主机实时信息，对Rancher的审计日志，主要通过读取Rancher的数据库来实现。所有的监控数据汇集到bosun之后，通过对bosun做一层封装，一方面用于按照自定义的格式设置告警规则、另一方面实现bosun对接Active MQ将监控信息发送到消息队列，从而对接第三方监控大数据平台。 镜像仓库镜像仓库分为测试仓库和生成仓库，这两个仓库均实现了与PaaS平台的权限模型对接，实现单点登录以及统一的鉴权控制。 另外值得提的是，客户对镜像从测试仓库到生产仓库的同步的流程划分为手动和自动，具体如下： 镜像在被提交到测试仓库后，默认为“开发中”状态； 开发完镜像后，受限用户通过外部协同管理平台来通知高级用户将镜像从测试仓库同步到生产仓库； 高级用户登录测试仓库后，可以修改镜像同步规则；在正式同步之前，高级用户可以修改镜像的“待同步”状态为“开发中”； 如果在生产仓库中已存在对应的Namespace，且高级用户勾选了自动同步，测试仓库会在同步周期超时时同步镜像并将“待同步”镜像状态改为“同步中”。如果同步成功，状态自动更新为“已同步”；否则为“待同步”； 如果在生产仓库中已存在对应的Namespace，且高级用户勾选了手动同步，需要高级用户在测试仓库中手动点击“同步”按钮来同步镜像到生产仓库；如果同步成功，状态自动更新为“已同步”；否则为“待同步”。 补充PaaS项目一期已经实现了绝大部分的功能，已经够满足PaaS平台的日常所需，但是作为一套PaaS平台，在以下方面还可以继续拓展： 未在PaaS平台中引入配置管理，对于微服务配置的更新还依赖于手动修改配置文件，这对于运维来讲是比较繁琐且容易引入故障的操作。 没有引入自动蓝绿部署，对服务的升级还是依赖于手动修改LB配置来实现，不能够实现全自动不中断业务切换业务的平滑升级和回滚。 没有Ingress高级路由控制，无法实现基于特定条件的流量或者用户请求进行区别对待以及灰度发布。","categories":[{"name":"others","slug":"others","permalink":"http://ljchen.net/categories/others/"}],"tags":[{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"},{"name":"paas","slug":"paas","permalink":"http://ljchen.net/tags/paas/"}]},{"title":"在Rancher 1.2中实现基于CNI的扁平网络","slug":"在Rancher-1-2中实现基于CNI的扁平网络","date":"2016-12-13T09:24:17.000Z","updated":"2020-04-06T09:24:01.487Z","comments":true,"path":"2016/12/13/在Rancher-1-2中实现基于CNI的扁平网络/","link":"","permalink":"http://ljchen.net/2016/12/13/在Rancher-1-2中实现基于CNI的扁平网络/","excerpt":"Rancher 1.2之于之前的版本在很多地方都有颠覆性的更新，今天我着重来谈网络方面。在1.2中Rancher实现了对CNI的支持，通过network-plugin来实现对CNI的调用；另外，network-plugin还实现了如为暴露端口的容器配置DNAT，MASQUERADE等操作。","text":"Rancher 1.2之于之前的版本在很多地方都有颠覆性的更新，今天我着重来谈网络方面。在1.2中Rancher实现了对CNI的支持，通过network-plugin来实现对CNI的调用；另外，network-plugin还实现了如为暴露端口的容器配置DNAT，MASQUERADE等操作。 Rancher v1.2网络现状但是，v1.2版本也并非彻底的拥抱CNI，原因如下： Network-plugin默认为必选项，其内部自动检测容器是否expose端口到host，并为容器端口配置DNAT规则；另外，所有容器默认使用docker0经过三层转发（通过Iptables规则控制）来访问外网，即全部配置MASQERADE；这一点限制了网络模型（二层广播域只能在host内部），将影响到希望使用另一张网卡来实现扁平网络的用户； Network-plugin的启动依赖于Metadata，而Metadata和DNS server均使用docker0的bridge网络（IP为169.254.169.250）。即，用户私有化的CNI网络必须要能够访问到docker0网络，否则Rancher提供的服务发现与注册以及其它为业务层提供的服务将不可用。 不支持多个网络，官方宣称只能选择一个CNI网络，在UI中对所添加的各类型的CNI网络均显示为托管网络。其中系统基础服务比如scheduler、health check以及load balance默认均只能在托管网络内工作。如若手工添加了其它CNI网络，将导致第二个CNI网络内，scheduler、health check以及load balance异常。 客户需求 在很多场景中用户对于容器网络的使用，还是希望业务与管理隔离，即通过一张独立的网卡来运行业务流量。 在混合组网的场景中，用户一部分业务运行在裸机中，另一部分业务运行在Rancher容器内，将这两张网络统一为一张扁平化网络的呼声也较大。 解决方案基于Rancher 1.2中CNI的诸多限制，有没有办法去实现扁平网络呢？答案是肯定的! 网络整体拓扑先看一张网络部署图，下图可分为两个区域，Rancher区域与裸机区域。 Rancher区域HOST-1和HOST-2分别为Rancher的agent节点（每个节点有两张网卡），按业务划分，该区域内部可以通过容器部署一些变动大、常启停或常扩缩容的业务。 裸机区域Host-3以及其它主机为物理服务器（即裸机），按照业务划分，host-3上可运行一些相对业务对硬件资源要求较高，且不常变动的业务组件。 这两个区域通过业务交换机二层互联，如果网络规模小，这样的拓扑结果是没有问题的。如若网络规模大，需要考虑广播域的问题，为了避免广播风暴，一些客户会使用一些支持SDN的设备来取代业务交换机，从而对二层广播做限制。 扁平网络内部（包括两个区域的所有主机）统一使用外部的路由器做网关，比如图中，Rancher内部的容器的子网范围为10.43.0.0/24, IP地址池范围为10.43.1.2-10.43.1.254。同理，裸机域内，子网范围为10.43.0.0/24, IP地址池范围为10.43.2.2-10.43.2.254。 之所以要将管理网络和业务网路经过同一个路由器（或者防火墙）是因为scheduler需要访问cattle，即管理网；另一方面，scheduler又需要由CNI网络中的health check 做健康检查和故障恢复。若考虑安全问题，可以在防火墙上配置规则，对业务网对管理网的访问做限制。 Rancher内部CNI网络内部CNI网络主要需要解决两个问题： 如何访问Metadata和DNS server的地址169.254.169.250； 采用独立的网卡来转发业务流量后，二层广播域跨主机了，若每台主机上还通过同一个IP 169.254.169.250访问DNS和Metadata服务，如何解决地址冲突的问题； 下图是宿主机内部CNI网络的拓扑图以及流量转发规则： 由于扁平网络需要使用自定义的bridge，与docker0无关。同一个network内部的所有容器属同一个二层网络，且都不可见169.254.169.250地址。为了让容器可以访问该地址，我们采用将br0（CNI bridge）与docker0连通，然后再该链路上的流量做限制来实现。具体如下： container-1内部有到达169.254.169.250的一条主机路由，即要访问169.254.169.250需要先访问10.43.0.2； 通过veth-cni与veth-doc的链接，CNI bridge下的container-1可以将ARP请求发送到docker0的10.43.0.2地址上。由于10.1.0.2的ARP response报文是被veth-cni放行的，于是container-1能够收到来自10.43.0.2的ARP response报文。 然后container-1开始发送到169.254.169.250的IP请求，报文首先被送到docker0的veth-doc上，docker0查询路由表，将报文转到DNS/metadata对应的容器。然后IP报文原路返回，被docker0路由到veth1上往br0发送，由于来自169.254.169.250的IP报文都是被放行的，因此container-1最终能够收到IP。 由于属于该network的所有的宿主机的docker0上都需要绑定IP地址10.43.0.2；因此，该IP地址必须被预留，即，在catalog中填写CNI的netconf配置时，不能将其放入IP地址池。 同时，为了保障该地址对应的ARP请求报文不被发送出主机，从而收到其他主机上对应接口的ARP响应报文，需要对所有请求10.1.0.2地址的ARP REQUEST报文做限制，不允许其通过br0发送到宿主机网卡。 具体转发规则对应的ebtables规则如下所示： Drop All traffic from veth-cni except: IP response from 169.254.169.250 ARP response from 10.43.0.2 123456ebtables -t broute -A BROUTING -i veth-cni -j DROPebtables -t broute -I BROUTING -i veth-cni -p ipv4 --ip-source 169.254.169.250 -j ACCEPTebtables -t broute -I BROUTING -i veth-cni -p arp --arp-opcode 2 --arp-ip-src 10.43.0.2 -j ACCEPTDrop ARP request for 10.43.0.2 on eth1ebtables -t nat -D POSTROUTING -p arp --arp-opcode 1 --arp-ip-dst 10.43.0.2 -o eth1 -j DROP","categories":[{"name":"sdn","slug":"sdn","permalink":"http://ljchen.net/categories/sdn/"}],"tags":[{"name":"CNI","slug":"CNI","permalink":"http://ljchen.net/tags/CNI/"},{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"},{"name":"flatnetwork","slug":"flatnetwork","permalink":"http://ljchen.net/tags/flatnetwork/"}]},{"title":"探讨容器中使用块存储","slug":"探讨容器中使用块存储","date":"2016-11-30T01:03:32.000Z","updated":"2020-04-06T09:24:01.501Z","comments":true,"path":"2016/11/30/探讨容器中使用块存储/","link":"","permalink":"http://ljchen.net/2016/11/30/探讨容器中使用块存储/","excerpt":"块存储是将裸磁盘空间通过划逻辑盘，做Raid，或者LVM（逻辑卷）等方式逻辑划分出N个逻辑的硬盘，然后采用映射的方式将这些逻辑盘挂载到主机。主机的操作系统认为这些磁盘均为物理硬盘，跟直接拿一块物理硬盘挂载到操作系统没有区别。","text":"块存储是将裸磁盘空间通过划逻辑盘，做Raid，或者LVM（逻辑卷）等方式逻辑划分出N个逻辑的硬盘，然后采用映射的方式将这些逻辑盘挂载到主机。主机的操作系统认为这些磁盘均为物理硬盘，跟直接拿一块物理硬盘挂载到操作系统没有区别。 块存储的优点不言而喻： 使用了Raid与LVM等手段，可以多数据做冗余保护； 可以使用磁盘阵列，组成大容量的逻辑盘对外提供服务，提高容量； 对逻辑盘写数据可转化为对几块物理磁盘的并行写入，提升了读写效率。 需求在IaaS中，块存储被广泛应用于为虚拟机提供持久卷。虚拟机故障后依然能够通过在其他虚拟机上挂载旧数据卷的方式来访问磁盘数据，因此被普遍认可。受此影响，一些客户在设计PaaS时，自然而然的联想到共享存储的这一优势，提出在PaaS中为容器挂载块存储的需求。 但是，凭借在IaaS中的卓越表现，块存储也能够跻身PaaS业务中吗？ 场景一“我使用容器技术，但还不是深度用户，只使用docker run之类的命令来手工启动单个容器，这些容器中运行的服务都比较消耗磁盘空间。”这类用户局限于将容器作为一个临时工具，并没有将其与业务紧密结合。 如果对磁盘没有持久化的需求，那么在主机磁盘空间空余足够的情况下可以考虑直接映射宿主机文件系统。 若想让磁盘独占共享磁盘，或希望在不同宿主机上启用容器时，均能重复使用之前访问过的数据卷；此时可以在启动docker时，指定volume-driver为ceph-rbd的方式来使用由Ceph集群提供的块存储。 块存储驱动框架图如下所示： 场景二“我使用容器编排工具来管理应用，比如docker-compose、Rancher或Kubernetes。但我在每一个service下只需要启用一个container，且对service没有扩容的需求；这些service都比较消耗磁盘，我希望在容器被重启或重新调度后，仍可使用旧的数据盘。” 该场景比较特殊，每一个service只对应一个container。因此，不会有多个container同时读写一块数据盘的需求，只需保障container之前所挂载的存储卷在container故障恢复或正常迁移后依然能够被container访问即可；即存储卷对容器的自动跟随。 迁移场景如下图所示： 但是，值得注意的是： 假设之前的container运行在host-1上，对应的，块存储就挂载在host-1上。 当原有container因故被调度到新的host-2上时，编排框架检测到该变化，将host-1上的原有块设备卸载，然后挂载到host-2。 按照该流程，块存储的每一次迁移都需要从一台主机上卸载，再到另外一台主机上挂载；新container的启动依赖于volume，因此容器或者业务的恢复速度依赖于块存储的迁移速度。 假设平台未能及时检测到host-1上的container故障、旧有的container卡死无法快速销毁，或者host-1突然断电时，Ceph Server必须等待超时后，才允许host-2重新挂载之前被使用的RBD块。此时，container启动时间就会变得难以忍受，显然这是与容器的秒起秒停的优势相互违背的。 场景三我使用Rancher或者Kubernetes之类的容器编排软件来管理应用，每一个应用有多个微服务；每一个微服务又对应多个容器来并行对外提供服务。 Rancher在Rancher里面，应用被称之为Stack，每一个stack包含一到多个service； service即微服务，微服务之间按照单一职责划分，只做一件事情。service属于逻辑的概念，真正做事的是各service对应的containers。如果希望通过为service扩容，就增加service对应的container数量；这些container无状态，可被调度到多台宿主机上，它们必须使用共享存储来保障业务的持续性。 Kubernetes在Kubernetes中，业务也是由一到多个service来共同完成的，这里的service与Rancher中service的概览类似。Kubernetes的service是一个外部访问点（endpoint），通过selector指定labels的方式可以选定一组pods，service为这组pods提供访问代理和负载均衡。外部的客户端只需知道service所暴露的端口和IP就能够访问到业务。由于pod是无状态的，因此同Rancher一样，也需要将业务数据存储到共享存储上，还必须保障同一个service对应的多个pods均能共享该业务数据。 限于块存储只能同时被一个客户端（主机）所挂载，当service的多个containers或pods调度到多台主机上时，块存储就难以应付了。此时，原始共享文件系统的方法又体现出了它的优势，NAS会是一个最好的选择！","categories":[{"name":"container","slug":"container","permalink":"http://ljchen.net/categories/container/"}],"tags":[{"name":"storage","slug":"storage","permalink":"http://ljchen.net/tags/storage/"},{"name":"container","slug":"container","permalink":"http://ljchen.net/tags/container/"},{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"}]},{"title":"K8s Network in Rancher v1.6","slug":"k8s-network-in-rancher-v1-6","date":"2016-08-24T15:06:22.000Z","updated":"2020-04-06T09:24:01.482Z","comments":true,"path":"2016/08/24/k8s-network-in-rancher-v1-6/","link":"","permalink":"http://ljchen.net/2016/08/24/k8s-network-in-rancher-v1-6/","excerpt":"该幻灯片是之前参加Rancher组织的深圳meetup时准备的，主要是讲述k8s network在rancher v1.6中的实现。对k8s网络不了解的可以当做入门教程。","text":"该幻灯片是之前参加Rancher组织的深圳meetup时准备的，主要是讲述k8s network在rancher v1.6中的实现。对k8s网络不了解的可以当做入门教程。 幻灯片正文 希望对你有所帮助！","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://ljchen.net/categories/kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://ljchen.net/tags/k8s/"},{"name":"opensource","slug":"opensource","permalink":"http://ljchen.net/tags/opensource/"},{"name":"rancher","slug":"rancher","permalink":"http://ljchen.net/tags/rancher/"},{"name":"network","slug":"network","permalink":"http://ljchen.net/tags/network/"}]}]}